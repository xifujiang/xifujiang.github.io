[{"title":"jvm","date":"2020-01-12T16:00:00.000Z","path":"/posts/f0675471/","text":"JVMjava虚拟机（java virtual machine，JVM），一种能够运行java字节码的虚拟机。JVM不只是专用于java语言，只要生成的编译文件匹配JVM对加载编译文件格式要求，任何语言都可以由JVM编译运行。如kotlin、scala等。 jvm有很多，不只是Hotspot，还有JRockit、J9等。 一、JVM的基本结构JVM由三个主要的子系统构成 类加载子系统 运行时数据区（内存结构） 执行引擎 二、类加载子系统1、类的生命周期加载->连接（验证，准备，解析）->初始化->使用->卸载 1.加载将.class文件从磁盘读到内存 2.连接2.1验证验证字节码文件的准确性 2.2准备给类的静态变量分配内存，并赋予默认值 2.3解析类装载器装入类所引用的其他所有类 3.初始化为类的静态变量赋予正确的初始值，上述的准备阶段为静态变量赋予的是虚拟机默认的初始值，此处赋予的才是编程编写者为变量分配的真正的初始值，执行静态代码块。 4.使用5.卸载2、类的加载器种类启动类加载器（Bootstrap ClassLoader）负责加载JRE的核心类库，如JRE目标下的rt.jar，charsets.jar等 扩展类加载器（Extension ClassLoadEer）负责加载JRE扩展目录ext中jar类包 系统类加载器（Application ClassLoader）负责加载ClassPath路径下的类包 用户自定义加载器（User ClassLoader）负责加载用户自定义路径下的类包 2、类加载机制全盘负责委托机制当一个ClassLoader加载一个类的时候，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个ClassLoader载入 双亲委派机制指先委托父类加载器寻找目标类，在找不到的情况下载自己的路径中查找并载入目标类 双亲委派模式的优势 沙箱安全机制：比如自己写的String.class类不会被加载，这样可以防止核心库被随意篡改 避免类的重复加载：当父ClassLoader已经加载了该类的时候，就不需要子ClassLoader再加载一次 GC算法和收集器新生代：轻GC 老年代：full GC 如何判断对象可以被回收堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能被任何途径使用的对象） 引用计数法给对象添加一个引用计数器，每当有一个地方引用，计数器就加1。当引用失效，计数器就减1。任何时候计数器为0的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中没有选择这个算法来管理内存，最主要的原因是它很难解决对象之前相互循环引用的问题。 可达性分析算法！！这个算法的基本思想就是通过一系列的称为“GC Roots”的对象作为起点，这些节点开始向下搜索，节点走过的路径与当一个对象到GC Roots没有任何引用链相连的话，则证明此对象不可用的。 GC Roots根结点：类加载器、Thread、虚拟机栈的本地变量表、static成员、常量引用、本地方法栈的变量等等。 对象的引用：强引用 软引用 弱引用 虚引用 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。 如果在常量池中存在字符串“abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量“abc”就是废弃常量。 如何判断一个类是无用的类需要满足以下三个条件： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 垃圾回收算法标记-清除算法它是最基础的收集算法，这个算法分为两个阶段，“标记”和“清除”。首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它有两个不足的地方： 1.效率问题，标记和清除两个过程的效率都不高。 2.空间问题，标记清除后会产生大量不连续的碎片。 复制算法为了解决效率问题，复制算法出现了。它可以把内存分为大小相同的两块，每次只使用其中一块。当这一块的内存使用完后，就将还存活的对象复制到另一块区，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对 内存区间的一般进行回收。 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程和“标记-清除”算法一样，但是后续步骤不是直接对可回收对象进行回收，而是让所有存活的对象向一段移动然后直接清理掉边界以外的内存。 分代收集算法现在的商用虚拟机的垃圾收集器基本都采用“分代收集”算法，这种算法就是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法， 在新生代中，每次收集都有大量对象死去，所以可以选择复制算法，只要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，就必须选择“标记-清除”或者“标记-整理”算法进行垃圾收集。 三、垃圾收集器Java虚拟机规范对垃圾收集器应该如何实现没有任何规定，因为没有所谓最好的垃圾收集器出现，只能根据具体的应用场景选择合适的垃圾收集器。 Serial收集器Serial（串行）收集器是最基本、历史最悠久的垃圾收集器。是一个单线程收集器。它的“单线程”的意义不仅仅意味着它只会使用一条垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有线程（“Stop The World”），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程应用程序暂停——应用程序线程 Serial收集器相比其他垃圾收集器更加简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是不错的选择。 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等）和Serial收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程 多线程并发 应用程序暂停—— 应用程序线程 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器）配合工作。 Parallel Scavenge收集器Parallel Scavenge收集器类似于ParNew收集器。 Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程 多线程并发 应用程序暂停—— 应用程序线程 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种是JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 Parallel Old收集器Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以用Scavenge收集器和Parallel Old收集器。 CMS收集器并行和并发概念补充： 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集器同线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种”标记-清除”算法实现的，它的运作过程分为四个步骤： 初始标记（CMS initial mark）：暂停所有的其他线程，并记录下直接与root相连的对象，速度很快。 并发标记（CMS concurrent mark）：同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记（CMS remark）：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清除（CMS concurrent sweep）：开启用户线程，同时GC线程开始对为标记的区域做清扫。 CMS主要优点：并发收集、低停顿。 缺点： 对CPU资源敏感； 无法处理浮动垃圾； 它使用的回收算法“标记-清除”算法导致收集结束时会有大量空间碎片产生。 G1收集器G1（Garbage-First）是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器。以极高概率满足GC停顿时间要求的同时，还具备高吞吐性能特征。（被视为JDK1.7中HotSpot虚拟机的一个重要进化特征） 并行和并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但还是保留了分代的概念。 空间整合：与CMS的“标记-清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器，从局部上来看是基于“” 可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。 G1收集器的运作步骤： 初始标记 并发标记 最终标记 筛选回收 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率。 怎么选择垃圾收集器？1、优先调整堆的大小让服务自己来选择 2、如果内存小于100m，使用串行收集器 3、如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 4、如果允许停顿时间超过1秒，选择并行或者JVM自己选 5、如果响应时间最重要，并且不能超过1秒，使用并发收集器 官方推荐G1，性能高 调优JVM调优主要是调整下面两个指标 停顿时间：垃圾收集器做垃圾回收中断应用执行时间。-XX:MaxGCPauseMillis 吞吐量：垃圾收集的时间和总时间的占比：1/（1+n），吞吐量为1-1/（1+n）。-XX:GCTimeRatio=99 GC调优步骤 ！！1、打印GC日志 123idea在vm options处加入-XX:+PrintGCDetails测试:代码调用system.gc后输出以下内容: 测试:代码调用system.gc后输出以下内容: Tomcat可以直接加载Java_OPTS变量里 2、分析日志得到关键性指标 3、分析GC原因，调优JVM参数 使用GCeasy分析 四、运行时数据区1、内存私有程序计数器 较小的内存空间，线程所执行的字节码的行号指示器，线程私有Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式实现的。在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。每个线程都有一个独立的程序计数器。1、如果执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址2、如果正在执行的是Native方法，这个计数器值则为空。 Java虚拟机栈 线程私有的，生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型。每个方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。通常我们讲的栈就是局部变量表部分。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char…..）、对象引用（reference类型）、returnArrdress类型（指向了一条字节码指令的地址） 本地方法栈 本地方法栈则为虚拟机使用到的Native方法服务。本地方法栈区域也会抛出StackOverflowError的OutOfMemoryError异常 Java堆 是JVM所管理的内存最大的一块。唯一的目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。java堆是垃圾收集器管理的主要区域，有时候也称为GC堆Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在堆中没有完成实例分配，并且堆也无法再扩展时。会抛出OutOfMemoryError异常 方法区 方法区是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。运行时常量池是方法区的一部分 执行引擎新生代 老年代Java堆是Java虚拟机管理的最大的一块内存空间，主要存放对象实例。 在Java中，堆被分为两块区域：新生代、老年代。 堆大小=新生代+老年代。（分别占堆空间为1/3、2/3） 新生代新生代又被分为Eden、from survivor、to survivor（8:1:1） 新生代这样划分是为了更好的管理堆内存中的对象，方便GC算法–>“复制算法”来进行垃圾回收。 JVM每次只会使用Eden和其中一块survivor来为对象服务，所以无论什么时候，都会有一块survivor空间，因此新生代实际可用空间为90%。 新生代GC（minor gc）：指发生在新生代的垃圾回收动作，因为Java对象大多数都是“朝生夕死”的特性，所以minor GC非常频繁，使用复制算法快速的回收。 新生代几乎是所有Java对象出生的地方，Java对象申请的内存和存放都是在这个地方。 当对象在Eden（包括一个survivor，假如是from），当此对象经过一次minor GC后仍然存活，并且能够被另一块survivor所容纳（这里的survivor则是to），则使用复制算法将这些仍然存活的对象复制到to survivor区域中，然后清理掉Eden和from survivor区域，并将这些存活的对象年龄+1，以后对象在survivor中每熬过一次则+1，当达到某个值（默认为15），这些对象会成为老年代！ 事情不是绝对，有些较大的对象（需要分配连续的内存空间），则直接进入老年代。 老年代老年代GC（major GC）：指发生在老年代的垃圾回收动作，所采用的的是“标记–整理”算法。 老年代几乎都是从survivor中熬过来的，不会轻易“死掉”，因此major GC不会像minor GC那样频繁 什么叫复制算法 两块survivor，每次使用其中的块。当这一块使用完了，就将还存储着的对象复制到另一块survivor上面，然后再把已经使用过的内存空间一次清理掉，下图为示意图。 优点：不用考虑内存碎片问题，实现简单，运行效率高。 缺点：当对象存活较高（PS：老年代）时，就要进行较多的复制操作，效率会很低。 复制算法示意图 什么叫标记–整理算法 与“标记-清理”算法相似，只是后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，下图为示意图。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"JAVAEE笔记04——微服务","date":"2020-01-06T16:00:00.000Z","path":"/posts/7a6c41a4/","text":"一、原始服务的调用通过网页的请求，这个网页请求到另一个网页，利用nginx做负载均衡。 消费者 12345678910@RestControllerpublic class IndexController { @Autowired RestTemplate restTemplate; @RequestMapping(\"/indexClient.do\") public String index() { ResponseEntity responseEntity = restTemplate.getForEntity(\"http://127.0.0.1/index.do\",String.class); return responseEntity.getBody(); }} RestTemplate： 是Spring用于同步client端的核心类，简化了与http服务的通信，并满足RestFul原则，程序代码可以给它提供URL，并提取结果。默认情况下，RestTemplate默认依赖jdk的HTTP连接工具。当然你也可以 通过setRequestFactory属性切换到不同的HTTP源，比如Apache HttpComponents、Netty和OkHttp。 生产者：1111 1234567891011@RestControllerpublic class IndexController { @RequestMapping(\"/index.do\") public List index() { List list= new ArrayList(); Map map = new HashMap(); map.put(\"key\",\"port:1111\"); list.add(map); return list; }} 生产者：1121 1234567891011@RestControllerpublic class IndexController { @RequestMapping(\"/index.do\") public List index() { List list= new ArrayList(); Map map = new HashMap(); map.put(\"key\",\"port:1121\"); list.add(map); return list; }} nginx负载均衡 12345678910111213141516171819upstream backser { server 127.0.0.1:1111; server 127.0.0.1:1121;} server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://backser; } } 二、 服务注册中心eureka Eureka 是 Netflix 开发的，一个基于 REST 服务的，服务注册与发现的组件 它主要包括两个组件：Eureka Server 和 Eureka Client Eureka Client：一个Java客户端，用于简化与 Eureka Server 的交互（通常就是微服务中的客户端和服务端） Eureka Server：提供服务注册和发现的能力（通常就是微服务中的注册中心） Eureka Server依赖包 1234 org.springframework.cloud spring-cloud-starter-netflix-eureka-server Eureka Client依赖包 1234 org.springframework.cloud spring-cloud-starter-netflix-eureka-client 依赖管理 1234567891011 org.springframework.cloud spring-cloud-dependencies ${spring-cloud.version} pom import Server application.yml 123456789101112server: port: 9000eureka: server: enable-self-preservation: false instance: hostname: localhost client: register-with-eureka: false fetch-registry: false service-url: default-zone: http://${eureka.instance.hostname}:${server.port}/eureka ServerApplication 1234567@SpringBootApplication@EnableEurekaServerpublic class EurakeApplication { public static void main(String[] args) { SpringApplication.run(EurakeApplication.class, args); }} client application.yml 123456789server: port: 1111eureka: client: service-url: defaultZone: http://127.0.0.1:9000/eurekaspring: application: name: shadow-order-service-1111 ClientApplication 1234567@SpringBootApplication@EnableDiscoveryClientpublic class OrderApplication1111 { public static void main(String[] args) { SpringApplication.run(OrderApplication1111.class, args); }} 三、dubbo zookeeperzookeeper服务器启动的逻辑：1、初始化配置 2、监听端口 3、初始化DataTree 4、领导者选举 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"shiro笔记","date":"2019-12-24T02:05:40.081Z","path":"/posts/7d9cd029/","text":"一、概念什么是shiroshiro是基于java的开源的安全管理框架。可以完成认证，授权，会话管理，加密，缓存等功能。 Primary ConcernsAuthentication：认证 验证用户是否合法。也就是登陆。 Authorization：授权 授予谁具有访问某些资源的权限。 Session Management：会话管理 用户登录后的用户信息通过Session Management来进行管理，不管是在上面应用中。 Cryptograpy：加密 提供了常见的一些加密算法，使得在应用中很方便的实现数据安全，并且使用很便捷。 Supporting FeaturesWeb support：web应用程序支持 ​ Shiro可以很方便的集成到web应用程序中。 Caching：缓存 ​ Shiro提供了对缓存的支持，多种缓存架构：如ehcache，还支持缓存数据库：如redis Concurrency：并发支持 ​ 支持多线程并发访问 Testing：测试 RunAs：一个允许用户假设为另一个用户身份（如果允许）的功能。 Remember Me：记住我 Shiro的架构Subject（org.apache.shiro.subject.Subject） ​ 当前与软件进行交互的实体（用户，第三方服务，cron job，等等）的安全特定“视图”。 SecurityManager(org.apache.shiro.mgt.SecurityManager)：安全管理器 ​ 如上所述，SecurityManager是Shiro架构的心脏。它基本上是一个“保护伞”对象，协调其管理的组件以确保它们能够一起顺利的工作。它还管理每个应用程序用户的shiro的视图，因此它知道如何执行每个用户的安全操作 Authenticator(org.apache.shiro.authc.Authenticator)：认证器 ​ 负责验证用户的身份 Authorizer：授权器 ​ 负责为合法的用户指定其权限。控制用户可以访问那些资源。 Realms：域 用户通过shiro来完成相关的安全工作，shiro是不会维护数据信息的。在shiro的工作过程中，数据的查询和获取工作是通过Realm从不同的数据源来获取的。Realm可以获取数据库信息，文本信息等。 二、用户认证Authentication：用户认证 验证用户是否合法，需要提交身份和凭证给shiro。 principals：用户的身份信息，是subject的标识属性。能够唯一标识Subject。如：电话号码，电子邮箱，身份证号码等。 Credentials 凭证：密码。是只被subject知道的秘密值。可以是密码，也可以是数字证书等。 Pricipals/Credentials 最常见的组合：用户名/密码。在shiro中通常使用UsernamePasswordToken来指定身份和凭证信息。 在shiro中用户的认证流程1、subject -> 2、sucurity manager ->3、Authenticator ->4、Authentication-> 5、JDBC Realm shiro.ini 123[users]zhangsan=1111lisi=1111 AuthenticationDemo 1234567891011121314151617181920212223242526public class AuthenticationDemo { public static void main(String[] args) { //1、创建SecurityManager工厂 读取相应的配置文件 Factory factory = new IniSecurityManagerFactory(\"classpath:shiro.ini\"); //2、通过SecurityManager工厂获取SecurityManager的实例 SecurityManager securityManager = factory.getInstance(); //3、将SecurityManager对象 设置到运行环境中 SecurityUtils.setSecurityManager(securityManager); //4、通过SecurityUtils获取主体Subject Subject subject = SecurityUtils.getSubject(); try { //5、加入登录的用户名zhangsan和1111， UsernamePasswordToken token = new UsernamePasswordToken(\"zhangsan\", \"1111\"); //进行用户身份验证 subject.login(token); //通过subject判断用户是否通过验证 if (subject.isAuthenticated()) { System.out.println(\"用户登录成功\"); } else { System.out.println(\"用户名或密码不正确\"); } }catch (AuthenticationException e) { e.printStackTrace(); } }} 常见的异常信息及处理在认证过程中有一个父异常为：AuthenticationException。 该异常有几个子类： DisabledAccountException 账户失效异常 ExcessiveAttemptsException 尝试次数过多 UnknowAccountException 用户不正确 ExpiredCredentialsException 凭证过期异常 IncorrrectCredentialsException 凭证不正确 执行流程 通过shiro相关api，创建securityManager及获取Subject实例 封装token信息 通过subject.login(token)进行用户认证 Subject接收token，通过其实现类DelegatingSubject将token委托给SecurityManager来完成认证。SecurityManager是接口通过DefaultSecurityManager来完成相关的功能。由DefaultSecurityManager中login来完成认证过程。在login调用了该类authenticate()来完成认证。该方法是由AuthenticatingSecurityManager来完成的。在该类的authenticate()中，通过调用authenticator(认证器)来完成认证工作。Authenticator是由其默认实现类ModularRealmAuthentucator来完成认证。通过ModularRealmAuthentucator中的doAuthenticate来获取Realms信息。如果是单realm直接将token和realm中的数据进行比较，判断是否认证成功。如果是多realm那么需要通过Authentication Strategy来判断对应的认证工作。 通过subject.isAuthenticate()来判断是否认证成功。 依赖包1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 org.apache.shiro shiro-aspectj 1.3.2 org.apache.shiro shiro-cas 1.3.2 org.apache.shiro shiro-core 1.3.2 org.apache.shiro shiro-ehcache 1.3.2 org.apache.shiro shiro-guice 1.3.2 org.apache.shiro shiro-quartz 1.3.2 org.apache.shiro shiro-spring 1.3.2 org.apache.shiro.tools shiro-tools-hasher 1.3.2 org.apache.shiro shiro-web 1.3.2 org.slf4j slf4j-log4j12 1.7.12 test org.slf4j slf4j-api 1.7.21 com.mchange c3p0 0.9.5.5 mysql mysql-connector-java 5.1.47 三、JDBCRealm及Authentication Strategy1、使用shiro框架来完成认证工作，默认情况下使用的是IniRealm。如果需要使用其他Realm，name需要进行相关的配置。 2、Ini配置文件： [main] section是配置应用程序SecurityManager实例及任何它的依赖组件（如Realms）的地方 1234[main]myRealm=cn.sxt.realm.MyRealm# 依赖注入securityManager.reakm=$myRealm [users]section允许定义一组静态的用户账户。在这大部分拥有少数用户账户或用户账户不需要在运行时被动地创建的环境下是很有用的。 123[users]Zhangsan=1111Lisi=2222,role1,role2 [roles]section允许把定义在[user]section中的角色与权限关联起来。另外，这在大部分拥有少数用户账户或账户或用户账户不需要在运行时被动态地创建的环境下是很有用的。 12345[users]Zhangsan=1111Lisi=2222,role1,role2[roles]role1=user:add,user:delete 3.使用JdbcRealm来完成身份认证 需要实现JdbcRealm 需要为JdbcRealm设置dataSource 在指定的dataSource所对应的数据库中应用表users，该表中有username，password，password_salt等字段 4.实现步骤： 新建数据库表 配置shiro.ini文件 编写认证 5.Authentication Strategy：认证策略，在shiro中有三种认证策略； AtLeastOneSuccessfulStrategy：如果一个（或更多Realm）验证成功，则整体尝试被认为是成功的。如果没有一个成功，则整体尝试失败。 第一个成功则被认为成功 所有成功被认为成功 四、MD5加密123456789101112131415public class Md5Demo { public static void main(String[] args) { //使用md5加密算法加密 Md5Hash md5 = new Md5Hash(\"1111\"); System.out.println(\"1111==\"+md5.toString()); //加盐 md5 = new Md5Hash(\"1111\", \"sxt\"); System.out.println(\"1111==\"+md5.toString()); //迭代次数 md5 = new Md5Hash(\"1111\", \"sxt\",2); System.out.println(\"1111==\"+md5.toString()); SimpleHash hash = new SimpleHash(\"md5\", \"1111\", \"sxt\", 2); System.out.println(hash.toString()); }} 12341111==b59c67bf196a4758191e42f76670ceba1111==96c0335dbdd59d920980f1c6a74ed1b01111==e41cd85110c7533e3f93b729b25235c3e41cd85110c7533e3f93b729b25235c3 六、授权1、授权：给身份认证通过的人，授予他可以访问某些资源的权限。 2、权限粒度：分为粗粒度和细粒度。 粗粒度：对user的crud操作。 细粒度：对记录的操作。如：值允许查询id为1的user的工资。 shiro一般管理的是粗粒度。如：菜单、按钮、url。一般细粒度的权限是通过业务来管理的。 3、角色：权限的集合。 4、权限表示规则：资源:操作:实例。可以用通配符表示： 如：user:add 表示对user有添加的权限。user:* 表示对user具有所有操作的权限 user:delete:100表示对user标识为100的记录有删除的权限。 5、shiro中的权限流程： image-20191230173141540 七、Shiro中的权限检查方式有3种 编程式 123if(subject.hasRole(\"管理员\")) {//操作某个资源} 注解式 1234@RequireRolepublic void list(){ //查询数据} 标签 123 更新 八、授权流程： 获取subject主体 判断主体是否通过认证 调用subject.isPermitted*/hasRole*来进行权限的判断 Subject是由其实现类DelegatingSubject来调用方法的，该类将处理交给了SecurityManager SecurityManager是由其实现类DefaultSecurityManager来进行处理，该类的isPermitted来处理，其本质父类AuthorizingSecurityManager来处理的。该类将 处理交给了authorizer（授权器） Authorizer由其实现类ModularRealmAuthorizer来处理该类可以调用对应的Realm来获取数据，在该类有PermissionResolver对权限字符串进行解析，在对应的Realm中也有对应的PermissionResolver交给WildcardPermissionResolver该类调用WildcardPermission来进行权限字符串的解析 返回处理结果 九、自定义Realm实现授权1、仅仅通过配置文件来指定权限不够灵活，并且不方便。在实际的应用中大多数情况下都是将用户信息，角色信息，权限信息保存到了数据库中。所以需要从数据库中去获取相关的数据信息。可以使用shiro提供的JdbcRealm来实现，也可以自定义realm来实现。使用jdbcRealm往往不够灵活， document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"git笔记","date":"2019-12-19T01:05:31.891Z","path":"/posts/d9283bc6/","text":"一、git init搭建仓库通过git init命令把这个目录变成Git可以管理的仓库 这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 二、git add添加所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等。 把一个文件放到Git仓库只需要两步。 第一步，用命令git add告诉Git，把文件添加到仓库： 1$ git add readme.txt 三、git commit提交第二步，用命令git commit告诉Git，把文件提交到仓库： 1$ git commit -m \"wrote a readme file\" git commit命令执行成功后会告诉你： 1 file changed：1个文件被改动（我们新添加的readme.txt文件）； 2 insertions：插入了两行内容（readme.txt有两行内容）。 添加文件到Git仓库，分两步： 使用命令git add ，注意，可反复多次使用，添加多个文件； 使用命令git commit -m ，完成。 四、git status查看状态123456789$ git statusOn branch masterChanges not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: readme.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") git status命令可以让我们时刻掌握仓库当前的状态，上面的命令输出告诉我们，readme.txt被修改过了，但还没有准备提交的修改。 五、git diff查看文件具体修改内容查看某文件修改的具体内容 123456789$ git diff readme.txt diff --git a/readme.txt b/readme.txtindex 46d49bf..9247db6 100644--- a/readme.txt+++ b/readme.txt@@ -1,2 +1,2 @@-Git is a version control system.+Git is a distributed version control system. Git is free software. 要随时掌握工作区的状态，使用git status命令。 如果git status告诉你有文件被修改过，用git diff可以查看修改内容。 六、git log查看日志，历史版本查看修改历史 123456789101112131415161718$ git logcommit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master)Author: Michael Liao Date: Fri May 18 21:06:15 2018 +0800 append GPL //修改名commit e475afc93c209a690c39c13a46716e8fa000c366Author: Michael Liao Date: Fri May 18 21:03:36 2018 +0800 add distributed //修改名commit eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0Author: Michael Liao Date: Fri May 18 20:59:18 2018 +0800 wrote a readme file //修改名 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数： 1234$ git log --pretty=oneline1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master) append GPLe475afc93c209a690c39c13a46716e8fa000c366 add distributedeaadf4e385e865d25c48e7ca9c8395c3f7dfaef0 wrote a readme file 1094adb7b9b3807259d8cb349e7df1d4d6477073是commit id()版本号，通过SHA1计算。 七、git reset 回退版本12$ git reset --hard HEAD^HEAD is now at e475afc add distributed git log 查看版本库的现在状态，发现最新版已经看不到了，已经回退 如果想回到原来的最新版，办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是1094adb...，于是就可以指定回到未来的某个版本： 12$ git reset --hard 1094aHEAD is now at 83b0afe append GPL Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL： 123456789┌────┐│HEAD│└────┘ │ └──> ○ append GPL │ ○ add distributed │ ○ wrote a readme file 改为指向add distributed： 123456789┌────┐│HEAD│└────┘ │ │ ○ append GPL │ │ └──> ○ add distributed │ ○ wrote a readme file 然后顺便把工作区的文件更新了。所以你让HEAD指向哪个版本号，你就把当前版本定位在哪。 八、git reflog记录每次命令可以通过这个命令查看历史操作，可以查看commit id，方便 12345$ git refloge475afc HEAD@{1}: reset: moving to HEAD^1094adb (HEAD -> master) HEAD@{2}: commit: append GPLe475afc HEAD@{3}: commit: add distributedeaadf4e HEAD@{4}: commit (initial): wrote a readme file 九、工作区与暂存区工作区（Working Directory）：当前电脑创建的git管理的仓库。 版本区（Repository）： Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 分支和HEAD的概念我们以后再讲。 前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的： 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。 简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。 修改文件，并add添加到暂存区后，状态变为 git-stage commit后，版本库变为下面图，暂存区没有任何内容。 git-stage-after-commit 十、git checkout丢弃工作区的修改1$ git checkout -- readme.txt 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 git checkout -- file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令 十一、git reset HEAD 撤销暂存区的内容，重新放回到工作区123$ git reset HEAD readme.txtUnstaged changes after reset:M readme.txt git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 再用git status查看一下，现在暂存区是干净的，工作区有修改： 12345$ git statusOn branch masterChanges not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) 小结场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD ，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，git reset xxxx，不过前提是没有推送到远程库。 十二、删除文件的解决办法。如果先添加一个文件，并提交，然后把该文件给删了。 123$ git add test.txt$ git commit -m \"add test.txt\"$ rm test.txt 这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了： 123456789$ git statusOn branch masterChanges not staged for commit: (use \"git add/rm ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) deleted: test.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") 现在你有两个选择，1、一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： 12$ git rm test.txt$ git commit -m \"remove test.txt\" 现在，文件就从版本库中被删除了。 2、另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1$ git checkout -- test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 小结命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 十三、设置SSH Key第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key： 1$ ssh-keygen -t rsa -C \"youremail@example.com\" 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面： 然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容： 点“Add Key”，你就应该看到已经添加的Key： 十四、添加远程库现在的情景是，你已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，真是一举多得。 首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库：在Repository name填入learngit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库： 目前，在GitHub上的这个learngit仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。 现在，我们根据GitHub的提示，在本地的learngit仓库下运行命令： 1$ git remote add origin git@github.com:xifujiang/learngit.git 请千万注意，把上面的michaelliao替换成你自己的GitHub账户名，否则，你在本地关联的就是我的远程库，关联没有问题，但是你以后推送是推不上去的，因为你的SSH Key公钥不在我的账户列表中。 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。 下一步，就可以把本地库的所有内容推送到远程库上： 1$ git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 从现在起，只要本地作了提交，就可以通过命令： 1$ git push origin master 把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！ 小结要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git； 关联后，使用命令git push -u origin master第一次推送master分支的所有内容； 此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改。 十五、克隆地址1git clone 地址 小结要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 分支管理Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch 切换分支：git checkout 或者git switch 创建+切换分支：git checkout -b 或者git switch -c 合并某分支到当前分支：git merge 删除分支：git branch -d 解决冲突当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 用git log --graph命令可以看到分支合并图。 推送分支推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： 1$ git push origin master 如果要推送其他分支，比如dev，就改成： 1$ git push origin dev 抓取分支多人协作的工作模式通常是这样： 首先，可以试图用git push origin 推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin 推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to origin/。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 小结 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Java包名","date":"2019-12-05T02:16:00.000Z","path":"/posts/c38d43d9/","text":"Java Bean、POJO、 Entity、 VO ， 其实都是java 对象，只不过用于不同场合罢了。 按照 Spring MVC 分层结构： JavaBean: 表示层 （Presentation Layer） Entity： 业务层 （Service layer） Dao： 数据访问层 （data access layer）。 Entity接近原始数据，Model接近业务对象～Entity：是专用于EF的对数据库表的操作，Model：是为页面提供数据和数据校验的，所以两者可以并存POJO：POJO是Plain OrdinaryJava Object的缩写不错，但是它通指没有使用Entity Beans的普通java对象，可以把POJO作为支持业务逻辑的协助类。domain：domain这个包国外很多项目经常用到，字面意思是域的意思。 POJO实质上可以理解为简单的实体类，顾名思义POJO类的作用是方便 程序员使用数据库中的数据表，对于广大的程序员，可以很方便的将POJO类当做对象来进行使用，当然也是可以方便的调用其get,set方法。 - JavaBean: 先说JavaBean，JavaBean更多的是一种规范，也即包含一组set和get方法的Java对象。 - POJO: 普通的Java对象，对于属性一般实现了JavaBean的标准，另外还可以包含一些简单的业务逻辑(方法)。 - PO: POJO在持久层的体现，对POJO持久化后就成了PO。PO更多的是跟数据库设计层面相关，一般PO与数据表对应，一个PO就是对应数据表的一条记录。 - DAO: PO持久化到数据库是要进行相关的数据库操作的(CRUQ)，这些对数据库操作的方法会统一放到一个Java对象中，这就是DAO。 - BO: POJO在业务层的体现，对于业务操作来说，更多的是从业务上来包装对象，如一个User的BO，可能包括name, age, sex, privilege, group等，这些属性在数据库中可能会在多张表中，因为每一张表对应一个PO，而我们的BO需要这些PO组合起来(或说重新拼装)才能成为业务上的一个完整对象。 - VO(Value Object/View Object): POJO在表现层的体现。 当我们处理完数据时，需要展现时，这时传递到表现层的POJO就成了VO。它就是为了展现数据时用的。 - DTO(Data Transfer Object): POJO在系统间传递时。当我们需要在两个系统间传递数据时，一种方式就是将POJO序列化后传递，这个传递状态的POJO就是DTO。 - EJB(Enterprise JavaBean): 我认为它是一组”功能”JavaBean的集合。上面说了JavaBean是实现了一种规范的Java对象。这里说EJB是一组JavaBean，的意思是这一组JavaBean组合起来实现了某个企业组的业务逻辑。这里的一组JavaBean不是乱组合的，它们要满足能实现某项业务功能的搭配。找个比方，对于一身穿着来说，包括一顶帽子，一件衣服，一条裤子，两只鞋,这穿着就是EJB. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:19:25.299Z","path":"/posts/0/","text":"1、线程1、线程的类别 Thread（类）、Runable、Callable、Future，后面三个都是接口。 Runable有个无返回值的run的抽象方法，使用时可通过实现类重写run方法，使用Thread封装实现类，然后运行。 Callable有一个有返回值V的call方法，并抛出Exception异常。使用时做Callable的实现类，使用FutureTask类做封装，再用Thread类对FutureTask做封装，运行 FutureTask继承RunableFuture，RunableFuture实现Runable和Callable Thread的构造方法有对Runable的参数，重载的方法还有（Runable r, String name） 2、线程池，创建方式 创建大小不固定的线程池：类实现Runable，重写run方法 1、创建大小不固定的线程池：主函数创建线程池的方式，具有缓冲功能的线程池，系统根据需要创建线程，线程会被缓冲到线程池中，如果线程池大小超过了处理任务所需要的线程，线程池就会回收空闲的线程池，当处理任务增加时，线程池可以增加线程来处理任务，线程池不会对线程的大小进行限制，线程池的大小依赖于操作系统 123456ExecutorService es=Executors.newCachedThreadPool();for(int i=0;i线程的一些方法4、同步锁可以锁什么 sync5、volatile关于jmm内存模型，可保证原子的操作吗4、Spring1、Spring的理解2、@Autwired 和 @Resource区别3、spring注入方式4、拦截器和过滤器的底层5、bean的作用域问题6、编译内核7、面向对象和面向切面8、mybatis的一对多，多对一，以及多对对的配置和使用5、数据库1、模糊查询2、索引有哪些8、前后端值传递分布式开发 注意并发计算机网络重定向状态码操作系统命令：网络、连接等命令TCP/IP客户端向服务器端发送SYN包；服务器端向客户端发送SYN+ACK；客户端回复ACK。 访问一个网站，浏览器工作的流程数据库连接过程1、加载驱动 2、建立连接 3、写sql语句 4、创建statement、preparedStatement 5、ResultSet接收 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"WebMagic","date":"2019-12-01T12:04:27.199Z","path":"/posts/a896d992/","text":"1、架构介绍1.1、四大组件WebMagic的结构分为Downloader、PageProcessor、Scheduler、Pipeline四大组件，并由Spider将彼此组织起来。这四大组件对应爬虫声明周期中的下载、处理、管理和持久化等功能。WebMagic的设计参考了Scapy，但是实现方式更Java化一些。 img 1.2、用于数据流转的对象1、Request request是对url地址的一层封装，一个Request对应一个url地址。 它是PageProcessor与Downloader交互的载体，也是PageProcessor控制Downloader唯一方式。 2、Page Page代表了从Downloader下载到的一个页面——可能是HTML，也可能是Json或者其他文本格式的内容。 Page是WebMagic抽取过程的核心对象，它提供一些方法可供抽取、结果保存等。 3、ResultItems ResultItems相当于一个Map，它保存PageProcessor处理的结果供Pipeline使用。它的API与Map很类似，值得注意的是它有一个字段skip，若设置为true，则不应被Pipeline处理。 2、实现PageProcessor2.1抽取元素SelectablewebMageic里使用的三种抽取技术：XPath、正则表达式和CSS选择器。对于 JSON格式的内容，可使用JsonPath进行解析。 2.2 抽取元素APISelectable相关的抽取元素链式API是WebMagic的一个核心 功能。使用Selectable接口，可以直接完成页面元素的链式抽取，也无需去关心抽取的细节。 从刚才的例子中可以看到，page.getHtml()返回的是一个Html对象，它实现了Selectable接口。这个接口包含的方法分为两类：抽取部分和获取结果部分。 方法 说明 xpath(String xpath) 使用XPath选择 html.xpath(“//div[@class=’title’]”) $(String selector) 使用Css选择器选择 html.$(“div.title”) $(String selector, String attr) 使用Css选择器选择 html.$(“div.title”,”text”) css(String selector) 功能同$(),使用Css选择器选择 html.css(“div.title”) links() 选择所有链接 html.links() regex(String regex) 使用正则表达式抽取 html.regex(“\\(.\\*?)\\“) 不同API获取一个或多个元素 说明 示例 get() 返回一条String类型的结果 String link = html.links().get() toString() 同get()，返回一条String类型的结果 String link = html.links().toString() all() 返回所有抽取结果 List links = html.links().all() 3、实现多页面爬虫流程开始-> 列表页面 -> 获取url（url去重） -> url加入任务 ->结束 3.1 Scheduler组件Scheduler是WebMagic中进行URL管理的组件。 作用： 对待抓取的URL队列进行管理。 对已抓取的URL进行去重。 WebMagic内置了几个常用的Scheduler。 类 说明 备注 DulicateRemovedScheduler 抽象基类，提供一些模板方法 继承它可以实现自己的功能 QueueScheduler 使用内存队列保存待抓取URL PriorityScheduler 使用带有优先级的内存队列保存待抓取URL 耗费内存较QueueScheduler更大，但是当设置request.priority之后，只能使用PriorityScheduler才可使优先级生效 FileCacheQueueScheduler 使用文件保存抓取URL，可以在关闭程序并下次启动时，从之前抓取到的URL继续抓取 需指定路径，会建立.urls.txt和.cursor.txt RedisScheduler 使用R额滴神保存抓取队列，可进行多台机器同时合作抓取 需要安装并启动redis 去重部分被单独抽象成一个接口：DuplicateRemover，从而可以为同一个Scheduler选择不同的去重方式，以适应不同的需要，目前提供两种去重方式。 类 说明 HashSetDuplicateRemover 使用HashSet来进行去重，占用内存较大 BloomFilterDuplicateRemover 使用BloomFilter来进行去重，占用内存较小，但是可能漏抓页面 3.2 三种去重方式 HashSet 使用Java中的HashSet不能重复的特点去重 优点：容易理解，使用方便。 缺点：占用内存大，性能较低。 Redis去重 使用Redis的set进行去重。 优点：速度快（Redis本身速度快），去重不会占有爬虫服务器的资源，可以处理更大数据量的数据爬取 缺点：需要准备Redis服务器，增加开发和使用成本。 布隆过滤器（BloomFilter） 优点：占用的内存要比使用HashSet要小的多，也适合大量数据的去重操作。 缺点：有误判的可能。没有重复可能会判断重复，但是重复的数据一定会判断重复。 介绍： 布隆过滤器是一种space efficient的概率型数据结构，用于判断一个元素是否在集合中。在垃圾邮件过滤黑白名单方法、爬虫（crawler）的网址判重模块中等等经常被用到。 哈希表也能用于判断元素是否在集合中，但布隆过滤器只需要哈希表的1/8或1.4的空间复杂度就能完成同样的问题。布隆过滤器可以插入元素，但不可以删除已有元素。其中的元素越多，误报率越大，但是漏报是不可能的。 原理： 布隆过滤器需要的是位数组（二进制数组），全部赋值为0，对于有n个元素的集合S={S1, S2…Sn}，通过k个映射函数{f1,f2,…..fk}，将集合S中的每个元素Sj（1 { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T11:42:01.012Z","path":"/posts/0/","text":"一、数据库1、left join 和 inner join的区别 1、举例a、b表： bId bNum 1 20 2 30 aId aNum 1 10 2 20 3 30 left join以左表为准 select * from a left join b on a.aId=b.bId; aId aNum bId bNum 1 10 1 20 2 20 2 30 3 30 NULL NULL inner join 选两张表都有的，一一相等的 aId aNum bId bNum 1 10 1 20 2 20 2 30 left join 会保留左边表的数据，而inner join是等值连接，必须满足条件才会有数据。 left join的速度比inner join快些。 2、drop、truncate、delete区别最基本： drop直接删掉表。 truncate删除表中数据，再插入时自增长id又从1开始。 delete删除表中数据，可以加where字句。 3.触发器的作用？ 触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。 4.什么是存储过程？用什么来调用？ 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 调用： 1）可以用一个命令对象来调用存储过程。 2）可以供外部程序调用，比如：java程序。 5.什么叫视图？游标是什么？ 视图： 是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。 游标： 是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。 6.非关系型数据库和关系型数据库区别，优势比较? 非关系型数据库的优势： 性能：NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性：同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 关系型数据库的优势： 复杂查询：可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持：使得对于安全性能很高的数据访问要求得以实现。 7、范式第一范式：（确保每列保持原子性），所有的字段值都是不可分解的原子值。 第二范式：（确保表中的每列都和主键相关）在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 第三范式：（确保每列都和主键列直接相关，而不是间接相关）数据表中的每一列数据都和主键直接相关，而不能间接相关。 8、索引1.什么是索引？ 何为索引： 数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2.索引的作用？它的优点缺点是什么？ 索引作用： 协助快速查询、更新数据库表中数据。 为表设置索引要付出代价的： 一是增加了数据库的存储空间 二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动) 3.索引的优缺点？ 创建索引可以大大提高系统的性能（优点）： 1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 增加索引也有许多不利的方面(缺点)： 1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4.哪些列适合建立索引、哪些不适合建索引？ 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。 一般来说，应该在这些列上创建索引： （1）在经常需要搜索的列上，可以加快搜索的速度； （2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； （3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； （4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； （5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； （6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 对于有些列不应该创建索引： （1）对于那些在查询中很少使用或者参考的列不应该创建索引。 这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 （2）对于那些只有很少数据值的列也不应该增加索引。 这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 （3）对于那些定义为text, image和bit数据类型的列不应该增加索引。 这是因为，这些列的数据量要么相当大，要么取值很少。 (4)当修改性能远远大于检索性能时，不应该创建索引。 这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 5.什么样的字段适合建索引 唯一、不为空、经常被查询的字段 6.MySQL B+Tree索引和Hash索引的区别?Hash索引和B+树索引的特点： Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位; B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问; 为什么不都用Hash索引而使用B+树索引？ Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样； Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样； Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用； Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据； Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。 9、事务1.什么是事务？ 事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。 2.事务四大特性（ACID）原子性、一致性、隔离性、持久性? 原子性（Atomicity）:原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency）:事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）:隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）:持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 !!!!! 事务隔离级别为：未提交读时，写数据只会锁住相应的行。 事务隔离级别为：可重复读时，写数据会锁住整张表。 事务隔离级别为：串行化时，读写数据都会锁住整张表。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大 10、JDBC连接步骤 1、加载JDBC驱动 2、建立连接 3、写SQL语句 4、得到preparedStatement 5、执行sql，得到结果集ResultSet 6、处理结果集 7、关闭资源 数据库优化1、存储引擎，包括myisam，innoDB，锁。 2、缓存 3、执行计划、SQL运行 4、分表 5、读写分离、主从复制 6、碎片整理 7、备份 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T11:40:28.576Z","path":"/posts/0/","text":"Redisredis支持的数据类型String，hash（哈希），List（列表），set（集合），zset（有序集合） Redis持久化持久化就是把内存的数据写到磁盘中去，防止服务器宕机了内存数据丢失。 持久化方式：RDB(默认) 和 AOF RDB：redis DataBase 功能核心函数rdbSave（生成RDB文件）和rdbLoad（从文件加载内存）两个函数 AOF：Append-only file 比较： 1、AOF文件比RDB更新频率高，优先使用AOF还原数据。 2、AOF比RDB更安全也更大 3、RDB性能比AOF好 4、如果两个都配了优先加载AOF 架构模式单机版： 特点：简单 问题：1、内存容量有限 2、处理能力有限 3、无法高可用。 主从复制： 有多个redis服务器，数据从主服务器（master）复制到从服务器（slave），数据写入主服务器，从服务器负责读取数据。 特点： 1、master/slave 角色 2、master/slave 数据相同 3、降低 master 读压力在转交从库 问题： 无法保证高可用 没有解决 master 写的压力 哨兵： Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。 特点： 1、保证高可用 2、监控各个节点 3、自动故障迁移 缺点：主从模式，切换需要时间丢数据 没有解决 master 写的压力 集群（proxy）： Twemproxy 是一个 Twitter 开源的一个 redis 和 memcache 快速/轻量级代理服务器； Twemproxy 是一个快速的单线程代理程序，支持 Memcached ASCII 协议和 redis 协议。 特点： 1、多种 hash 算法：MD5、CRC16、CRC32、CRC32a、hsieh、murmur、Jenkins 2、支持失败节点自动删除 3、后端 Sharding 分片逻辑对业务透明，业务方的读写方式和操作单个 Redis 一致 缺点：增加了新的 proxy，需要维护其高可用。 集群（直连型）： 从redis 3.0之后版本支持redis-cluster集群，Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。 特点： 1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。 2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。 3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。 4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本 5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master 的角色提升。 缺点： 1、资源隔离性较差，容易出现相互影响的情况。 2、数据通过异步复制,不保证数据的强一致性 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T10:32:11.282Z","path":"/posts/0/","text":"一、线程1、线程的类别 Thread（类）、Runable、Callable、Future，后面三个都是接口。 Runable有个无返回值的run的抽象方法，使用时可通过实现类重写run方法，使用Thread封装实现类，然后运行。 Callable有一个有返回值V的call方法，并抛出Exception异常。使用时做Callable的实现类，使用FutureTask类做封装，再用Thread类对FutureTask做封装，运行 FutureTask继承RunableFuture，RunableFuture实现Runable和Callable Thread的构造方法有对Runable的参数，重载的方法还有（Runable r, String name） 2、线程池，创建方式 创建大小不固定的线程池：类实现Runable，重写run方法 1、创建大小不固定的线程池：主函数创建线程池的方式，具有缓冲功能的线程池，系统根据需要创建线程，线程会被缓冲到线程池中，如果线程池大小超过了处理任务所需要的线程，线程池就会回收空闲的线程池，当处理任务增加时，线程池可以增加线程来处理任务，线程池不会对线程的大小进行限制，线程池的大小依赖于操作系统 123456ExecutorService es=Executors.newCachedThreadPool();for(int i=0;i { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-18T12:41:17.234Z","path":"/posts/0/","text":"NIO原理解析BIO的缺点两个地方有阻塞——并发需要多线程支持——多线程会造成服务器资源浪费 高并发量引起的问题一个使用传统阻塞I/O的系统,如果还是使用传统的一个请求对应一个线程这种模式,一旦有高并发的大量请求,就会有如下问题： 1、线程不够用, 就算使用了线程池复用线程也无济于事; 2、阻塞I/O模式下,会有大量的线程被阻塞,一直在等待数据,这个时候的线程被挂起,只能干等,CPU利用率很低,换句话说,系统的吞吐量差; 3、如果网络I/O堵塞或者有网络抖动或者网络故障等,线程的阻塞时间可能很长。整个系统也变的不可靠; NIO的设计思路设计初衷：利用单线程处理并发 初始设计缺陷： for循环在Java应用程序中——–>OS———–> java—jni—自己写的一个c for循环太多，且可能大多无意义———->epoll redis——单线程——C——epoll epoll性能大于select IO和NIO的区别原有的 IO 是面向流的、阻塞的，NIO 则是面向块的、非阻塞的。 怎么理解IO是面向流的、阻塞的java1.4以前的io模型，一连接对一个线程。 原始的IO是面向流的，不存在缓存的概念。Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区 Java IO的各种流是阻塞的，这意味着当一个线程调用read或 write方法时，该线程被阻塞，直到有一些数据被读取，或数据完全写入，该线程在此期间不能再干任何事情了。 阻塞I/O模型 怎么理解NIO是面向块的、非阻塞的NIO是面向缓冲区的。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就增加了处理过程中的灵活性。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来,根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个。 NIO的核心实现在标准IO API中，你可以操作字节流和字符流，但在新IO中，你可以操作通道和缓冲，数据总是从通道被读取到缓冲中或者从缓冲写入到通道中。 NIO核心API Channel, Buffer, Selector 通道ChannelNIO的通道类似于流，但有些区别如下： \\1. 通道可以同时进行读写，而流只能读或者只能写 \\2. 通道可以实现异步读写数据 \\3. 通道可以从缓冲读数据，也可以写数据到缓冲: 可以从通道读取数据到缓冲区，也可以把缓冲区的数据写到通道中 缓存Buffer缓冲区本质上是一个可以写入数据的内存块，然后可以再次读取，该对象提供了一组方法，可以更轻松地使用内存块，使用缓冲区读取和写入数据通常遵循以下四个步骤： \\1. 写数据到缓冲区； \\2. 调用buffer.flip()方法； \\3. 从缓冲区中读取数据； \\4. 调用buffer.clear()或buffer.compat()方法； 当向buffer写入数据时，buffer会记录下写了多少数据，一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式，在读模式下可以读取之前写入到buffer的所有数据，一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。 Buffer在与Channel交互时，需要一些标志: buffer的大小/容量 - Capacity 作为一个内存块，Buffer有一个固定的大小值，用参数capacity表示。 当前读/写的位置 - Position 当写数据到缓冲时，position表示当前待写入的位置，position最大可为capacity – 1；当从缓冲读取数据时，position表示从当前位置读取。 信息末尾的位置 - limit 在写模式下，缓冲区的limit表示你最多能往Buffer里写多少数据； 写模式下，limit等于Buffer的capacity，意味着你还能从缓冲区获取多少数据。 下图展示了buffer中三个关键属性capacity，position以及limit在读写模式中的说明： buffer中三个关键属性capacity，position以及limit在读写模式中的说明 缓冲区常用的操作 向缓冲区写数据： ​ \\1. 从Channel写到Buffer； ​ \\2. 通过Buffer的put方法写到Buffer中； 从缓冲区读取数据： ​ \\1. 从Buffer中读取数据到Channel； ​ \\2. 通过Buffer的get方法从Buffer中读取数据； flip方法： ​ 将Buffer从写模式切换到读模式，将position值重置为0，limit的值设置为之前position的值； clear方法 vs compact方法： ​ clear方法清空缓冲区；compact方法只会清空已读取的数据，而还未读取的数据继续保存在Buffer中； Selector一个组件，可以检测多个NIO channel，看看读或者写事件是否就绪。 多个Channel以事件的方式可以注册到同一个Selector，从而达到用一个线程处理多个请求成为可能。 一个thread对应多个channel,一个channel处理一个请求。 当你调用Selector的select()或者 selectNow() 方法它只会返回有数据读取的SelectableChannel的实例. 模拟一个单线程的处理高并发的应用模拟JVM（C C++）实现socket对象的源码模拟一个C语言 JVM当中实现NIO document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"hadoop详细学习","date":"2019-09-13T09:50:11.087Z","path":"/posts/a5ead4c6/","text":"1、hadoop1.1 hadoop介绍1）hadoop是一个由Apache基金会所开发的分布式系统基础架构。 2）主要解决海量数据的存储和海量数据的分析计算问题。 3）Hadoop生态圈 1.2 Lucence框架1）Lucene框架使Doug Cutting开创的开源软件，用户Java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎。 2）2001年年底Lucene成为Apache基金会的一个子项目。 3）对于海量数据的场景，Lucence面对与Google同样的困难，存储数据困难，检索速度慢。 4）学习和模仿Google解决这些问题的办法：微型版Nutch。 5）Google是Haddop的思想之源。 6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 7）2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 8）2006年3月份，Map-Reduce和Nutch Distributed File System（NDFS）分别被纳入称为Hadoop的项目中。 9）名字来源于Doug Cutting儿子的玩具大象。 1.3 hadoop三大发行版本Apache、Cloudera、Hortonworks。 1.4 hadoop的优势（面试）1)高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。 2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。 3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 4）高容错性：能够自动将失败的任务重新分配。 1.5 hadoop1.x 和hadoop2.x区别hadoop1.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算+资源调度） CPU 8 内存：128MB 磁盘 8T hadoop2.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算）、yarn（资源调度） CPU 8 内存：128MB 磁盘 8T 对比：hadoop1.x时代，MapReduce既处理运算，又处理资源调度，耦合性较大。hadoop1.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。 1.6 组件架构1、hdfs架构概述1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、福本数、文件权限），以及每个文件的块列表和块所在的DataNode等。 2）DataNode（dn）：在本地文件系统存储文件数据，以及数据的校验和。 3）Secondary NameNode（2nn）：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。加快启动速度。 2、Yarn架构1）ResourceManager （RM）主要作用如下 （1）处理客户端请求 （2）监控NodeManager （3）启动或监控ApplicationMaster（集群中运行的一个job） （4）资源的分配与调度 yarn架构 2）NodeManager（NM）主要作用如下 （1）管理单个节点上的资源 （2）处理来自ResourceManager的命令 （3）处理来自ApplicationMaster的命令 3）ApplicationMaster（AM）作用如下 （1）负责数据的切分 （2）为某个程序申请资源并分配给内部的任务 （3）任务的监控与容错 4）Container container是Yarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 3、MapReduce架构概述MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据。 2）Reduce阶段对Map结果进行汇总 1.7 大数据技术生态体系 大数据技术生态体系 图中涉及的技术名词解释如下： 1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行**数据的迁移，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： （1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 （2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 （3）支持通过Kafka服务器和消费机集群来分区消息。 （4）支持Hadoop并行数据加载。 4）Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。 6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。 7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 11）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。 12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 1.8 hadoop重要目录（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本 （2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 （3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） （4）sbin目录：存放启动或停止Hadoop相关服务的脚本 （5）share目录：存放Hadoop的依赖jar包、文档、和官方案例 1.9完全分布式运行模式1、准备三台客户机（关闭防火墙、静态ip、主机名称） 2、安装JDK 3、配置环境变量 4、安装hadoop 5、配置环境变量 6、配置集群 7、单点启动 8、配置ssh 9、群起并测试集群 2、HDFS2、1 介绍产生背景：数据量大，一个操作系统存不过来，需要一种系统管理多台机器上的文件 定义：HDFS(Hadoop Distributed File System)，分布式文件管理系统。用于存储文件，通过目录树来定位文件。 使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。 2.2.1 优点1、高容错性 （1）数据自动保存多个副本。通过增加副本的形式，提高容错性。 （2）某个副本丢失后，可以自动恢复 2、适合处理大数据 （1）数据规模：那个处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3、可构建在廉价机器上，通过多副本机制，提高可靠性。 2.2.2 缺点1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 2）无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 3）不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许 多个线程同时写。 （2）仅支持数据append（追加），不支持文件的随机修改。 2.3 组件功能1）NameNode（nn）：就是Master，它是一个管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据块（Block）映射信息； （4）处理客户端读写请求。 2）DataNode：就是slave。NameNode下达命令。DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 3）client：客户端 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如NameNode格式化； （5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作； 4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能替代NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 2.4 文件块大小HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版中是64M。 1、集群中的block，2、如果寻址时间约为10ms，即查到目标block的时间为10ms，3寻址时间为传输时间的1%时，则为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s。4 而目前磁盘的传输速率普遍为100MB/s。 思考：为什么块的大小不能设置太小，也不能设置太大？ 1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； 2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需要的时间。导致程序在处理这块数据时。会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 2.5 HDFS的数据流（面试重点）1、HDFS的写数据流程 1568448314627 然后重复3-7步骤 2、网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 1568448821111 3、副本节点选择 1568449200994 4、HDFS的读数据流程 1568449571543 2.6 NameNode和SecondaryNameNode（面试开发重点）1、NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。 1568450098120 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改。 \\2. 第二阶段：Secondary NameNode工作 ​ （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。 ​ （2）Secondary NameNode请求执行CheckPoint。 ​ （3）NameNode滚动正在写的Edits日志。 ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint。 ​ （7）拷贝fsimage.chkpoint到NameNode。 ​ （8）NameNode将fsimage.chkpoint重新命名成fsimage。 注： NN和2NN工作机制详解： Fsimage：NameNode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。 SecondaryNameNode首先会询问是否需要（触发需要满足两个条件中的任意一个，定时时间到和中数据写满了）。直接带回是否检查结果。执行操作，首先会让滚动并生成一个空的，滚动的目的是给打个标记，以后所有新的操作都写入，其他未合并的和会拷贝到的本地，然后将拷贝的和加载到内存中进行合并，生成，然后将拷贝给，重命名为后替换掉原来的。在启动时就只需要加载之前未合并的和即可，因为合并过的中的元数据信息已经被记录在中。 1568451755272 2.7 DataNode 工作机制 2.7.1DataNode掉线时限参数设置1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信 2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。 3、HDFS默认的超时时长为10分钟+30秒。 4=如果定义超时时间为TimeOut，则超时时长的计算公式为：timeout = 2XXX(分钟)+10\\YYY（秒）。 2.7.2 服役新数据节点、退役旧数据节点1、添加白名单 2、黑名单退役 3、不允许白名单和黑名单中同时出现同一个主机名称。 2.8 HDFS新特性2.8.1 集群间的数据拷贝1．scp实现两个远程主机之间的文件复制 12345scp -r hello.txt [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) // 推 pushscp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pullscp -r [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2．采用distcp命令实现两个Hadoop集群之间的递归数据复制 12[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 2.8.2 小文件存档1、HDFS存储小文件的弊端 大量的小文件会消耗NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量与数据块的大小无关。 2、解决存储小文件的办法之一 HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。也就是说HDFS存档问你件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 3、实操 1）先启动hadoop集群 2）把某目录下的所有文件归档成xx.har的归档文件，并把归档后文件存储到xxx/output路径下。 1[root@master ~]# hadoop archive -archiveName input.har -p /data/input /data/out 3）查看归档 1234[root@master ~]# hadoop fs -ls -R har:///data/out/input.har-rw-r--r-- 3 root supergroup 38 2019-09-15 23:21 har:///data/out/input.har/1-rw-r--r-- 3 root supergroup 26 2019-09-15 23:21 har:///data/out/input.har/2-rw-r--r-- 3 root supergroup 97 2019-09-13 22:54 har:///data/out/input.har/wc.input 4）解归档文件 1hadoop fs -cp har:/// data/output/input.har/* /data 2.8.3 回收站功能参数1、默认值fs.trash.interval = 0, 0表示禁用回收站；其他值表示设置文件的存活时间。 2、默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。 3、要求：fs.trash.checkpoint.interval o.getSumFlow() ? -1 : 1; } 3.6.2 切片与MapTask并行度决定机制1．问题引出 MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。 思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 2．MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。 3.6.3 FileInputFormat切片源码解析：重点： 默认情况：切片大小= blocksize 每次切片时，要判断切完剩余部分是否大于块的1.1倍，不大于1.1倍就划分1块切片 img 切片时不考虑数据集整体，而是逐个对每一个文件单独切片。 3.6.4 FileInputFormat切片大小的参数配置 img 3.6.5 CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。 1、应用场景： CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。 2、虚拟存储切片最大值设置 CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 3、切片机制 生成切片过程包括：虚拟存储过程和切片过程二部分。！！！！ img 3.6 MapReduce详细工作流程 1568639170123 1568639177258 3.7 Shuffle机制3.7.1 Partition分区1、问题引出 要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 2、默认Partitioner分区 Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。如图4-14所示。 img 3.7.2 WritableComparable排序排序概述： ​ 排序是MapReduce框架中最重要的操作之一。 ​ MapTask和ReduceTask均会对数据按照Key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。 ​ 默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 ​ 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 ​ 1、部分排序 ​ MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。 ​ 2、全排序 ​ 最终输出结果只有一个文件，且文件内部有序。实现方法是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。 ​ 3、辅助排序（GroupingComparator分组） ​ 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 ​ 4、二次排序 ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 1、实现全排序 2、实现区内排序 3.7.3 Combiner合并（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。 （2）Combiner组件的父类就是Reducer （3）Combiner和Reducer的区别在于运行的位置 Combiner是在每一个MapTask所在的节点运行 Reducer是接收全局所有Mapper的输出结果 （4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。 （5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且Combiner的输出kv应该跟Reducer的输入kv类型对应起来。 order举例 3.8 MapReduce +shuffle 工作机制（面试） img （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 img （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.9 Join多种应用3.9.1 Reduce Join1、Reduce Join工作原理： ​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 ​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就OK了。 2、缺点及解决方案 ​ 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。 ​ 解决方案：Map端实现数据合并 3.9.2 Map Join 1．使用场景 Map Join适用于一张表十分小、一张表很大的场景。 2．优点 思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？ 在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。 3．具体办法：采用DistributedCache ​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 // 缓存普通文件到Task运行节点。 job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); 3.10 计数器应用hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 1、计数器API （1）采用枚举的方式统计计数 （2）采用计数器组、计数器名称的方式统计 （3）计数结果在程序运行后的控制台上查看。 3.11 数据清洗（ETL）3.12 Hadoop数据压缩3.12.1 压缩概念压缩概述： 压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。 数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启动压缩。 压缩策略和原则 优化策略 通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。 注意：采用压缩技术减少了磁盘IO，但同时也增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。 压缩原则： 1、运算密集型的job，少用压缩 2、IO密集型的job，多用压缩 3.12.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 3.12.3 压缩方式选择Gzip压缩： 优点：压缩率比较高，压缩/解压速度快；hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后130M以内的（1个块大小内），都可以考虑Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。 Bzip2压缩： 优点：支持split，具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便。 缺点：压缩/解压速度慢。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序的情况。 Lzo 优点：压缩/解压速度也比较快，合理的压缩率，支持split，是Hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定InputFormat为Lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越明显。 Snappy 优点：高速压缩速度和合理的压缩率。 缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装。 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另一个MapReduce作业的输入。 3.12.4 压缩位置的选择 img 5、Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 img 工作机制详解 ​ （1）MR程序提交到客户端所在的节点。 ​ （2）YarnRunner向ResourceManager申请一个Application。 ​ （3）RM将该应用程序的资源路径返回给YarnRunner。 ​ （4）该程序将运行所需资源提交到HDFS上。 ​ （5）程序资源提交完毕后，申请运行mrAppMaster。 ​ （6）RM将用户的请求初始化成一个Task。 ​ （7）其中一个NodeManager领取到Task任务。 ​ （8）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （9）Container从HDFS上拷贝资源到本地。 ​ （10）MRAppmaster向RM 申请运行MapTask资源。 ​ （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。 ​ （13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。 ​ （14）ReduceTask向MapTask获取相应分区的数据。 ​ （15）程序运行完毕后，MR会向RM申请注销自己。 img 6、优化6.1 MapReduce优化MapReduce程序效率的瓶颈在于两点： 1、计算机性能 ​ CPU、内存、磁盘健康、网络 2、I/O操作优化 （1）数据倾斜 （2）Map和Reduce数设置不合理 （3）Map运行时间太长，导致Reduce等待过久 （4）小文件过多 （5）大量的不可分块超大文件 （6）Spill次数过多 （7）Merge次数过多等。 优化方法： 1、数据输入 （1）合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 2、Map阶段 （1）减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO. （2）减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 （3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O。 3、Reduce阶段 （1）合理设置Map和Reduce数。 （2）设置Map、Reduce共存。调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。 （3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 （4）合理设置Reduce端的Buffer。 4、I/O传输 （1）采用数压缩的方式 （2）使用SequenceFile二进制文件 5、数据倾斜问题 1）数据倾斜现象 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 2）减少数据倾斜的方法 1）抽样和范围分区 2）自定义分区 3）Combine 4）采用Map Join， 尽量避免Reduce Join。 6.2HDFS小文件优化6.2.1 HDFS小文件弊端HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。 6.2.2 HDFS小文件解决方案小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。 （2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。 （3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。 1、MAVEN内存溢出!（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,nativeN -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 DataNode和NameNode进程同时只能工作一个。 clusterId不统一 10）执行命令不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 11）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 12）jps不生效。 原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。 13）8088端口连接不上 [atguigu@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102 ​ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"vue项目创建过程","date":"2019-08-08T03:08:59.729Z","path":"/posts/0/","text":"1、Vue 项目创建过程1.1项目搭建过程 创建项目 1# vue init webpack projectname 运行项目 1# npm run dev 1.2 遇到的问题解决方法新手创建时死机是经常出现的事，解决办法如下： 1231、清除缓存 npm clean --force （可能是这个，不太记得了）2、更新 npm install3、chromedriver无法下载，应该是谷歌退出中国网站不让访问，只能下载本地的网址。 1.3 项目前期准备举个栗子： 1.3.1 新建前端界面前端页面开发页面组件存放于src\\components文件夹中，新建一个Login组件。 1234567891011121314151617181920212223242526272829303132333435363738394041 用户名: 密码： 登录 export default { name: 'Login', data () { return { loginForm: { username: '', password: '' }, responseResult: [] } }, methods: { login () { this.$axios .post('/login', { username: this.loginForm.username, password: this.loginForm.password }) .then(successResponse => { if (successResponse.data.code === 200) { this.$router.replace({path: '/index'}) } }) .catch(failResponse => { }) } } } 1.3.2 前端相关配置——设置反向代理修改 src\\main.js 代码如下： 1234567891011121314151617import Vue from 'vue'import App from './App'import router from './router'// 设置反向代理，前端请求默认发送到 http://localhost:8443/apivar axios = require('axios')axios.defaults.baseURL = 'http://localhost:8443/api'// 全局注册，之后可在其他组件中通过 this.$axios 发送数据Vue.prototype.$axios = axiosVue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: ''}) 1.3.3 前端相关配置——设置页面路由修改 src\\router\\index.js 代码如下： 1234567891011121314151617import Vue from 'vue'import Router from 'vue-router'// 导入刚才编写的组件import Login from '@/components/Login'Vue.use(Router)export default new Router({ routes: [ // 下面都是固定的写法 { path: '/login', name: 'Login', component: Login } ]}) 1.3.4 前端相关配置——跨域支持为了让后端能够访问到前端的资源，需要配置跨域支持。 在 config\\index.js 中，找到 proxyTable 位置，修改为以下内容 123456789proxyTable: { '/api': { target: 'http://localhost:8443', changeOrigin: true, pathRewrite: { '^/api': '' } }} 1.4 各种 npm 安装1.4.1 element安装与配置1、安装命令 1npm i element-ui -S 2、main.js 中引入和使用ElementUI 1234import ElementUI from 'element-ui'import 'element-ui/lib/theme-chalk/index.css'Vue.use(ElementUI) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"git上传仓库步骤","date":"2019-07-15T09:33:15.538Z","path":"/posts/0/","text":"git上传仓库步骤有关git上传仓库步骤，请享用。 1、新建仓库 2、git config –global user.name “xxx” 3、git config –global user.email “xxx” 进入文件夹后初始化 git init git add . git pull –rebase 地址 //该步骤是复制之前的master，使得版本代码相同!! git commit -m “xxx1.version” git push 地址 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"hbase笔记","date":"2019-05-09T07:36:00.000Z","path":"/posts/346bfea7/","text":"通用命令 status: 提供HBase的状态，例如，服务器的数量。 version: 提供正在使用HBase版本。 table_help: 表引用命令提供帮助。 whoami: 提供有关用户的信息。 数据定义语言（表操作命令）。 create: 创建一个表。 list: 列出HBase的所有表。 disable: 禁用表。 is_disabled: 验证表是否被禁用。 enable: 启用一个表。 is_enabled: 验证表是否已启用。 describe: 提供了一个表的描述。 alter: 改变一个表。 exists: 验证表是否存在。 drop: 从HBase中删除表。 drop_all: 丢弃在命令中给出匹配“regex”的表。 Java Admin API: 在此之前所有的上述命令，Java提供了一个通过API编程来管理实现DDL功能。在这个org.apache.hadoop.hbase.client包中有HBaseAdmin和HTableDescriptor 这两个重要的类提供DDL功能。 数据操纵语言 put: 把指定列在指定的行中单元格的值在一个特定的表。 get: 取行或单元格的内容。 delete: 删除表中的单元格值。 deleteall: 删除给定行的所有单元格。 scan: 扫描并返回表数据。 count: 计数并返回表中的行的数目。 truncate: 禁用，删除和重新创建一个指定的表。 Java client API: 在此之前所有上述命令，Java提供了一个客户端API来实现DML功能，CRUD（创建检索更新删除）操作更多的是通过编程，在org.apache.hadoop.hbase.client包下。 在此包HTable 的 Put和Get是重要的类。 创建样本模式 列族 创建表create 'emp', 'personal data', 'perfessional data' 更改列族单元格的最大数目hbase> alter 't1', NAME => 'f1', VERSIONS => 5 表范围运算符 使用alter，可以设置和删除表范围，运算符，如MAX_FILESIZE，READONLY，MEMSTORE_FLUSHSIZE，DEFERRED_LOG_FLUSH等。 #设置只读 hbase>alter 't1', READONLY(option) alter ‘emp’, READONLYalter ‘t1’, METHOD => ‘table_att_unset’, NAME => ‘MAX_FILESIZE’ drop 在删除一个表之前必须先将其禁用。hbase(main):018:0> disable 'emp' hbase(main):019:0> drop 'emp' HBase创建数据put 命令, add() - Put类的方法 put() - HTable 类的方法 使用put命令，可以插入行到一个表。它的语法如下： put '', 'row1', '','' 插入第一行 put 'emp', '1','personal data:name','xifu' put 'emp', '1', 'personal data:city','taizhou' put 'emp', '1', 'perfessional data:designation','manager' put 'emp', '1', 'perfessional data:salary','50000' document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"python爬虫笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/65abd992/","text":"一、爬虫如何抓取网页数据网页三大特征：1、网页都有自己的唯一的URL（统一资源定位符）来进行定位。2、网页都使用HTML(超文本标记语言)来描述页面信息。3、网页都使用HTTP/HTTPS(超文本传输协议)协议来传输HTML数据。 爬虫的设计思路1、首先确定需要爬取的网页URL地址。2、通过HTTP/HTTP协议协议来获取对应的HTML页面。3、提取HTML页面里有用的数据： a.如果是需要的数据，就保存起来。 b.如果是页面里的其他URL，那就继续执行第二步。 #为什么选择Python做爬虫？PHP：是世界上最好的语言，但他天生不是干爬虫的，对多线程，异步支持不够好。爬虫是工具性程序，对速度和效率的要求比较高。Java爬虫生态圈很完善，是Python爬虫最大的对手。但是Java语言本身很笨重，代码量很大。爬虫经常需要修改部分采集代码，所以Java不合适。C/C++运行效率和性能几乎最强，但是学习成本很高。代码成型比较慢。能用C/C++做爬虫，只能说是能力的表现，但不是正确的选择。Python语法优美、代码简介、开发效率高、支持的模块多，相关HTTP请求模块还有强大的爬虫Scrapy，以及成熟高效的scrapy-redis分布式策略。而且，调用其他接口也非常方便（胶水语言） 二、如何抓取HTML页面：HTTP请求的处理，urllib、urllib2、requests 处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件 #解析服务器响应的内容 re、xpath、BeautifulSoup4（bs4）、jsonpath、pyquery等 使用某种描述性一样来给我们需要提取的数据定义一个匹配规则 符合这个规则的数据就会被匹配 #如何采集动态HTML、验证码的处理 通过动态页面采集，Selenium+PhantomJS(无界面)：模拟真实浏览器加载js、ajax等非静态的数据。 Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入/专门的打码平台。 sccrapy框架：（Scrapy，Pyspider） 搞定制性高性能（异步网络框架 twisterd），所以数据下载速度非常快，提供了数据存储、数据下载、提取规则等组件。 分布式策略：scrapy-redis，在Scrapy的基础上添加了一套以Redis数据库为核心的一套组件。让Scrapy框架支持分布式的功能。主要在Redis里做请求指纹去重、请求分配、数据临时存储。 三、爬虫–反爬虫–反反爬虫 之间的斗争：其实怕重做到最后，最头疼的不是复杂的页面，也是灰色的数据，而是网站另一边的反爬虫人员。 User-Agent、代理、验证码、动态数据加载、加密数据。 数据价值、是否值的去费劲做反爬虫。 1. 机器成本 + 人力成本 > 数据价值，就不反了，一般做到封IP就结束了。 2、面子的战争...... 爬虫和反爬虫之间的斗争，最后一定是爬虫获胜。 为什么？只要是真实用户可以浏览的网页数据，爬虫就一定能爬下来！ #根据使用场景 分为 ：通用爬虫 聚焦爬虫1通用爬虫：搜索引擎用的爬虫系统。1、目标：就是尽可能吧互联网上所有的网页下载下来，放到本地服务器里形成备份； 再对这些网页做相关处理（提取关键字、去掉广告），最后提供一个用户检索接口。2、抓取流程： a） 首选选取一部分已有的URL，把这些URL放到待爬取队列。 b） 从队列里取出这些URL，然后解析DNS得到主机IP，然后去这个IP对应的服务器里下载HTML页面，保存到搜索引擎的本地服务器。之后把这个爬过的URL放入已爬取队列。 c）分析这些网页内容，找出网页里其他的URL链接，继续执行第二步，直到爬取条件结束。3、搜索引擎如何获取一个新网站的URL: A).主动向搜索引擎提交网址, B).向其他网站里设置网站的外链。 C).搜索引擎会和DNS服务商进行合作，可以快速收录新的网站。 DNS：就是把域名解析成IP的一种技术。4、通用爬虫并不是万物皆可爬，它也需要遵守规则：Robots协议：协议会指明通用爬虫可以爬取网页的权限。Robots.txt 只是一个建议。并不是所有爬虫都遵守，一般只有大型的搜索引擎爬虫才会遵守。咱们个人写的爬虫，就不用管了。5、通用爬虫工作流程：爬取网页 - 存储数据 - 内容处理 - 提供检索/排名服务6、搜索引擎排名： ·PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站越值钱，排名越靠前。 ·竞价排名：谁给钱多，谁排名就高。7、通用爬虫的缺点： 1、只能提供和文本相关的内容（HTML、Word、PDF）等等，但是不能提供多媒体（音乐、图片、视频）和二进制文件（程序、脚本）等。 2、提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。 3、不能理解人类语义上的检索。 2聚焦爬虫：爬虫程序员写的针对某种内容爬虫。面向主题爬虫、面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求息息相关。 http的端口号：80；https的端口是：443； Python自带的模块：/usr/lib/python2.7/urllib2.py Python的第三方模块： /usr/local/lib/python2.7/site-packages urllib2 默认的 User-Agent：Python-urllib/2.7 User-Agent: 是爬虫和反爬虫斗争的第一步，养成好习惯，发送请求带User-Agent response 是服务器响应的类文件，除了支持文件操作的方法外，还支持以下常用的方法： 返回 HTTP的响应码，成功返回200，4服务器页面出错，5服务器问题 print response.getcode() 返回 返回实际数据的实际URL，防止重定向问题print response.geturl() 返回 服务器响应的HTTP报头print response.info() 四、User-Agent 历史： Mosaic 世界上第一个浏览器：美国国家计算机应用中心 Netscape 网景：Netscape（支持框架），慢慢开始流行….(第一款支持框架的浏览器) Microsoft 微软：Internet Explorer（也支持框架） 第一次浏览器大战：网景公司失败..消失 Mozilla 基金组织：Firefox 火狐 - （Gecko内核）(第一款浏览器内核) User-Agent 决定用户的浏览器，为了获取更好的HTML页面效果。 IE开了个好头，大家都开就给自己披着了个 Mozilla 的外皮 Microsoft公司：IE（Trident） Opera公司：Opera（Presto） Mozilla基金会：Firefox（Gecko） Linux组织：KHTML （like Gecko） Apple公司：Webkit（like KHTML） Google公司：Chrome（like webkit） 其他浏览器都是IE/Chrome内核 五、 Scrapy架构图（绿线是数据流向）：ScrapyEngine（引擎）：负责通讯，信号、数据传递 制作Scrapy爬虫 四步骤： 新建项目（scrapy startproject xxx）:新建一个新的爬虫项目 明确目标（编写items.py）：明确你想要爬取的目标 制作爬虫（spiders/xxspider.py）：制作爬虫开始爬取网页 存储内容（pipelines.py）：设计管道存储爬取内容 创建爬虫项目1234#创建普通项目scrapy startproject 项目名#创建模板scrapy startproject 项目名 网站名 六、正则表达式1、规范 表达式 描述 . 除了\\n和\\r的所有字符 \\d 数字 \\D 非数字 \\w 字母和下划线 \\W 非字母和下划线 \\s 空格（包括制表符、换页符等） [a-z] 小写英文字母 [a-zA-Z0-9] 大小写英文字母与数字 [123] 数字123 [^123] 不是数字123 * 出现次数>=0 + 出现次数>=1 {n} 出现次数=n {n,m} m>=出现次数>=n ^ 以开头 $ 以结尾 ? 关闭贪婪模式 () 用于获取括号内匹配成功的字符串 2、匹配div标签要取class=“class1”中的文本内容 1要匹配的内容 正则表达 div_pattern1=’ { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"python笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/c00b24b1/","text":"一、基本语法1、ipython命令技能实现python命令，也能实现部分linux命令。2、python2不能识别文件里的中文，如果硬要识别，在头文件中加上 -- coding:utf-8 -- （python官方推荐这种方式）3、输入一个数 1high = input(\"请输入一个数\") 4、输出一个数 12345age = 18print(\"age的值为%d\"%age)name = \"西芙\"print(\"name的值为%s\"%name);print(\"name的值是%s,年龄的值是%d\"%(name,age)) 4、if else语句12345678910age = 19if age>18: print(\"已经成年\")else print(\"未成年\")#input获取的所有数据都会当成字符串类型#python 规定 str（）>int()#所以如果 age = input(\"一个值\")#要与int比较，需要把age换成int类型，即#age_num = int(age) 5、python语言是弱类型的编程语言，及赋值的时候已经知道是什么类型6、查看变量类型 1type(a) 7、python的变量类型123456·Numbers(数字)： int(有符号整型)，long（长整型[也可代表八进制和十六进制]），float（浮点型），complex（复数）·布尔类型： True，False·String（字符串）·List（列表）·Tuple（元组）·Dictionary（字典） 8、关键字查看关键字命令 12import keyword #导入keyword包keyword.kwlist 9、运算符 12345\"s\"*10#输出'ssssssssss'#幂#** 10、is是地址相同的意思，在数值-5到256间，a与b地址相同，其余不同￼11、深拷贝与浅拷贝123456789101112131415161718192021222324252627282930313233#例1、浅拷贝a = [11, 22, 33]b = a #浅拷贝，地址拷贝，地址相同#例2、深拷贝import copyc = copy.deepcopy(a) #开辟一个内存空间，拷贝内容#例3、在数组中，深拷贝数组后，a = [11,22,33]b = [44,55,66]c = [a, b]d = copy.deepcopy[c]#查看c，d的id，并不相同，深拷贝#在a数组中添加[44],查看c[0],d[0]a.append(44)c[0][11,22,33,44]d[0][11,22,33]#说明深拷贝后，d另外拷贝了有关a、b的拷贝#例4、有关指向a，b的拷贝e = copy.copy(c)a.append(55)c[0]e[0]#所看到的结果都是[11,22,33,44,55]id(c)id(e)#所看到的地址不同，所以他们是指向不同的地址但所指向数组的内容地址是相同的#但如果是c = (a,b)，是元组的话，由于元组是不可变类型，e = copy.copy(c),他们所指向的地址是一样的#所以使用copy模块的copy功能时候，它会根据当前拷贝的数据类型是可变类型还是不可变类型有不同的处理方式 12、私有化12345678910class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__numt = Test()t.__num = 200 #可以使用print(t.__num) 升级版私有化 property使用123456789class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__num num = property(getNum, setNum)#!!!!#此时，可以用t.num = ?赋值 或者 t.num 取值，不用调用函数 装饰器使用12345678910class Test(object): def __init__(self): self.__num = 100 @property def num(self): return self.__num @num.setter def num(self, newNum): self.__num = newNum#与上面的代码意思相同。理解一下吧~ 1、xx:公有变量2、x:单前置下划线，私有化属性或方法，from somemodule import * 禁止导入，类对象和子类可以访问3、__x:双前置下划线，避免与子类中的属性命名冲突，无法在外部直接访问（名字重整所以访问不到）4、x：双前后下划线，用户名字空间的魔法对象或属性。例如init,__不要自己发明这样的名字5、xx_:单后置下划线，用于避免与python关键词的冲突 ###但是私有属性实际上是可以访问，python把私有属性的名字由原来的_私有属性名 改成 _类名私有属性名，所以可以通过对象._类名__私有属性名可以获取值！！！ 13、迭代器迭代器 迭代器是访问集合元素的一种方式，迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，知道所有的元素被访问完结束。迭代器只能往前不会后退。可迭代对象： 可直接作用于for循环的数据类型有以下几种： 一类是集合数据类型：如list、tuple、dict、set、str等 一类是generator，包括生成器和带yield的generator function。 可以直接作用于for循环的对象统称为可迭代对象：Iterable。 14、闭包：定义：函数里面有另外的函数，并且里面的函数用到了外面的函数的变量闭包的应用：1234567891011121314def test(number): print(\"--1--\") def test_in(number2): print(\"--2--\") print(number+number2) print(\"--3--\") return test_in ret = test(100)print(\"-\"*30)ret(1)ret(100)ret(200)#优点 简化了步骤 linux快捷键： %s/^/#/g 所有行前面加# 1,14/#//g 去掉1-14行的#15、装饰器12345678910111213141516171819def w1(func): def inner(): print(\"----正在验证权限---\") func() return innerdef f1(): print(\"---f1---\")def f2(): print(\"---f2---\")#innerFunc = w1(f1)#innerFunc()f1 = w1(f1) #把w1（f1）赋值给f1，也就是说f1 = w1.inner，f1指向的就是inner这个函数f1() #调用后，执行的是inner，但由于之前传入f1的参数，所以输出结果为inner的输出结果+f1函数的输出结果@w1 #与上面f1=w1(f1)意义相同，但只要python执行器执行到了这个代码，name就会自动的进行装饰，而不是等到调用的时候才装饰的 语法糖 @w1def f1(): print(\"---f1---\") 12345678910111213141516171819#若有参数def func(functionName): print(\"---func---1---\") def func_in(*args, **kwargs):#如果没有定义参数，会导致调用的时候出现问题 print(\"---func_in---1---\") functionName(*args, **kwargs)#传入同样的参数 print(\"---func_in---2---\") print(\"---func---2---\") return func_in@funcdef test(a, b, c): print(\"---test-a=%d,b=%d,c=%d---\"%(a,b,c)) @funcdef test2(a, b, c, d): print(\"---test-a=%d,b=%d,c=%d,d=%d---\"%(a,b,c,d))test(11,22,33)test2(44,55,66,77) 12345678910111213141516171819202122232425262728#通用装饰器def func(functionName): def func_in(*args, **kwargs): print(\"---记录日志---\") ret = functionName(*args, **kwargs) return ret return func_in@funcdef test(): print(\"---test---\") return \"haha\"@funcdef test2(): print(\"---test2---\")@funcdef test3(a): print(\"---test3---a=%d--\"%a) ret = test()print(\"test return value is %s\"%ret)a = test2()print(\"test2 return value is %s\"%a)test3(11) 16、作用域 #什么是命名空间在某个范围内所能用到的作用域 17、类方法与静态方法1234567891011121314151617#给类添加方法import types #导入types包class Person(object): def __init__(self, newName, newAge): self.name = newName self.age = newAge def eat(self): print(\"-----%s正在吃----\"%self.name)def run(self): print(\"-----%s正在跑----\"%self.name) p1 = Person(\"p1\", 10)p1.eat()p1.run = types.MethodType(run, p1)p1.run() 18、限制class实例添加的属性123456class Person(object): __slots__ = (\"name\", \"age\")p = Person()p.name = \"老王\"p.age = 20p.score = 100 #会报错！！！ 二、生成器、迭代器、装饰器、闭包19、生成器 12345678910In [1]: a = [x*2 for x in range(10)]In [2]: aOut[2]: [0, 2, 4, 5, 6, 10, 12, 14, 16, 18]#如果用[]，会生成数组，同时加载到内存中#如果用括号（），输出的是一个地址，需要时才提取出In [3]: b = (x*2 for x in range(10))In [4]: bOut[4]: In [5]:next(b)Out[5]:0 20、a，b交换123456789101112131415def creatNum(): print(\"----start----\") a, b = 0, 1 for i in range(5): print(\"----1----\") yield b #python中有一个非常有用的语法叫做生成器，所利用到的关键字就是yield。有效利用生成器这个工具可以有效地节约系统资源，避免不必要的内存占用。当执行到这的时候，会做停顿。 print(\"----2----\") a, b = b, a+b print(\"----3----\") print(\"----stop----\")#创建了一个生成器对象a = creatNum()#实现交换a, b = 0, 1a, b = b, a 21、斐波拉契数列12a,b = 0, 1a,b = b, a+b #重复此步骤 22、类当做装饰器12345678910111213141516171819202122232425262728#1class Test(object): def __call__(self): print(\"---test---\") t = Test()t()#out: ---test---#2class Test(object): def __init__(self, func): print('---初始化---') print('func name is %s'func.__name__) self.__func = func def __call__(self): print('---装饰器中的功能---') self.__func()@Test #相当于 t = Test(test) def test(): print('---test---')#out: ---初始化---#out:func name is 'test'test()#out: ---装饰器中的功能#out:---test--- 23、元类1234567891011Test = type(\"Test\", (), {})#相当于创建一个类Test#out:__main.Test#创建一个拥有属性num=0的Person类Person = type(\"Person\",(),{\"num\":0})#创建一个拥有方法printNum()的Test3类def printNum(self): print(\"--num-%d--\"%self.num)Test3 = type(\"Test3\",(),{\"printNum\":printNum}) 123456789101112131415161718192021222324#设置Foo类的属性变大写！！，通过metaclass设置def upper_attr(future_class_name, future_class_parents, future_class_attr): #Foo object {bar:\"bip\"} #遍历属性字典，把不是__开头的属性名字变成大写 newAttr = {} for name, value in future_class_attr.items(): if not name.startswith(\"__\"): newAttr[name.upper()] = value #调用type创建一个类 return type(future_class_name,future_class_parents,newAttr)class Foo(object,metaclass=upper_attr): # __metaclass__ = upper_attr #设置Foo类的元类为upper_attr python2 bar = \"bip\"print(hasattr(Foo, \"bar\"))print(hasattr(Foo, \"BAR\"))f = Foo()print(f.BAR)#out:False#out:True#out:bip 24、内建属性123456789101112131415161718class Itcast(object): def __init__(self,subject1): self.subject1 = subject1 self.subject2 = 'cpp' #属性访问时拦截器，打log def __getattribute__(self,obj): #obj-->\"subject1\" if obj == 'subject1': print('log subject1') return 'redirect python' else: return object__getattribute__(self,obj) def show(self): print('this is Itcast') s = Itcast(\"python\")print(s.subject1)print(s.subject2) 三、内建方法25、lambda之map123456789101112131415#函数需要一个参数map(lambda x: x*x, [1, 2,3])#结果为：[1, 4, 9]#函数需要两个参数map(lambda x, y: x+y, [1, 2, 3], [4, 5, 6])#结果为:[5,7,9]def f1(x, y): return (x, y)l1 = [0,1,2,3,4,5,6]l2 = ['Sun', 'M', 'T', 'W', 'T', 'F', 'S']l3 = map(f1, l1, l2)print(list(l3))结果为：[(0,'Sun'), (1,'M').....] 26、lambda之filter12345fliter(lambda x : x%2, [1, 2 ,3, 4]) #如果为1，则输出[1, 3]filter(None, \"she\")'she' 27、lambda之reduce12345678reduce(lambda x, y: x+y, [1,2,3,4])10 #先把1赋值给x，2赋值给2，x+y后=3，把3赋值给x，数组中的3赋值给y，再累加，以此类推reduce(lambda x, y:x+y, [1,2,3,4], 5)15 #如果前面是数组，先把5赋值给x，再累加[1,,2,3,4]reduce(lambda x, y: x+y, ['aa', 'bb', 'cc'], 'dd')ddaabbcc 28、sort123456789a = [9,8,7,6,5,4,3,2,1]a.sort()a#[1,2,3,4,5,6,7,8,9]b = ['dd','cc','bb','aa']b.sort()b#['aa','bb','cc','dd'] 28、python的functools包中提供了一个叫wraps的装饰器来消除这样的副作用1234567891011121314151617import functoolsdef note(func): \"note function\" @functoolswraps(func) def wrapper(): \"wrapper function\" print(\"note something\") return func() return wrapper@notedef test(): \"test function\" print('I am test')test()print(test.__doc__) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Linux复习","date":"2019-03-28T06:34:00.000Z","path":"/posts/0/","text":"Linux一、Linux起源Linux系统诞生于1991年，由芬兰大学生李纳斯（Linux Torvalds）。 二、命令2.1文件处理命令 ls （-lh...） 查看当前目录 -rw-r--r-- 1 root root 8690 3月 1 13:23 install.log.syslog 1是引用计数（代表此文件被调用几次 引用几次） 文件所有者 文件所属组 字节文件大小 最后修改时间 文件名称 -rw-r--r-- - 文件类型（- 二进制文件 d目录 l软连接文件） rw- r-- r-- u g o u所有者 g所属组 o其他人 r读 w写 x执行 cd 进入目录 pwd 显示当前绝对路径 mkdir 2.2网络通信命令 ping 测试网络连接 ifconfig 显示或配置网络设备（网络接口卡）的命令 2.3系统命令 shutdown 关机 reboot 重启系统 2.3文本编辑器 vim/vi 三、虚拟机安装 CPU：建议主频为1GHz以上 内存：建议1GB以上 硬盘：建议分区空闲空间8GB以上 3.1Linux分区 boot：400MB home 分区： 4G swap交换分区：2G 没有挂载点，就是没有盘符，swap不是给用户用的，是给操作系统或内科自己调用的 设置为2G 超过两个G就没有意义了 逻辑分区 / 剩余所有空间：第四个分区，linux不知道要分几个区，所有先创建一个sda5（第一个逻辑分区），一块硬盘只能有4个分区是第四个默认为逻辑分区，三个主分区，一个逻辑分区 3.2系统磁盘分区 基本分区（primary partion） 扩充分区（extension partion） 基本分区和扩充分区的数目之和不能大于四个。且基本分区可以马上被使用单不能再分区。扩充分区必须再进行分区后才能使用，也就是说它必须还要进行二次分区。扩充分区下面是逻辑分区（logical partion），逻辑分区没有数量上的限制。 主分区：最多只能有4个。 扩展分区： 最多只能有1个。 主分区加扩展分区最多有4个。 不能写入数据，只能包含逻辑分区 3.3分区格式化目的：为了给这个分区写入文件系统，也就是告诉我们的操作系统，如何去管理这个分区的数据。 分区：把大硬盘分为小的逻辑分区 格式化：写入文件系统 分区设备文件名：给每个分区定义设备文件名 挂载：给每个分区分配挂载点 四、虚拟机网络连接方式 桥接模式 NAT模式 仅主机模式 自定义模式 4.1桥接模式 你的虚拟机是利用你的真实网卡，一块本地有线网卡(以太网)，一块是本地无线网卡（wlan）装完虚拟机后会出现两块虚拟网卡，一块vmnet1，一块vmnet8，如何选择桥接你的虚拟机将会利用你的网卡以太网或无线网卡和你的真实机进行通信 好处是配置简单，和你的真是机的ip地址设置同一网段，和真机进行通信，局域网的其他电脑也可以通信。 缺点占用同意网段的一个ip地址，在家里宿舍没有问题，在教室可能会和其他同学的ip地址冲突。 4.2NAT模式 你的虚拟机是通过VMnet8这块假的虚拟网卡和你的真实机连接 4.3仅主机模式 你的虚拟机是通过VMnet1这块假的虚拟网卡和你的真实机连接。 桥接是不仅和你的真机通信还可以与你的局域网的其他机器通信，和一台笔记本是一个意思。Nat hostonly 只能和你真机通信，不可以和局域网其他电脑通信，不用占用你的真实网段的一个ip地址。Hostonly 只能和你计算机主机通信。NAT和主机通信，如果你的主机可以联网，虚拟机也可以联网 4.4df盘符分区命令df （-h）文件系统 1k-块 已用 可用 已用% 挂载点 五、文件处理命令5.1ls 查看目录下的文件 -a 显示所有文件，包括隐藏文件 -l 详细信息显示 -d 查看目录属性 -i Inode索引节点 5.2mkdir 创建目录 -p 创建递归 5.3cd 切换目录5.4pwd 显示当前目录5.5rmdir 删除空目录5.6cp 复制文件或目录 -rp 【源文件或目录】 【目标目录】 -r 复制目录 -p 保留文件属性 5.7clear 清屏操作5.8mv 剪切文件、改名5.9rm 删除文件-r 删除目录 -f 强制执行 5.10touch 创建空文件5.11cat 显示文件内容-n 显示行号 5.12more 分页显示文件内容(空格)或f 翻页 (Enter) 换行 q或Q 退出 5.13less 分页显示文件内容5.14ln 生成链接文件 -s创建软连接 软链接特征：类似windows快捷方式 lrwxrwxrwxl软链接 文件大小——只是符号链接 /tmp/issue.soft -> /etc/issue 箭头指向源文件 硬链接特征： 1、拷贝cp -p + 同步更新 2、通过i节点识别 3、不能跨分区 4、不能针对目录使用 5.15shutdown命令 -c 取消前一个关机命令 -h 关机 -r 重启 5.16vi/vim 建立、编辑、显示文本文件 vim是一个功能强大的全屏幕文本编辑器，是Linux/UNIX上最常用的文本编辑器，它的作用是建立、编辑、显示文本文件。 vim没有菜单，只有命令。 插入命令 命令 作用 a 在光标所在字符后插入 A 在光标所在行尾插入 i 在光标所在字符前插入 I 在光标所在行行首插入 o 在光标下插入新行 O 在光标上插入新行 定位命令 命令 作用 :set nu 设置行号 :set nonu 取消行号 gg 到第一行 G 到最后一行 nG 到第n行 :n 到第n行 $ 移至行尾 0 移至行首 删除命令 命令 作用 x 删除光标所在处字符 nx 删除光标所在处后n个字符 dd 删除光标所在行，ndd删除n行 dG 删除光标所在行到文件末尾内容 D 删除光标所在处到行尾内容 n1,n2d 删除指定范围的行 复制和剪切命令 命令 作用 yy 复制当前行 nyy 复制当前行以下n行 dd 剪切当前行 ndd 剪切当前行以下n行 p、P 粘贴在当前光标所在行下或行上 搜索和搜索替换命令 命令 作用 /string 搜索指定字符串 搜索时忽略大小写：set ic n 搜索指定字符串的下一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/g 在一定范围内替换指定字符串 保存和退出命令 命令 作用 :w 保存修改 :w new_filename 另存为指定文件 :wq 保存修改并退出 zz 快捷键，保存修改并退出 :q! 不保存修改退出 :wq! 保存修改并退出（文件所有者及root可使用） 5.17 sort排序命令 sort [选项] 文件名 -f 忽略大小写 -n 以数值型进行排序 -r 反向排序 -t 指定分隔符，默认是分割符是制表符 -k n[,m] 按照指定的字段范围排序，从第n个开始，m字段结束（默认到行尾） 5.18 统计命令 wc wc [选项] 文件名 -l 只统计行数 -w 只统计单词数 -m 只统计字符数 5.19 awk grep sed grep 更适合单纯的查找或匹配文本 sed 更适合编辑匹配到的文本 awk 更适合格式化文本，对文本进行较复杂的格式处理（算一门编程语言） 六、权限管理命令6.1chmod 改变文件或目录（权限） -R 递归修改 权限的数字表示 r——4 w——2 x——1 文件目录权限总结 代表字符 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以在目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 6.2chown 改变文件或目录的（所有者）6.3chgrp 改变文件或目录的（所属组）chgrp 【用户组】【文件或目录】 6.4Groupadd 所有组名称新的命令6.5umask 显示、设置文件的缺省权限 umask -S 以rwx形式显示新建文件缺省权限 在linux创建文件默认权限没有x权限，防止病毒可执行脚本 在linux创建目录默认权限有x 七、文件搜索命令7.1find 【搜索范围】【匹配条件】尽量不要使用linux里的find命令浪费资源特别是使用高峰期的时候，目录和文件合理的命名存放位置最重要。 Find 路径 选项 关键字 完全搜索 Find / -name a 模糊搜索 Find / -name a 模糊搜索 Find / -name *a?? 模糊搜索 Find / -iname *a?? （iname不区分大小写） Find -size +n -n n 查询文件大小 (+大于 -小于 什么都不写 =) Find -user dj 根据所有者查找文件 find /etc -cmin -5 在/etc下查找5分钟内被修改过属性的文件和目录（+超过多长时间 -在多少分钟内） amin 访问时间 access cmin 文件属性 change mmin 文件内容 modify 八、用户管理命令 简介：所以越是对服务器安全性要求高的服务器，越需要建立合理的用户权限登记制度和服务器操作规范。在linux中主要是通过用户配置文件来查看好修改用户信息。1、添加更多的用户分配不同的权限。2、通过配置文件添加信息，使用命令重启可能就消失了。 8.1用户配置文件——用户用户信息文件 /etc/passwd 第1字段：用户名称第2字段：密码标志第3阶段：UID（用户ID） 0： 超级用户 1-499： 系统用户（伪用户） 500-65535： 普通用户第4字段：GID（用户初始组ID）第5字段：用户说明第6字段：家目录 普通用户：/home/用户名/ 超级用户：/root/第7字段：登录之后的Shell 初始组和附加组 初始组：就是指用户一登录就立刻拥有这个用户组的相关权限，每个用户的初始组只能有一个，一般就是和这个用户名相同的组名作为这个用户的初始组。 附加组：指用户可以加入多个其他的用户组，并拥有这些组的权限，附加组可以有多个。 shell是什么？ 1、shell就是Linux的命令解释器2、在/etc/passwd当中，除了标准shell是/bin/bash之外，还可以写如/sbin/nologin。 8.2用户配置文件——影子1、影子文件/etc/shadow 第1字段：用户名 第2字段：加密密码 加密算法升级为SHA512散列加密算法 如果密码位是“!!”或“*”代表没有密码，不能登录 第3字段：密码最后一次修改日期 使用1970年1月1日作为标准时间，每过一天时间戳加1 第4字段：两次密码的修改间隔时间（和第三字段相比） # 0代表随时可以修改密码，10代表10天后才能修改密码 第5字段：密码有效期（和第3字段相比） 第6字段：密码修改到期前的警告天数（和第5字段相比） 第7字段：密码过期后的宽限天数（和第5字段相比） 0：代表密码过期后立即失效 -1：则代表密码永远不会失效。 第8字段：账号失效时间 （到了不管过期 6 7） 要用时间戳表示 第9阶段：保留 2、时间戳换算 把时间戳换算为日期 >> date -d \"1970-01-01 16066 days\" 把日期换算为时间戳 >> echo $(($(date --date=\"2014/01/06\" +%s)/86400 +1)) 8.3用户配置文件——组件1、组信息文件/etc/group 第1字段：组别 第2字段：组密码标志 第3阶段：GID 第4字段：组中附加用户 2、组密码文件/etc/gshadow 第1字段：组别 第2字段：组密码 第3阶段：组管理员用户名 第4字段：组中附加用户 8.4用户管理相关文件用户的家目录 普通用户：/home/用户名/，所有者和所属组都是此用户，权限是700超级用户：/root/，所有者和所属组都是root用户，权限是550 8.5用户管理命令 useradd命令 useradd[选项] 用户名 -u UID：手工指定用户的UID号 -d 家目录 手工指定用户的家目录 -c 用户说明 手工指定用户的说明 -g 组名 手工指定用户的初始组 -G 组名 指定用户的附加组 -s shell 手工指定用户的登录shell。默认是/bin/bash passwd命令格式 passwd[选项] 用户名 -S 查询用户密码的密码状态。仅root用户可用。 -l 暂时锁定用户。仅root用户可用。 -u 解锁用户。仅root用户可用。 --stdin 可以通过管道符输出的数据作为用户的密码。 修改用户信息usermod usermod[选项] 用户名 -u UID 修改用户的uid号 -c 用户说明 修改用户的说明信息 -G 组名 修改用户附加组 -L 临时锁定用户(lock) -U 解锁用户锁定（Unlock） 修改用户密码状态chage chage[选项] 用户名 -l 列出用户的详细密码状态 -d 日期 修改密码最后一次更改时间（shadow3字段） -m 天数 两次密码修改间隔（4字段） -M 天数 密码有效期（5字段） -W 天数 密码过期前警告天数（6字段） -l 天数 密码过期后宽限天数（7字段） -E 日期 账号失效时间（8字段） 删除用户userdel userdel[-r] 用户名 -r 删除用户的同时删除用户家目录 切换用户身份su su [选项] 用户名 - 选项只使用“-”代表连带用户的环境变量一起切换 -c命令 仅执行一次命令，而不切换用户身份 九、压缩解压命令tar命令 解包：tar zxvf FileName.tar 打包：tar czvf FileName.tar DirName 十、网络命令10.1write 给用户发信息 网络 w命令查看用户在线情况 打开两个窗体（写错的时候 退格键 ctrl+backspace键） 写好了 ctrl+D保存结束 远程终端第一个0 终结符EOF 10.2wall 【message】 发广播信息广播信息 在线用户 10.5ping 测试网络连通性ping 选项 IP地址 -C 指定发送次数 10.4ifconfig 查看和设置网卡信息eth0第一块网卡 Ethernet网络昵称以太网 网络类型目前我们接触的都是以太网 Hwaddr 网卡的物理地址 Inet addr 当前计算机地址 Bcast 发送广播的ip地址 Mask 子网掩码网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分 Rx接受数据包数量 byte 接收到的数据包的总大小 TX发送数据包数量 Interrupt 网卡在内存中的物理地址 Lo回环网卡每台机器都有用来做本机网络测试的 10.5mail 查看发送电子邮件不在线也能收到 收到直接数据mail 直接回车 n代表没有读的邮件 1代表一份邮件 输入1回车查看第一份信内容 10.6last 列出目前与过去登入系统的用户信息 计算机所有用户登录系统信息 dj pts/1 192.168.40.1 Thu Mar 7 13:04 still logged in 第二个远程终端 登录的远程ip 一直在登录 10.7netstat 【选项】 显示网络相关信息 选项 用途 -t TCP协议 -u UDP协议 -l 监听 -r 路由 -n 显示IP地址和端口号 范例 netstat -tlun 查看本机监听的端口 netstat -an 查看本机所有的网络连接 netstat -rn 查看本机路由器 netstat -ntlp Tcp http 用的协议 三次握手 安全可靠 传输 打电话 Udp 快 发短信 端口 ip地址为公司名字 找某人 就是端口 Destination Gateway Genmask Flags MSS Window irtt Iface setup 配置网络 dhcp 自动分配自动获取服务 * 默认相当于windows自动获取ip地址没有用，原因个人电脑和家里的环境不会有路由分配。 setup操作结束后 使用 service network restart命令 十一、shell概述 hell是一个命令行解释器。它为用户提供了一个向linux内核发送请求以便运行程序的界面系统级程序。用户可以用shell来启动、挂起、停止甚至是编写一些程序。 注释： 内核 机器语言01010外层 pwd ls命令shell就是黑色及交互命令窗体 shell还是一个功能相当强大的编程语言，易编写，易调试，灵活性较强。shell是解释执行的脚本语言，在shell中可以直接调用Linux系统命令。 shell的分类 1、Bourne Shell: 从1979起Unix就开始使用Bourne Shell，Bourne Shell的主文件名为sh。 2、 C Shell： C Shell主要在BSD版的Unix系统中使用，其语法和C语言相类似而得名。 shell的两种语法类型有Bourne和C，这两种语法彼此不兼容。Bourne家族主要包括sh、ksh、Bash、psh、zsh；C家族主要包括：csh、tcsh linux标准shell是伯恩 shell bash ei Bash:Bash与sh兼容，现在使用的linux就是使用bash作为用户的基本shell。 linux支持的shell /etc/shells 11.1shell脚本运行echo输出命令 echo [选项] [输出内容] -e 支持反斜线控制的字符转换转移符 echo -e ‘\\e[1,31m abcd \\e[0m’ 变色 11.2linux标准shellbase历史命令 history [选项] [历史命令保存文件] -c 清空历史记录 -W 把缓存中的历史命令写入历史命令保存文件 ~/.bash_history 历史命令默认会保存1000条，可以在环境变量配置文件/etc/profile中进行修改 11.2标准输入输出 设备 设备文件名 文件描述符 类型 键盘 /dev/stdin 0 标准输入 显示器 /dev/sdtout 1 标准输出 显示器 /dev/sdterr 2 标准错误输出 11.3输出重定向 类型 符号 作用 标准输出重定向 命令 > 文件 以覆盖的方式，把命令中的正确输出输出到指定的文件或设备当中 标准输出重定向 命令 >> 文件 以追加的形式，把命令中的正确输出输出到指定的文件或设备当中 标准错误输出重定向 错误命令 2>文件 以覆盖的方式，把命令的错误输出输出到指定的文件或设备当中 标准错误输出重定向 错误命令 2>>文件 以追加的形式，把命令的错误输出输出到指定的文件或设备当中 13.4输入重定向 命令name=\"xifu\" #变量定义 >#变量叠加 >aa=123 >aa=\"$aa\"456 >aa=${aa}789 >echo $name #变量调用 >set #变量查看 > unset name #变量删除 环境变量 用户自定义变量只在当前的shell中生效，而环境变量会在当前shell和这个shell的所有子shell中生效。如果把环境变量写入相应的配置文件，name这个环境变量就会在所有的shell中生效。 环境变量语法 export 变量名=变量值 #申明变量env #查询变量unset 变量名 #删除变量 位置参数变量 位置参数变量 作用 $n n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数需要用大括号包含，如${10} $* 这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体 $@ 这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待 $# 这个变量代表命令行中所有参数的个数 例1： 12345#!/bash/bashnum1=$1num2=$2sum=$(($num1+$num2))echo $sum #打印变量sum的值 例2： 1234567#!/bash/bash#使用$#代表所有参数的个数echo 'A total of $# parameters'#使用$*代表所有参数echo 'The parameters is: $*'#使用$@代表所有参数echo 'The parameters is: $@' 预定义变量 预定义变量 作用 $? 最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。 $$ 当前进程的进程号（PID） $! 后台运行的最后一个进程的进程号（PID） && || 是通过什么第二命令知道第一命令是否正常运行的呢 其实是通过$?的数字才判断的 程序员用户眼睛判断命令是否正确 计算机是通过$?判断命令是否正确 接收键盘输入 read -p “提示信息”：在等待read输入时，输出提示信息 -t 描述 read命令会一直等待用户输入，使用此选项可以指定等待时间 -n 字符数 read命令只接受指定的字符数，就会执行 -s 隐藏输入的数据，适用于机密信息的输入 12345678910111213#提示“请输入姓名”并等待30秒，把用户的输入保存入变量name中read -t 30 -p \"Please input your name:\" nameecho \"Name is $name\"#年龄是隐私，用“-s”选项隐藏输入read -s -t 30 -p \"Please enter your age:\" ageecho -e \"\\n\"echo \"Age is $age\"#使用“-n 1”选项只接收一个输入字符就会执行（都不用输入回车）read -n 1 -t 30 -p \"please select your gender[M/F]:\" genderecho -e \"\\n\"echo \"sex is $gender\" declare声明变量类型 declare [+/] [选项] [变量名] 给变量设定类型属性 取消变量的类型属性 -i 将变量声明为整数型（integer） -x 将变量声明为环境变量 -p 显示指定变量的被声明的类型 例：方法一，数值运算——方法1 1234aa=11bb=22给变量aa和bb赋值declare -i cc=$aa+$bb 例：方法二，expr或let数值运算工具 12345aa=11bb=22#给变量aa和bb赋值dd=$(expr $aa + $bb)#dd的值是aa和bb的和。注意“+”号左右两侧必须有空格 例：方法三，”$((运算式))” 或”$[运算式]” 123456aa=11bb=22ff=$(( $aa+$$bb ))gg=$[ $aa+$$bb ]#单个小括号是系统命令#双个小括号代表数值运算符 变量置换方式 变量y没有设置 变量y为空值 变量y设置值 x=${y-新值} x=新值 x为空值 x=$y x=${y:-新值} x=新值 x为新值 x=$y x=${y+新值} x为空 x为新值 x=新值 x=${y:+新值} x为空 x为空值 x=新值 x=${y=新值} x=新值，y=新值 x为空值，y值不变 x=$y，y值不变 x=${y:=新值} x=新值，y=新值 x为新值，y为新值 x=$y，y值不变 x=${y?新值} 新值输出到标准错误输出 x为空值 x=$y x=${y:?新值} 新值输出到标准错误输出 新值输出到标准错误输出 x=$y 十二、正则表达式 正则表达式用来在文件中匹配符合条件的字符串，正则是包含匹配。grep、awk、sed等命令可以支持正则表达式。 通配符用来匹配符合条件的文件名，通配符是完全必配。ls、find、cp这些命令不支持正则表达式，所以只能使用shell自己的通配符来进行匹配。 元字符 作用 * 前一个字符匹配0次或任意多次 . 匹配除了换行符外任意一个字符 ^ 匹配行首。 $ 匹配行尾。 [] 匹配中括号中的任意一个字符，例如[a-z0-9] [^] 匹配除中括号的字符以外的任意一个字符。[^a-z] \\ 转义符 {n} 表示其前面的字符恰好出现几次 {n,} 表示其前面的字符出现不小于n次 {n,m} 表示前面的字符至少出现n次，最多出现m次。 十三、Linux 服务管理13.1服务管理分类 源码包可以看到源代码 可以自定义Rpm没有源代码 自定义差 独立服务：服务直接在内存中 客户直接调用服务，服务直接相应用户，速度快，服务多了浪费内存资源基于xinetd服务本身是独立的 本身没有功能，后面有一系列服务rsync 网络备份服务，通过xinted相应 rsync 相应最后客户端相应速度慢 本身不占用内存 13.2启动与自启动服务启动：就是当前系统让服务运行，并提供功能 服务自启动：是指让服务在系统开机或重启之后，随着系统的启动而自启动服务。 13.3查询已安装的服务RPM包安装的服务 chkconfig --list 123456aegis 0:off 1:off 2:on 3:on 4:on 5:on 6:offbt 0:off 1:off 2:on 3:on 4:on 5:on 6:offnetconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offnginx 0:off 1:off 2:off 3:off 4:off 5:off 6:off 0关机 1单用户 2不完全多用户 3字符界面 4未分配 5图形界面 6重启动 源码包安装的服务 查看服务安装位置 卸载 rpe -e --nodeps（不检查依赖性） 包名 十四、Linux系统备份与恢复14.1备份和恢复概述 Linux系统中需要备份的数据 /root/目录 /home/目录 /var/spool/mail/目录 /etc/目录 其他目录 14.2 备份策略 完全备份： 优点是数据恢复方便 缺点备份的数据量较大,备份时间较长,占用的空间较大增量备份： 优点备份的数据较少，耗时较短，占用的空间较小； 缺点是数据恢复比较麻烦，先恢复完全备份的数据每次增量备份的数据，最终才能恢复所有的数据。差异备份： 优点恢复数据简单方便快捷 缺点数据量庞大、备份速度缓慢、占用空间较大。 14.3、备份和恢复命令 备份命令 dump [选项]备份之后的文件名 原文件或目录 恢复命令 restore [模式选项][选项] 十五、Linux系统管理1、进程管理查看 ps aux用户 进程id 占用cpu 内存 内存daxiao 物理大 终端 状态 开始时间 占用cpu时间 命令 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.5 0.0 2872 1416 ? Ss 15:36 0:01 /sbin/init root 2 0.0 0.0 0 0 ? S 15:36 0:00 [kthreadd] 2、进程管理终止 kill命令3、工作管理4、系统资源查看5、系统定时任务 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"HDFS上传和读取流程","date":"2019-03-28T06:34:00.000Z","path":"/posts/e4062a35/","text":"![Git][git] 上传流程 1.根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。 2.namenode返回是否可以上传 3.client请求第一个 block该传输到哪些datanode服务器上 4.namenode返回3个datanode服务器ABC 5.client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将这个pipeline建立完成，逐级返回客户端 6.client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 7.当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。读取流程客户端读取HDFS数据相比写入数据要简单一些，以下是读取数据步骤： 1.client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 2.就近挑选一台datanode服务器，请求建立输入流。 3.开始读取这个数据的第一个block块，读取完全之后，开始接着读取这个文件的第二个block，直至把这个数据所有的block都读完了则文件读取完全了。 4.数据读完之后关闭流连接。 5.如果读取过程当中读取失败，将会依次读取该数据块的下一个副本，失败的节点将会被记录，不再连接。 ￼# SecondaryNameNodeSecondaryNameNode 是 HDFS 架构中的一个组成部分，它用来保存名称节点中对HDFS元数据信息的备份，减小Editlog文件大小，从而缩短名称节点重启的时间。 它一般是单独运行在一台机器上。 SecondaryNameNode让EditLog变小的工作流程如下: (1)SecondaryNameNode 会定期和 NameNode 通信，请求其停止使用 EditLog 文件，暂时将 新的写操作写到一个新的文件 edit.new 中，这个操作是瞬间完成的，上层写日志的函数完全感觉不到差别。 (2) SecondaryNameNode 通过 HTTP GET 方式从 NameNode 上获取到 Fslmage 和 EditLog 文 件，井下载到本地的相应目录下。 (3) SecondaryNameNode 将下载下来的 Fslmage 载入到内存，然后一条一条地执行 EditLog 文件中的各项更新操作，使内存中的 Fslmage 保持最新。 这个过程就是 EditLog 和 Fslmage 文件合 井。 (4) SecondaryNameNode 执行完（3）操作之后，会通过 post 方式将新的 Fslmage 文件发送 到 NameNode 节点上 (5) NameNode 将从 SecondaryNameNode 接收到的新的 Fslmage 替换旧的 Fslmage 文件，同 时将 Edit.new 替换 EditLog 文件，从而减小 EditLog 文件大小。￼从上面的过程可以看出，第二名称节点相当于为名称节点设置一个“检查点” ，周期性备份 名称节点中的元数据信息，但第二名称节点在 HDFS 设计中只是一个冷备份，并不能起到“热备 份”的作用。 HDFS 设计并不支持当名称节点故障时直接切换到第二名称节点。 HDFS FederationHDFSl.O 的单 NameNode 设计不仅存在单点故障问题，还存在可扩展性和性能问题。只有一 个 NameNode， 不利于水平扩展。 HDFS Federation (HDFS 联邦）特性允许一个 HDFS 集群中存在 多个 NameNode 同时对外提供服务，这些 NameNode 分管一部分目录（水平切分），彼此之间相 互隔离，但共享底层的 DataNode 存储资源。每个 NameNode 是独立的，不需要和其他 Nam巳Node 协调合作。 如图 4-5 所示 ， Federation 使用了 多 个独立的 NameNode/NameSpace 命名空间。这些 NameNode 之间是联合的，也就是说， 它们之间相互独立且不需要互相协调，各自分工管理自己的 区域。分布式的 DataNode 被用作通用的数据块存储设备。每个 DataNode 要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告， 并执行来自所有 NameNode 的命令。每一个 DataNode 作为统一的块存储设备被所有 NameNode 节点使用。 每一个 DataNode 节点都在所有的 NameNode 进行注册。 DataNode 发送心跳信息、块报告到 所有 NameNode，同时执行所有 NameNode 发来的命令。￼ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"bat批量存放信息到excel","date":"2019-03-28T06:34:00.000Z","path":"/posts/5fe19fd1/","text":"情景：最近喵仙子有个需求，需要抽取日志文件中的有效信息到excel，刚开始我用MapReduce给他做了一份，不过他有点蠢配不了环境，于是乎我研究了下bat，终于给他整了个好方法，请看下文。 需求需要从log中抽取两列数据到excel中。 本西芙没了解过bat语法，所以查了一下，通过结合这两篇解答完成bat命令。1、实现了按照空格批处理分割txt数据，存储excelhttps://zhidao.baidu.com/question/1770548208106936980.html123456789101112@echo offrem 保存为bat文件跟txt文件放一起运行md \"结果\\\" 2>nulfor /f \"delims=\" %%a in ('dir /a-d/b *.txt') do ( setlocal EnableDelayedExpansion (for /f \"delims=\" %%b in ('type \"%%a\"') do ( set \"str=%%b\" echo;!str: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 2、批处理分割字符串https://blog.csdn.net/zhju85126com/article/details/46649961123456789101112@echo offset str=100x200x300y400y500x600y700for /f \"tokens=1,3-5,* delims=x|y\" %%a in (\"%str%\") do ( set c1=%%a set c3=%%b set c4=%%c set c5=%%d set c6=%%e)echo %c1%, %c3%, %c4%, %c5%, %c6%pauserem 输出结果为输出结果为：100, 300, 400, 500, 600y700。其中tokens=1,3-5,*表示提取第1、3至5列，同时把第5列后所有剩余字符串作为第6列，一个输出了5个变量，也可以写作tokens=1,3,4,5,*。 最终我所加工的代码，其中代码中tokens=1,2是按照_或空格分割的位置，相当于数组的某一元素、把它赋值给str112345678910111213@echo offmd \"结果\\\" 2>nulrem 循环数据for /f \"delims=\" %%a in ('dir /a-d/b *.log') do ( setlocal EnableDelayedExpansion rem 切割方式 (for /f \"tokens=1,2 delims=_| \" %%b in ('type \"%%a\"') do ( set \"str1=%%b %%c\" echo;!str1: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 首先根据第一篇解答套入转excel模板，其次根据第二篇解答对bat for循环中分割和选取的描述，修改第五行代码。可以实现。 演示 文件内容 文件内容 目录下的文件 目录下的文件 2、点击aa.bat,生成csv文件 生成csv文件 3、任意打开一个csv文件 csv文件内容 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]}]