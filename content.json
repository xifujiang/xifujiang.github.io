[{"title":"hadoop详细学习","date":"2019-09-13T09:50:11.087Z","path":"/posts/a5ead4c6/","text":"1、hadoop1.1 hadoop介绍1）hadoop是一个由Apache基金会所开发的分布式系统基础架构。 2）主要解决海量数据的存储和海量数据的分析计算问题。 3）Hadoop生态圈 1.2 Lucence框架1）Lucene框架使Doug Cutting开创的开源软件，用户Java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎。 2）2001年年底Lucene成为Apache基金会的一个子项目。 3）对于海量数据的场景，Lucence面对与Google同样的困难，存储数据困难，检索速度慢。 4）学习和模仿Google解决这些问题的办法：微型版Nutch。 5）Google是Haddop的思想之源。 6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 7）2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 8）2006年3月份，Map-Reduce和Nutch Distributed File System（NDFS）分别被纳入称为Hadoop的项目中。 9）名字来源于Doug Cutting儿子的玩具大象。 1.3 hadoop三大发行版本Apache、Cloudera、Hortonworks。 1.4 hadoop的优势（面试）1)高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。 2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。 3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 4）高容错性：能够自动将失败的任务重新分配。 1.5 hadoop1.x 和hadoop2.x区别hadoop1.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算+资源调度） CPU 8 内存：128MB 磁盘 8T hadoop2.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算）、yarn（资源调度） CPU 8 内存：128MB 磁盘 8T 对比：hadoop1.x时代，MapReduce既处理运算，又处理资源调度，耦合性较大。hadoop1.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。 1.6 组件架构1、hdfs架构概述1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、福本数、文件权限），以及每个文件的块列表和块所在的DataNode等。 2）DataNode（dn）：在本地文件系统存储文件数据，以及数据的校验和。 3）Secondary NameNode（2nn）：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。加快启动速度。 2、Yarn架构1）ResourceManager （RM）主要作用如下 （1）处理客户端请求 （2）监控NodeManager （3）启动或监控ApplicationMaster（集群中运行的一个job） （4）资源的分配与调度 yarn架构 2）NodeManager（NM）主要作用如下 （1）管理单个节点上的资源 （2）处理来自ResourceManager的命令 （3）处理来自ApplicationMaster的命令 3）ApplicationMaster（AM）作用如下 （1）负责数据的切分 （2）为某个程序申请资源并分配给内部的任务 （3）任务的监控与容错 4）Container container是Yarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 3、MapReduce架构概述MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据。 2）Reduce阶段对Map结果进行汇总 1.7 大数据技术生态体系 大数据技术生态体系 图中涉及的技术名词解释如下： 1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行**数据的迁移，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： （1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 （2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 （3）支持通过Kafka服务器和消费机集群来分区消息。 （4）支持Hadoop并行数据加载。 4）Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。 6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。 7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 11）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。 12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 1.8 hadoop重要目录（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本 （2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 （3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） （4）sbin目录：存放启动或停止Hadoop相关服务的脚本 （5）share目录：存放Hadoop的依赖jar包、文档、和官方案例 1.9完全分布式运行模式1、准备三台客户机（关闭防火墙、静态ip、主机名称） 2、安装JDK 3、配置环境变量 4、安装hadoop 5、配置环境变量 6、配置集群 7、单点启动 8、配置ssh 9、群起并测试集群 2、HDFS2、1 介绍产生背景：数据量大，一个操作系统存不过来，需要一种系统管理多台机器上的文件 定义：HDFS(Hadoop Distributed File System)，分布式文件管理系统。用于存储文件，通过目录树来定位文件。 使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。 2.2.1 优点1、高容错性 （1）数据自动保存多个副本。通过增加副本的形式，提高容错性。 （2）某个副本丢失后，可以自动恢复 2、适合处理大数据 （1）数据规模：那个处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3、可构建在廉价机器上，通过多副本机制，提高可靠性。 2.2.2 缺点1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 2）无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 3）不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许 多个线程同时写。 （2）仅支持数据append（追加），不支持文件的随机修改。 2.3 组件功能1）NameNode（nn）：就是Master，它是一个管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据块（Block）映射信息； （4）处理客户端读写请求。 2）DataNode：就是slave。NameNode下达命令。DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 3）client：客户端 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如NameNode格式化； （5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作； 4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能替代NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 2.4 文件块大小HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版中是64M。 1、集群中的block，2、如果寻址时间约为10ms，即查到目标block的时间为10ms，3寻址时间为传输时间的1%时，则为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s。4 而目前磁盘的传输速率普遍为100MB/s。 思考：为什么块的大小不能设置太小，也不能设置太大？ 1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； 2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需要的时间。导致程序在处理这块数据时。会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 2.5 HDFS的数据流（面试重点）1、HDFS的写数据流程 1568448314627 然后重复3-7步骤 2、网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 1568448821111 3、副本节点选择 1568449200994 4、HDFS的读数据流程 1568449571543 2.6 NameNode和SecondaryNameNode（面试开发重点）1、NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。 1568450098120 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改。 \\2. 第二阶段：Secondary NameNode工作 ​ （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。 ​ （2）Secondary NameNode请求执行CheckPoint。 ​ （3）NameNode滚动正在写的Edits日志。 ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint。 ​ （7）拷贝fsimage.chkpoint到NameNode。 ​ （8）NameNode将fsimage.chkpoint重新命名成fsimage。 注： NN和2NN工作机制详解： Fsimage：NameNode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。 SecondaryNameNode首先会询问是否需要（触发需要满足两个条件中的任意一个，定时时间到和中数据写满了）。直接带回是否检查结果。执行操作，首先会让滚动并生成一个空的，滚动的目的是给打个标记，以后所有新的操作都写入，其他未合并的和会拷贝到的本地，然后将拷贝的和加载到内存中进行合并，生成，然后将拷贝给，重命名为后替换掉原来的。在启动时就只需要加载之前未合并的和即可，因为合并过的中的元数据信息已经被记录在中。 1568451755272 2.7 DataNode 工作机制 2.7.1DataNode掉线时限参数设置1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信 2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。 3、HDFS默认的超时时长为10分钟+30秒。 4=如果定义超时时间为TimeOut，则超时时长的计算公式为：timeout = 2XXX(分钟)+10\\YYY（秒）。 2.7.2 服役新数据节点、退役旧数据节点1、添加白名单 2、黑名单退役 3、不允许白名单和黑名单中同时出现同一个主机名称。 2.8 HDFS新特性2.8.1 集群间的数据拷贝1．scp实现两个远程主机之间的文件复制 12345scp -r hello.txt [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) // 推 pushscp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pullscp -r [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2．采用distcp命令实现两个Hadoop集群之间的递归数据复制 12[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 2.8.2 小文件存档1、HDFS存储小文件的弊端 大量的小文件会消耗NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量与数据块的大小无关。 2、解决存储小文件的办法之一 HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。也就是说HDFS存档问你件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 3、实操 1）先启动hadoop集群 2）把某目录下的所有文件归档成xx.har的归档文件，并把归档后文件存储到xxx/output路径下。 1[root@master ~]# hadoop archive -archiveName input.har -p /data/input /data/out 3）查看归档 1234[root@master ~]# hadoop fs -ls -R har:///data/out/input.har-rw-r--r-- 3 root supergroup 38 2019-09-15 23:21 har:///data/out/input.har/1-rw-r--r-- 3 root supergroup 26 2019-09-15 23:21 har:///data/out/input.har/2-rw-r--r-- 3 root supergroup 97 2019-09-13 22:54 har:///data/out/input.har/wc.input 4）解归档文件 1hadoop fs -cp har:/// data/output/input.har/* /data 2.8.3 回收站功能参数1、默认值fs.trash.interval = 0, 0表示禁用回收站；其他值表示设置文件的存活时间。 2、默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。 3、要求：fs.trash.checkpoint.interval o.getSumFlow() ? -1 : 1; } 3.6.2 切片与MapTask并行度决定机制1．问题引出 MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。 思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 2．MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。 3.6.3 FileInputFormat切片源码解析：重点： 默认情况：切片大小= blocksize 每次切片时，要判断切完剩余部分是否大于块的1.1倍，不大于1.1倍就划分1块切片 img 切片时不考虑数据集整体，而是逐个对每一个文件单独切片。 3.6.4 FileInputFormat切片大小的参数配置 img 3.6.5 CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。 1、应用场景： CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。 2、虚拟存储切片最大值设置 CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 3、切片机制 生成切片过程包括：虚拟存储过程和切片过程二部分。！！！！ img 3.6 MapReduce详细工作流程 1568639170123 1568639177258 3.7 Shuffle机制3.7.1 Partition分区1、问题引出 要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 2、默认Partitioner分区 Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。如图4-14所示。 img 3.7.2 WritableComparable排序排序概述： ​ 排序是MapReduce框架中最重要的操作之一。 ​ MapTask和ReduceTask均会对数据按照Key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。 ​ 默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 ​ 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 ​ 1、部分排序 ​ MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。 ​ 2、全排序 ​ 最终输出结果只有一个文件，且文件内部有序。实现方法是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。 ​ 3、辅助排序（GroupingComparator分组） ​ 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 ​ 4、二次排序 ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 1、实现全排序 2、实现区内排序 3.7.3 Combiner合并（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。 （2）Combiner组件的父类就是Reducer （3）Combiner和Reducer的区别在于运行的位置 Combiner是在每一个MapTask所在的节点运行 Reducer是接收全局所有Mapper的输出结果 （4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。 （5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且Combiner的输出kv应该跟Reducer的输入kv类型对应起来。 order举例 3.8 MapReduce +shuffle 工作机制（面试） img （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 img （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.9 Join多种应用3.9.1 Reduce Join1、Reduce Join工作原理： ​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 ​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就OK了。 2、缺点及解决方案 ​ 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。 ​ 解决方案：Map端实现数据合并 3.9.2 Map Join 1．使用场景 Map Join适用于一张表十分小、一张表很大的场景。 2．优点 思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？ 在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。 3．具体办法：采用DistributedCache ​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 // 缓存普通文件到Task运行节点。 job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); 3.10 计数器应用hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 1、计数器API （1）采用枚举的方式统计计数 （2）采用计数器组、计数器名称的方式统计 （3）计数结果在程序运行后的控制台上查看。 3.11 数据清洗（ETL）3.12 Hadoop数据压缩3.12.1 压缩概念压缩概述： 压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。 数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启动压缩。 压缩策略和原则 优化策略 通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。 注意：采用压缩技术减少了磁盘IO，但同时也增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。 压缩原则： 1、运算密集型的job，少用压缩 2、IO密集型的job，多用压缩 3.12.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 3.12.3 压缩方式选择Gzip压缩： 优点：压缩率比较高，压缩/解压速度快；hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后130M以内的（1个块大小内），都可以考虑Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。 Bzip2压缩： 优点：支持split，具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便。 缺点：压缩/解压速度慢。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序的情况。 Lzo 优点：压缩/解压速度也比较快，合理的压缩率，支持split，是Hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定InputFormat为Lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越明显。 Snappy 优点：高速压缩速度和合理的压缩率。 缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装。 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另一个MapReduce作业的输入。 3.12.4 压缩位置的选择 img 5、Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 img 工作机制详解 ​ （1）MR程序提交到客户端所在的节点。 ​ （2）YarnRunner向ResourceManager申请一个Application。 ​ （3）RM将该应用程序的资源路径返回给YarnRunner。 ​ （4）该程序将运行所需资源提交到HDFS上。 ​ （5）程序资源提交完毕后，申请运行mrAppMaster。 ​ （6）RM将用户的请求初始化成一个Task。 ​ （7）其中一个NodeManager领取到Task任务。 ​ （8）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （9）Container从HDFS上拷贝资源到本地。 ​ （10）MRAppmaster向RM 申请运行MapTask资源。 ​ （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。 ​ （13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。 ​ （14）ReduceTask向MapTask获取相应分区的数据。 ​ （15）程序运行完毕后，MR会向RM申请注销自己。 img 6、优化6.1 MapReduce优化MapReduce程序效率的瓶颈在于两点： 1、计算机性能 ​ CPU、内存、磁盘健康、网络 2、I/O操作优化 （1）数据倾斜 （2）Map和Reduce数设置不合理 （3）Map运行时间太长，导致Reduce等待过久 （4）小文件过多 （5）大量的不可分块超大文件 （6）Spill次数过多 （7）Merge次数过多等。 优化方法： 1、数据输入 （1）合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 2、Map阶段 （1）减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO. （2）减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 （3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O。 3、Reduce阶段 （1）合理设置Map和Reduce数。 （2）设置Map、Reduce共存。调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。 （3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 （4）合理设置Reduce端的Buffer。 4、I/O传输 （1）采用数压缩的方式 （2）使用SequenceFile二进制文件 5、数据倾斜问题 1）数据倾斜现象 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 2）减少数据倾斜的方法 1）抽样和范围分区 2）自定义分区 3）Combine 4）采用Map Join， 尽量避免Reduce Join。 6.2HDFS小文件优化6.2.1 HDFS小文件弊端HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。 6.2.2 HDFS小文件解决方案小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。 （2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。 （3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。 1、MAVEN内存溢出!（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,nativeN -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 DataNode和NameNode进程同时只能工作一个。 clusterId不统一 10）执行命令不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 11）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 12）jps不生效。 原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。 13）8088端口连接不上 [atguigu@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102 ​ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"机器学习算法","date":"2019-09-09T13:04:00.000Z","path":"/posts/0/","text":"EM算法最大期望算法（Expectation-Maximization algorithm, EM），或Dempster-Laird-Rubin算法 [1] ，是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法 [2] ，通常作为牛顿迭代法（Newton-Raphson method）的替代用于对包含隐变量（latent variable）或缺失数据（incomplete-data）的概率模型进行参数估计 [2-3] 。 EM算法的标准计算框架由E步（Expectation-step）和M步（Maximization step）交替组成，算法的收敛性可以确保迭代至少逼近局部极大值 [4] 。EM算法是MM算法（Minorize-Maximization algorithm）的特例之一，有多个改进版本，包括使用了贝叶斯推断的EM算法、EM梯度算法、广义EM算法等 [2] 。 https://www.cnblogs.com/jiangxinyang/p/9278608.html document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"JAVA笔试经典题型及原因","date":"2019-09-09T13:04:00.000Z","path":"/posts/0/","text":"JAVA笔试经典题型及原因1、JVM​ JVM是Java Virtual Machine（Java虚拟机）的缩写，JVM是一种用于计算设备的规范，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 ​ 引入Java语言虚拟机后，Java语言在不同平台上运行时不需要重新编译。Java语言使用Java虚拟机屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。 1、内存管理​ （1）Java运行时涉及到的存储区域主要包括程序计数器、Java虚拟机栈、本地方法栈、java堆、方法区以及直接内存等等。 img 大多数JVM将内存区域划分为 Method Area(Non-Heap)(方法区),Heap(堆)Program Counter Register(程序计数器), VM Stack(虚拟机栈,也有翻译成JAVA方法栈的),Native Method Stack本地方法栈) 。 其中Method Area(方法区) 和Heap（堆） 是线程共享的， VM Stack，Native Method Stack 和Program Counter Register 是非线程共享的。 ​ 首先我们熟悉一下一个一般性的 Java 程序的工作过程。一个 Java 源程序文件，会被编译为字节码文件（以 class 为扩展名），每个java程序都需要运行在自己的JVM上，然后告知 JVM 程序的运行入口，再被 JVM 通过字节码解释器加载运行。 ​ 概括地说来，JVM初始运行的时候都会分配好 Method Area（方法区） 和Heap（堆） ，而JVM每遇到一个线程，就为其分配一个 Program Counter Register（程序计数器） , VM Stack（虚拟机栈）和Native Method Stack （本地方法栈）， 当线程终止时，三者（虚拟机栈，本地方法栈和程序计数器）所占用的内存空间也会被释放掉。这也是为什么我把内存区域分为线程共享和非线程共享的原因，非线程共享的那三个区域的生命周期与所属线程相同，而线程共享的区域与JAVA程序运行的生命周期相同，所以这也是系统垃圾回收的场所只发生在线程共享的区域（实际上对大部分虚拟机来说知发生在Heap上）的原因。 2、题目 1、下面有关JVM内存，说法错误的是？ 12> A.程序计数器是一个比较小的内存区域，用于指示当前线程所执行的字节码执行到了第几行，是线程隔离的> 12> B.虚拟机栈描述的是Java方法执行的内存模型，用于存储局部变量，操作数栈，动态链接，方法出口等信息，是线程隔离的> 12> C.方法区用于存储JVM加载的类信息、常量、静态变量、以及编译器编译后的代码等数据，是线程隔离的> 12> D.原则上讲，所有的对象都在堆区上分配内存，是线程之间共享的> 解析：堆区、方法区是线程共享的，程序计数区、虚拟机栈区是线程隔离的。本题选C，出错的原因就在此。 2、SQL1、Statement、PreparedStatement、CallableStatement1.它们都是接口(interface)。 2.Statement继承自Wrapper、PreparedStatement继承自Statement、CallableStatement继承自PreparedStatement。 3.Statement接口提供了执行语句和获取结果的基本方法； PreparedStatement接口添加了处理 IN 参数的方法； CallableStatement接口添加了处理 OUT 参数的方法。 4. a.Statement: 普通的不带参的查询SQL；支持批量更新,批量删除; b.PreparedStatement: 可变参数的SQL,编译一次,执行多次,效率高; 安全性好，有效防止Sql注入等问题; 支持批量更新,批量删除; c.CallableStatement: 继承自PreparedStatement,支持带参数的SQL操作; 支持调用存储过程,提供了对输出和输入/输出参数(INOUT)的支持; Statement每次执行sql语句，数据库都要执行sql语句的编译,最好用于仅执行一次查询并返回结果的情形，效率高于PreparedStatement。 PreparedStatement是预编译的，使用PreparedStatement有几个好处 在执行可变参数的一条SQL时，PreparedStatement比Statement的效率高，因为DBMS预编译一条SQL当然会比多次编译一条SQL的效率要高。 安全性好，有效防止Sql注入等问题。 对于多次重复执行的语句，使用PreparedStament效率会更高一点，并且在这种情况下也比较适合使用batch； 代码的可读性和可维护性。 2、题目1、下面有关jdbc statement的说法错误的是？ 12> A.JDBC提供了Statement、PreparedStatement 和 CallableStatement三种方式来执行查询语句，其中 Statement 用于通用查询， PreparedStatement 用于执行参数化查询，而 CallableStatement则是用于存储过程> 12> B.对于PreparedStatement来说，数据库可以使用已经编译过及定义好的执行计划，由于 PreparedStatement 对象已预编译过，所以其执行速度要快于 Statement 对象”> 12> C.PreparedStatement中，“?” 叫做占位符，一个占位符可以有一个或者多个值> 12> D.PreparedStatement可以阻止常见的SQL注入式攻击> 解析：选C，占位符只能有1个值。 3、Spring事务事务属性的种类： 传播行为、隔离级别、只读和事务超时 a) 传播行为定义了被调用方法的事务边界。 传播行为 意义 PROPERGATION_MANDATORY 表示方法必须运行在一个事务中，如果当前事务不存在，就抛出异常 PROPAGATION_NESTED 表示如果当前事务存在，则方法应该运行在一个嵌套事务中。否则，它看起来和PROPAGATION_REQUIRED 看起来没什么俩样 PROPAGATION_NEVER 表示方法不能运行在一个事务中，否则抛出异常 PROPAGATION_NOT_SUPPORTED 表示方法不能运行在一个事务中，如果当前存在一个事务，则该方法将被挂起 PROPAGATION_REQUIRED 表示当前方法必须运行在一个事务中，如果当前存在一个事务，那么该方法运行在这个事务中，否则，将创建一个新的事务 PROPAGATION_REQUIRES_NEW 表示当前方法必须运行在自己的事务中，如果当前存在一个事务，那么这个事务将在该方法运行期间被挂起 PROPAGATION_SUPPORTS 表示当前方法不需要运行在一个是事务中，但如果有一个事务已经存在，该方法也可以运行在这个事务中 b) 隔离级别 在操作数据时可能带来 3个副作用，分别是脏读、不可重复读、幻读。为了避免这 3 中副作用的发生，在标准的 SQL 语句中定义了 4种隔离级别，分别是未提交读、已提交读、可重复读、可序列化。而在 spring 事务中提供了 5 种隔离级别来对应在 SQL 中定义的 4种隔离级别，如下： 隔离级别 意义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别 ISOLATION_READ_UNCOMMITTED 允许读取未提交的数据（对应未提交读），可能导致脏读、不可重复读、幻读 ISOLATION_READ_COMMITTED 允许在一个事务中读取另一个已经提交的事务中的数据（对应已提交读）。可以避免脏读，但是无法避免不可重复读和幻读 ISOLATION_REPEATABLE_READ 一个事务不可能更新由另一个事务修改但尚未提交（回滚）的数据（对应可重复读）。可以避免脏读和不可重复读，但无法避免幻读 ISOLATION_SERIALIZABLE 这种隔离级别是所有的事务都在一个执行队列中，依次顺序执行，而不是并行（对应可序列化）。可以避免脏读、不可重复读、幻读。但是这种隔离级别效率很低，因此，除非必须，否则不建议使用。 c) 只读 如果在一个事务中所有关于数据库的操作都是只读的，也就是说，这些操作只读取数据库中的数据，而并不更新数据，那么应将事务设为只读模式（ READ_ONLY_MARKER ） , 这样更有利于数据库进行优化 。 因为只读的优化措施是事务启动后由数据库实施的，因此，只有将那些具有可能启动新事务的传播行为(PROPAGATION_NESTED 、 PROPAGATION_REQUIRED 、 PROPAGATION_REQUIRED_NEW) 的方法的事务标记成只读才有意义。 如果使用 Hibernate 作为持久化机制，那么将事务标记为只读后，会将 Hibernate 的 flush 模式设置为 FULSH_NEVER, 以告诉 Hibernate 避免和数据库之间进行不必要的同步，并将所有更新延迟到事务结束。 d) 事务超时 如果一个事务长时间运行，这时为了尽量避免浪费系统资源，应为这个事务设置一个有效时间，使其等待数秒后自动回滚。与设 置“只读”属性一样，事务有效属性也需要给那些具有可能启动新事物的传播行为的方法的事务标记成只读才有意义。 2、题目下面有关SPRING的事务传播特性，说法错误的是？ 12345> A.PROPAGATION_SUPPORTS：支持当前事务，如果当前没有事务，就以非事务方式执行> B.PROPAGATION_REQUIRED：支持当前事务，如果当前没有事务，就抛出异常> C.PROPAGATION_REQUIRES_NEW：新建事务，如果当前存在事务，把当前事务挂起> D.PROPAGATION_NESTED：支持当前事务，新增Savepoint点，与当前事务同步提交或回滚> 解析： PROPAGATION_REQUIRED–支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS–支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY–支持当前事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW–新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED–以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。PROPAGATION_NEVER–以非事务方式执行，如果当前存在事务，则抛出异常。 所以答案选择B。 4、Servlet1、CGI简介CGI（Common Gateway Interface）公共网关接口，是外部扩展应用程序与 Web 服务器交互的一个标准接口。服务器端与客户端进行交互的常见方式多，CGI 技术就是其中之一。根据CGI标准，编写外部扩展应用程序，可以对客户端浏览器输入的数据进行处理，完成客户端与服务器的交互操作。CGI规范定义了Web服务器如何向扩展应用程序发送消息，在收到扩展应用程序的信息后又如何进行处理等内容。对于许多静态的HTML网页无法实现的功能，通过 CGI可以实现，比如表单的处理、对数据库的访问、搜索引擎、基于Web的数据库访问等等。使用CGI实现客户端与服务器的交互有以下几个标准步骤，具体步骤如下： （1）Web 客户端的浏览器将URL的第一部分解码与Web服务器相连。 （2）Web 浏览器将URL的其余部分提供给服务器。 （3）Web 服务器将URL转换成路径和文件名。 （4）Web 服务器发送 HTML 和别的组成请求页面的文件给客户。一旦页面内容传送完， 这个连接自动断开。 （5）在客户端，HTML脚本提示用户做动作或输入。当用户响应后，客户请求Web服务器建立一个新的连接。 （6）Web 服务器把这些信息和别的进程变量传送给由HTML以URL的形式指定CGI程序。 （7）CGI 根据输入作出响应，把响应结果传送给 Web 服务器。 （8）Web 服务器把响应的数据传给客户，完成后关闭连接。 2、Servlet简介Servlet（Server Applet）是Java Servlet的简称，称为小服务程序或服务连接器，用Java编写的服务器端程序，具有独立于平台和协议的特性，主要功能在于交互式地浏览和生成数据，生成动态Web内容。 3、Servlet生命周期 客户端请求该 Servlet； 加载 Servlet 类到内存； 实例化并调用init()方法初始化该 Servlet； service()（根据请求方法不同调用doGet() 或者 doPost()，此外还有doHead()、doPut()、doTrace()、doDelete()、doOptions()、destroy()。 加载和实例化 Servlet。这项操作一般是动态执行的。然而，Server 通常会提供一个管理的选项，用于在 Server 启动时强制装载和初始化特定的 Servlet。 4、Servlet与CGI对比 与传统的 CGI 和许多其他类似 CGI 的技术相比，Java Servlet 具有更高的效率，更容易使用，功能更强大，具有更好的可移植性，更节省投资。在未来的技术发展过程中，Servlet 有可能彻底取代 CGI。 在传统的 CGI中，每个请求都要启动一个新的进程，如果 CGI 程序本身的执行时间较短，启动进程所需要的开销很可能反而超过实际执行时间。而在 Servlet 中，每个请求由一个轻量级的 Java 线程处理（而不是重量级的操作系统进程）。 在传统 CGI 中，如果有 N 个并发的对同一 CGI程序的请求，则该CGI程序的代码在内存中重复装载了 N 次；而对于 Servlet，处理请求的是 N 个线程，只需要一份 Servlet 类代码。在性能优化方面，Servlet 也比 CGI 有着更多的选择。 5、题目1、下面有关servlet和cgi的描述，说法错误的是？ 12345> A.servlet处于服务器进程中，它通过多线程方式运行其service方法> B.CGI对每个请求都产生新的进程，服务完成后就销毁> C.servlet在易用性上强于cgi，它提供了大量的实用工具例程，例如自动地解析和解码HTML表单数据、读取和设置HTTP头、处理Cookie、跟踪会话状态等> D.cgi在移植性上高于servlet，几乎所有的主流服务器都直接或通过插件支持cgi> 解析：cgi的移植性很差 2、下面有关servlet service描述错误的是？ 12345> A.不管是post还是get方法提交过来的连接，都会在service中处理> B.doGet/doPost 则是在 javax.servlet.GenericServlet 中实现的> C.service()是在javax.servlet.Servlet接口中定义的> D.service判断请求类型，决定是调用doGet还是doPost方法> 解析：doget/dopost与Http协议有关，是在 javax.servlet.http.HttpServlet 中实现的 img 3、下列有关Servlet的生命周期，说法不正确的是？ 12345> A.在创建自己的Servlet时候，应该在初始化方法init()方法中创建Servlet实例> B.在Servlet生命周期的服务阶段，执行service()方法，根据用户请求的方法，执行相应的doGet()或是doPost()方法> C.在销毁阶段，执行destroy()方法后会释放Servlet 占用的资源> D.destroy()方法仅执行一次，即在服务器停止且卸载Servlet时执行该方法> 解析：init()不是来创造servlet实例，而是用于初始化。详细见上面的生命周期。选A。 4、下面有关servlet中init,service,destroy方法描述错误的是？ 12345> A.init()方法是servlet生命的起点。一旦加载了某个servlet，服务器将立即调用它的init()方法> B.service()方法处理客户机发出的所有请求> C.destroy()方法标志servlet生命周期的结束> D.servlet在多线程下使用了同步机制，因此，在并发编程下servlet是线程安全的> 解析：选D servlet在多线程下其本身并不是线程安全的。 如果在类中定义成员变量，而在service中根据不同的线程对该成员变量进行更改，那么在并发的时候就会引起错误。最好是在方法中，定义局部变量，而不是类变量或者对象的成员变量。由于方法中的局部变量是在栈中，彼此各自都拥有独立的运行空间*而不会互相干扰，因此才做到线程安全。 5、Struts21、下面有关struts1和struts2的区别，描述错误的是？ 12345> A.Struts1要求Action类继承一个抽象基类。Struts 2 Action类可以实现一个Action接口> B.Struts1 Action对象为每一个请求产生一个实例。Struts2 Action是单例模式并且必须是线程安全的> C.Struts1 Action 依赖于Servlet API，Struts 2 Action不依赖于容器，允许Action脱离容器单独被测试> D.Struts1 整合了JSTL，Struts2可以使用JSTL，但是也支持OGNL> 解析：选B， struts2是多例模式 从action类上分析: 1.Struts1要求Action类继承一个抽象基类。Struts1的一个普遍问题是使用抽象类编程而不是接口。 Struts 2 Action类可以实现一个Action接口，也可实现其他接口，使可选和定制的服务成为可能。Struts2提供一个ActionSupport基类去实现常用的接口。Action接口不是必须的，任何有execute标识的POJO对象都可以用作Struts2的Action对象。从Servlet 依赖分析: Struts1 Action 依赖于Servlet API ,因为当一个Action被调用时HttpServletRequest 和 HttpServletResponse 被传递给execute方法。 Struts 2 Action不依赖于容器，允许Action脱离容器单独被测试。如果需要，Struts2 Action仍然可以访问初始的request和response。但是，其他的元素减少或者消除了直接访问HttpServetRequest 和 HttpServletResponse的必要性。从action线程模式分析: Struts1 Action是单例模式并且必须是线程安全的，因为仅有Action的一个实例来处理所有的请求。单例策略限制了Struts1 Action能作的事，并且要在开发时特别小心。Action资源必须是线程安全的或同步的。 Struts2 Action对象为每一个请求产生一个实例，线程不安全。（实际上，servlet容器给每个请求产生许多可丢弃的对象，并且不会导致性能和垃圾回收问题） document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-08-08T03:08:59.729Z","path":"/posts/0/","text":"time：2019/7/28 author：西芙酱1、Vue 项目创建过程1.1项目搭建过程 创建项目 1# vue init webpack projectname 运行项目 1# npm run dev 1.2 遇到的问题解决方法新手创建时死机是经常出现的事，解决办法如下： 1231、清除缓存 npm clean --force （可能是这个，不太记得了）2、更新 npm install3、chromedriver无法下载，应该是谷歌退出中国网站不让访问，只能下载本地的网址。 1.3 项目前期准备举个栗子： 1.3.1 新建前端界面前端页面开发页面组件存放于src\\components文件夹中，新建一个Login组件。 1234567891011121314151617181920212223242526272829303132333435363738394041 用户名: 密码： 登录 export default { name: 'Login', data () { return { loginForm: { username: '', password: '' }, responseResult: [] } }, methods: { login () { this.$axios .post('/login', { username: this.loginForm.username, password: this.loginForm.password }) .then(successResponse => { if (successResponse.data.code === 200) { this.$router.replace({path: '/index'}) } }) .catch(failResponse => { }) } } } 1.3.2 前端相关配置——设置反向代理修改 src\\main.js 代码如下： 1234567891011121314151617import Vue from 'vue'import App from './App'import router from './router'// 设置反向代理，前端请求默认发送到 http://localhost:8443/apivar axios = require('axios')axios.defaults.baseURL = 'http://localhost:8443/api'// 全局注册，之后可在其他组件中通过 this.$axios 发送数据Vue.prototype.$axios = axiosVue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: ''}) 1.3.3 前端相关配置——设置页面路由修改 src\\router\\index.js 代码如下： 1234567891011121314151617import Vue from 'vue'import Router from 'vue-router'// 导入刚才编写的组件import Login from '@/components/Login'Vue.use(Router)export default new Router({ routes: [ // 下面都是固定的写法 { path: '/login', name: 'Login', component: Login } ]}) 1.3.4 前端相关配置——跨域支持为了让后端能够访问到前端的资源，需要配置跨域支持。 在 config\\index.js 中，找到 proxyTable 位置，修改为以下内容 123456789proxyTable: { '/api': { target: 'http://localhost:8443', changeOrigin: true, pathRewrite: { '^/api': '' } }} 1.4 各种 npm 安装1.4.1 element安装与配置1、安装命令 1npm i element-ui -S 2、main.js 中引入和使用ElementUI 1234import ElementUI from 'element-ui'import 'element-ui/lib/theme-chalk/index.css'Vue.use(ElementUI) ​ 2、SpringBoot 项目创建过程 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"git上传仓库步骤","date":"2019-07-15T09:33:15.538Z","path":"/posts/0/","text":"git上传仓库步骤有关git上传仓库步骤，请享用。 1、新建仓库 2、git config –global user.name “xifujiang” 3、git config –global user.email “229694302@qq.com“ 进入文件夹后初始化 git init git add . git pull –rebase 地址 //该步骤是复制之前的master，使得版本代码相同!! git commit -m “xxx1.version” git push 地址 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"hbase笔记","date":"2019-05-09T07:36:00.000Z","path":"/posts/346bfea7/","text":"通用命令 status: 提供HBase的状态，例如，服务器的数量。 version: 提供正在使用HBase版本。 table_help: 表引用命令提供帮助。 whoami: 提供有关用户的信息。 数据定义语言（表操作命令）。 create: 创建一个表。 list: 列出HBase的所有表。 disable: 禁用表。 is_disabled: 验证表是否被禁用。 enable: 启用一个表。 is_enabled: 验证表是否已启用。 describe: 提供了一个表的描述。 alter: 改变一个表。 exists: 验证表是否存在。 drop: 从HBase中删除表。 drop_all: 丢弃在命令中给出匹配“regex”的表。 Java Admin API: 在此之前所有的上述命令，Java提供了一个通过API编程来管理实现DDL功能。在这个org.apache.hadoop.hbase.client包中有HBaseAdmin和HTableDescriptor 这两个重要的类提供DDL功能。 数据操纵语言 put: 把指定列在指定的行中单元格的值在一个特定的表。 get: 取行或单元格的内容。 delete: 删除表中的单元格值。 deleteall: 删除给定行的所有单元格。 scan: 扫描并返回表数据。 count: 计数并返回表中的行的数目。 truncate: 禁用，删除和重新创建一个指定的表。 Java client API: 在此之前所有上述命令，Java提供了一个客户端API来实现DML功能，CRUD（创建检索更新删除）操作更多的是通过编程，在org.apache.hadoop.hbase.client包下。 在此包HTable 的 Put和Get是重要的类。 创建样本模式 列族 创建表create 'emp', 'personal data', 'perfessional data' 更改列族单元格的最大数目hbase> alter 't1', NAME => 'f1', VERSIONS => 5 表范围运算符 使用alter，可以设置和删除表范围，运算符，如MAX_FILESIZE，READONLY，MEMSTORE_FLUSHSIZE，DEFERRED_LOG_FLUSH等。 #设置只读 hbase>alter 't1', READONLY(option) alter ‘emp’, READONLYalter ‘t1’, METHOD => ‘table_att_unset’, NAME => ‘MAX_FILESIZE’ drop 在删除一个表之前必须先将其禁用。hbase(main):018:0> disable 'emp' hbase(main):019:0> drop 'emp' HBase创建数据put 命令, add() - Put类的方法 put() - HTable 类的方法 使用put命令，可以插入行到一个表。它的语法如下： put '', 'row1', '','' 插入第一行 put 'emp', '1','personal data:name','xifu' put 'emp', '1', 'personal data:city','taizhou' put 'emp', '1', 'perfessional data:designation','manager' put 'emp', '1', 'perfessional data:salary','50000' document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"bat批量存放信息到excel","date":"2019-03-28T06:34:00.000Z","path":"/posts/5fe19fd1/","text":"![Git][git] 最近喵仙子有个需求，需要抽取大量.log文件中的有效信息到excel，刚开始我用mapreduce给他做了一份，不过他有点蠢配不了环境，于是乎我研究了下bat，终于给他整了个好方法，请看下文。 需求 需要从log中抽取两列数据到excel中。 本西芙没了解过bat语法，所以查了一下，通过结合这两篇解答完成bat命令。1、实现了按照空格批处理分割txt数据，存储excelhttps://zhidao.baidu.com/question/1770548208106936980.html123456789101112@echo offrem 保存为bat文件跟txt文件放一起运行md \"结果\\\" 2>nulfor /f \"delims=\" %%a in ('dir /a-d/b *.txt') do ( setlocal EnableDelayedExpansion (for /f \"delims=\" %%b in ('type \"%%a\"') do ( set \"str=%%b\" echo;!str: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 2、批处理分割字符串https://blog.csdn.net/zhju85126com/article/details/46649961123456789101112@echo offset str=100x200x300y400y500x600y700for /f \"tokens=1,3-5,* delims=x|y\" %%a in (\"%str%\") do ( set c1=%%a set c3=%%b set c4=%%c set c5=%%d set c6=%%e)echo %c1%, %c3%, %c4%, %c5%, %c6%pauserem 输出结果为输出结果为：100, 300, 400, 500, 600y700。其中tokens=1,3-5,*表示提取第1、3至5列，同时把第5列后所有剩余字符串作为第6列，一个输出了5个变量，也可以写作tokens=1,3,4,5,*。 最终我所加工的代码12345678910111213@echo offmd \"结果\\\" 2>nulrem 循环数据for /f \"delims=\" %%a in ('dir /a-d/b *.log') do ( setlocal EnableDelayedExpansion rem 切割方式 (for /f \"tokens=7,25 delims=_| \" %%b in ('type \"%%a\"') do ( set \"str1=%%b %%c\" echo;!str1: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 首先根据第一篇解答套入转excel模板，其次根据第二篇解答对bat for循环中分割和选取的描述，修改第五行代码。可以实现。 ###演示1、双击bat 2、生成结果 3、查看生成数据（完成） document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Git","slug":"Git","permalink":"https://xifujiang.github.io/tags/Git/"}]},{"title":"python笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/c00b24b1/","text":"一、基本语法1、ipython命令技能实现python命令，也能实现部分linux命令。2、python2不能识别文件里的中文，如果硬要识别，在头文件中加上 -- coding:utf-8 -- （python官方推荐这种方式）3、输入一个数 1high = input(\"请输入一个数\") 4、输出一个数 12345age = 18print(\"age的值为%d\"%age)name = \"西芙\"print(\"name的值为%s\"%name);print(\"name的值是%s,年龄的值是%d\"%(name,age)) 4、if else语句12345678910age = 19if age>18: print(\"已经成年\")else print(\"未成年\")#input获取的所有数据都会当成字符串类型#python 规定 str（）>int()#所以如果 age = input(\"一个值\")#要与int比较，需要把age换成int类型，即#age_num = int(age) 5、python语言是弱类型的编程语言，及赋值的时候已经知道是什么类型6、查看变量类型 1type(a) 7、python的变量类型123456·Numbers(数字)： int(有符号整型)，long（长整型[也可代表八进制和十六进制]），float（浮点型），complex（复数）·布尔类型： True，False·String（字符串）·List（列表）·Tuple（元组）·Dictionary（字典） 8、关键字查看关键字命令 12import keyword #导入keyword包keyword.kwlist 9、运算符 12345\"s\"*10#输出'ssssssssss'#幂#** 10、is是地址相同的意思，在数值-5到256间，a与b地址相同，其余不同￼11、深拷贝与浅拷贝123456789101112131415161718192021222324252627282930313233#例1、浅拷贝a = [11, 22, 33]b = a #浅拷贝，地址拷贝，地址相同#例2、深拷贝import copyc = copy.deepcopy(a) #开辟一个内存空间，拷贝内容#例3、在数组中，深拷贝数组后，a = [11,22,33]b = [44,55,66]c = [a, b]d = copy.deepcopy[c]#查看c，d的id，并不相同，深拷贝#在a数组中添加[44],查看c[0],d[0]a.append(44)c[0][11,22,33,44]d[0][11,22,33]#说明深拷贝后，d另外拷贝了有关a、b的拷贝#例4、有关指向a，b的拷贝e = copy.copy(c)a.append(55)c[0]e[0]#所看到的结果都是[11,22,33,44,55]id(c)id(e)#所看到的地址不同，所以他们是指向不同的地址但所指向数组的内容地址是相同的#但如果是c = (a,b)，是元组的话，由于元组是不可变类型，e = copy.copy(c),他们所指向的地址是一样的#所以使用copy模块的copy功能时候，它会根据当前拷贝的数据类型是可变类型还是不可变类型有不同的处理方式 12、私有化12345678910class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__numt = Test()t.__num = 200 #可以使用print(t.__num) 升级版私有化 property使用123456789class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__num num = property(getNum, setNum)#!!!!#此时，可以用t.num = ?赋值 或者 t.num 取值，不用调用函数 装饰器使用12345678910class Test(object): def __init__(self): self.__num = 100 @property def num(self): return self.__num @num.setter def num(self, newNum): self.__num = newNum#与上面的代码意思相同。理解一下吧~ 1、xx:公有变量2、x:单前置下划线，私有化属性或方法，from somemodule import * 禁止导入，类对象和子类可以访问3、__x:双前置下划线，避免与子类中的属性命名冲突，无法在外部直接访问（名字重整所以访问不到）4、x：双前后下划线，用户名字空间的魔法对象或属性。例如init,__不要自己发明这样的名字5、xx_:单后置下划线，用于避免与python关键词的冲突 ###但是私有属性实际上是可以访问，python把私有属性的名字由原来的_私有属性名 改成 _类名私有属性名，所以可以通过对象._类名__私有属性名可以获取值！！！ 13、迭代器迭代器 迭代器是访问集合元素的一种方式，迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，知道所有的元素被访问完结束。迭代器只能往前不会后退。可迭代对象： 可直接作用于for循环的数据类型有以下几种： 一类是集合数据类型：如list、tuple、dict、set、str等 一类是generator，包括生成器和带yield的generator function。 可以直接作用于for循环的对象统称为可迭代对象：Iterable。 14、闭包：定义：函数里面有另外的函数，并且里面的函数用到了外面的函数的变量闭包的应用：1234567891011121314def test(number): print(\"--1--\") def test_in(number2): print(\"--2--\") print(number+number2) print(\"--3--\") return test_in ret = test(100)print(\"-\"*30)ret(1)ret(100)ret(200)#优点 简化了步骤 linux快捷键： %s/^/#/g 所有行前面加# 1,14/#//g 去掉1-14行的#15、装饰器12345678910111213141516171819def w1(func): def inner(): print(\"----正在验证权限---\") func() return innerdef f1(): print(\"---f1---\")def f2(): print(\"---f2---\")#innerFunc = w1(f1)#innerFunc()f1 = w1(f1) #把w1（f1）赋值给f1，也就是说f1 = w1.inner，f1指向的就是inner这个函数f1() #调用后，执行的是inner，但由于之前传入f1的参数，所以输出结果为inner的输出结果+f1函数的输出结果@w1 #与上面f1=w1(f1)意义相同，但只要python执行器执行到了这个代码，name就会自动的进行装饰，而不是等到调用的时候才装饰的 语法糖 @w1def f1(): print(\"---f1---\") 12345678910111213141516171819#若有参数def func(functionName): print(\"---func---1---\") def func_in(*args, **kwargs):#如果没有定义参数，会导致调用的时候出现问题 print(\"---func_in---1---\") functionName(*args, **kwargs)#传入同样的参数 print(\"---func_in---2---\") print(\"---func---2---\") return func_in@funcdef test(a, b, c): print(\"---test-a=%d,b=%d,c=%d---\"%(a,b,c)) @funcdef test2(a, b, c, d): print(\"---test-a=%d,b=%d,c=%d,d=%d---\"%(a,b,c,d))test(11,22,33)test2(44,55,66,77) 12345678910111213141516171819202122232425262728#通用装饰器def func(functionName): def func_in(*args, **kwargs): print(\"---记录日志---\") ret = functionName(*args, **kwargs) return ret return func_in@funcdef test(): print(\"---test---\") return \"haha\"@funcdef test2(): print(\"---test2---\")@funcdef test3(a): print(\"---test3---a=%d--\"%a) ret = test()print(\"test return value is %s\"%ret)a = test2()print(\"test2 return value is %s\"%a)test3(11) 16、作用域 #什么是命名空间在某个范围内所能用到的作用域 17、类方法与静态方法1234567891011121314151617#给类添加方法import types #导入types包class Person(object): def __init__(self, newName, newAge): self.name = newName self.age = newAge def eat(self): print(\"-----%s正在吃----\"%self.name)def run(self): print(\"-----%s正在跑----\"%self.name) p1 = Person(\"p1\", 10)p1.eat()p1.run = types.MethodType(run, p1)p1.run() 18、限制class实例添加的属性123456class Person(object): __slots__ = (\"name\", \"age\")p = Person()p.name = \"老王\"p.age = 20p.score = 100 #会报错！！！ 二、生成器、迭代器、装饰器、闭包19、生成器 12345678910In [1]: a = [x*2 for x in range(10)]In [2]: aOut[2]: [0, 2, 4, 5, 6, 10, 12, 14, 16, 18]#如果用[]，会生成数组，同时加载到内存中#如果用括号（），输出的是一个地址，需要时才提取出In [3]: b = (x*2 for x in range(10))In [4]: bOut[4]: In [5]:next(b)Out[5]:0 20、a，b交换123456789101112131415def creatNum(): print(\"----start----\") a, b = 0, 1 for i in range(5): print(\"----1----\") yield b #python中有一个非常有用的语法叫做生成器，所利用到的关键字就是yield。有效利用生成器这个工具可以有效地节约系统资源，避免不必要的内存占用。当执行到这的时候，会做停顿。 print(\"----2----\") a, b = b, a+b print(\"----3----\") print(\"----stop----\")#创建了一个生成器对象a = creatNum()#实现交换a, b = 0, 1a, b = b, a 21、斐波拉契数列12a,b = 0, 1a,b = b, a+b #重复此步骤 22、类当做装饰器12345678910111213141516171819202122232425262728#1class Test(object): def __call__(self): print(\"---test---\") t = Test()t()#out: ---test---#2class Test(object): def __init__(self, func): print('---初始化---') print('func name is %s'func.__name__) self.__func = func def __call__(self): print('---装饰器中的功能---') self.__func()@Test #相当于 t = Test(test) def test(): print('---test---')#out: ---初始化---#out:func name is 'test'test()#out: ---装饰器中的功能#out:---test--- 23、元类1234567891011Test = type(\"Test\", (), {})#相当于创建一个类Test#out:__main.Test#创建一个拥有属性num=0的Person类Person = type(\"Person\",(),{\"num\":0})#创建一个拥有方法printNum()的Test3类def printNum(self): print(\"--num-%d--\"%self.num)Test3 = type(\"Test3\",(),{\"printNum\":printNum}) 123456789101112131415161718192021222324#设置Foo类的属性变大写！！，通过metaclass设置def upper_attr(future_class_name, future_class_parents, future_class_attr): #Foo object {bar:\"bip\"} #遍历属性字典，把不是__开头的属性名字变成大写 newAttr = {} for name, value in future_class_attr.items(): if not name.startswith(\"__\"): newAttr[name.upper()] = value #调用type创建一个类 return type(future_class_name,future_class_parents,newAttr)class Foo(object,metaclass=upper_attr): # __metaclass__ = upper_attr #设置Foo类的元类为upper_attr python2 bar = \"bip\"print(hasattr(Foo, \"bar\"))print(hasattr(Foo, \"BAR\"))f = Foo()print(f.BAR)#out:False#out:True#out:bip 24、内建属性123456789101112131415161718class Itcast(object): def __init__(self,subject1): self.subject1 = subject1 self.subject2 = 'cpp' #属性访问时拦截器，打log def __getattribute__(self,obj): #obj-->\"subject1\" if obj == 'subject1': print('log subject1') return 'redirect python' else: return object__getattribute__(self,obj) def show(self): print('this is Itcast') s = Itcast(\"python\")print(s.subject1)print(s.subject2) 三、内建方法25、lambda之map123456789101112131415#函数需要一个参数map(lambda x: x*x, [1, 2,3])#结果为：[1, 4, 9]#函数需要两个参数map(lambda x, y: x+y, [1, 2, 3], [4, 5, 6])#结果为:[5,7,9]def f1(x, y): return (x, y)l1 = [0,1,2,3,4,5,6]l2 = ['Sun', 'M', 'T', 'W', 'T', 'F', 'S']l3 = map(f1, l1, l2)print(list(l3))结果为：[(0,'Sun'), (1,'M').....] 26、lambda之filter12345fliter(lambda x : x%2, [1, 2 ,3, 4]) #如果为1，则输出[1, 3]filter(None, \"she\")'she' 27、lambda之reduce12345678reduce(lambda x, y: x+y, [1,2,3,4])10 #先把1赋值给x，2赋值给2，x+y后=3，把3赋值给x，数组中的3赋值给y，再累加，以此类推reduce(lambda x, y:x+y, [1,2,3,4], 5)15 #如果前面是数组，先把5赋值给x，再累加[1,,2,3,4]reduce(lambda x, y: x+y, ['aa', 'bb', 'cc'], 'dd')ddaabbcc 28、sort123456789a = [9,8,7,6,5,4,3,2,1]a.sort()a#[1,2,3,4,5,6,7,8,9]b = ['dd','cc','bb','aa']b.sort()b#['aa','bb','cc','dd'] 28、python的functools包中提供了一个叫wraps的装饰器来消除这样的副作用1234567891011121314151617import functoolsdef note(func): \"note function\" @functoolswraps(func) def wrapper(): \"wrapper function\" print(\"note something\") return func() return wrapper@notedef test(): \"test function\" print('I am test')test()print(test.__doc__) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"HDFS上传和读取流程","date":"2019-03-28T06:34:00.000Z","path":"/posts/e4062a35/","text":"![Git][git] 上传流程 1.根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。 2.namenode返回是否可以上传 3.client请求第一个 block该传输到哪些datanode服务器上 4.namenode返回3个datanode服务器ABC 5.client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将这个pipeline建立完成，逐级返回客户端 6.client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 7.当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。读取流程客户端读取HDFS数据相比写入数据要简单一些，以下是读取数据步骤： 1.client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 2.就近挑选一台datanode服务器，请求建立输入流。 3.开始读取这个数据的第一个block块，读取完全之后，开始接着读取这个文件的第二个block，直至把这个数据所有的block都读完了则文件读取完全了。 4.数据读完之后关闭流连接。 5.如果读取过程当中读取失败，将会依次读取该数据块的下一个副本，失败的节点将会被记录，不再连接。 ￼# SecondaryNameNodeSecondaryNameNode 是 HDFS 架构中的一个组成部分，它用来保存名称节点中对HDFS元数据信息的备份，减小Editlog文件大小，从而缩短名称节点重启的时间。 它一般是单独运行在一台机器上。 SecondaryNameNode让EditLog变小的工作流程如下: (1)SecondaryNameNode 会定期和 NameNode 通信，请求其停止使用 EditLog 文件，暂时将 新的写操作写到一个新的文件 edit.new 中，这个操作是瞬间完成的，上层写日志的函数完全感觉不到差别。 (2) SecondaryNameNode 通过 HTTP GET 方式从 NameNode 上获取到 Fslmage 和 EditLog 文 件，井下载到本地的相应目录下。 (3) SecondaryNameNode 将下载下来的 Fslmage 载入到内存，然后一条一条地执行 EditLog 文件中的各项更新操作，使内存中的 Fslmage 保持最新。 这个过程就是 EditLog 和 Fslmage 文件合 井。 (4) SecondaryNameNode 执行完（3）操作之后，会通过 post 方式将新的 Fslmage 文件发送 到 NameNode 节点上 (5) NameNode 将从 SecondaryNameNode 接收到的新的 Fslmage 替换旧的 Fslmage 文件，同 时将 Edit.new 替换 EditLog 文件，从而减小 EditLog 文件大小。￼从上面的过程可以看出，第二名称节点相当于为名称节点设置一个“检查点” ，周期性备份 名称节点中的元数据信息，但第二名称节点在 HDFS 设计中只是一个冷备份，并不能起到“热备 份”的作用。 HDFS 设计并不支持当名称节点故障时直接切换到第二名称节点。 HDFS FederationHDFSl.O 的单 NameNode 设计不仅存在单点故障问题，还存在可扩展性和性能问题。只有一 个 NameNode， 不利于水平扩展。 HDFS Federation (HDFS 联邦）特性允许一个 HDFS 集群中存在 多个 NameNode 同时对外提供服务，这些 NameNode 分管一部分目录（水平切分），彼此之间相 互隔离，但共享底层的 DataNode 存储资源。每个 NameNode 是独立的，不需要和其他 Nam巳Node 协调合作。 如图 4-5 所示 ， Federation 使用了 多 个独立的 NameNode/NameSpace 命名空间。这些 NameNode 之间是联合的，也就是说， 它们之间相互独立且不需要互相协调，各自分工管理自己的 区域。分布式的 DataNode 被用作通用的数据块存储设备。每个 DataNode 要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告， 并执行来自所有 NameNode 的命令。每一个 DataNode 作为统一的块存储设备被所有 NameNode 节点使用。 每一个 DataNode 节点都在所有的 NameNode 进行注册。 DataNode 发送心跳信息、块报告到 所有 NameNode，同时执行所有 NameNode 发来的命令。￼ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Linux复习","date":"2019-03-28T06:34:00.000Z","path":"/posts/0/","text":"Linux复习第一章linux系统诞生于1991年，由芬兰大学生李纳斯（Linux Torvalds）。 文件处理命令 ls （-lh…） -rw-r--r-- 1 root root 8690 3月 1 13:23 install.log.syslog 1是引用计数（代表此文件被调用几次 引用几次） 文件所有者 文件所属组 字节文件大小 最后修改时间 文件名称 -rw-r--r-- - 文件类型（- 二进制文件 d目录 l软连接文件） rw- r-- r-- u g o u所有者 g所属组 o其他人 r读 w写 x执行 cd pwd mkdir 网络通信命令 ping ifconfig 系统关闭命令 shutdown reboot 重启系统 文本编辑器 vim/vi 虚拟机安装 CPU：建议主频为1GHz以上 内存：建议1GB以上 硬盘：建议分区空闲空间8GB以上 linux分区 boot 400mb home 分区， 4G swap 交换分区2G 没有挂载点，就是没有盘符，swap不是给用户用的，是给操作系统或内科自己调用的 设置为2G 超过两个G就没有意义了 逻辑分区 / 剩余所有空间 第四个分区，linux不知道要分几个区，所有先创建一个sda5（第一个逻辑分区），一块硬盘只能有4个分区是第四个默认为逻辑分区，三个主分区，一个逻辑分区 系统磁盘分区 基本分区（primary partion） 扩充分区（extension partion） 基本分区和扩充分区的数目之和不能大于四个。且基本分区可以马上被使用单不能再分区。扩充分区必须再进行分区后才能使用，也就是说它必须还要进行二次分区。扩充分区下面是逻辑分区（logical partion），逻辑分区没有数量上的限制。 主分区：最多只能有4个。 扩展分区： 最多只能有1个。 主分区加扩展分区最多有4个。 不能写入数据，只能包含逻辑分区 分区格式化目的：为了给这个分区写入文件系统，也就是告诉我们的操作系统，如何去管理这个分区的数据。 分区：把大硬盘分为小的逻辑分区 格式化：写入文件系统 分区设备文件名：给每个分区定义设备文件名 挂载：给每个分区分配挂载点 虚拟机网络连接方式 桥接模式 NAT模式 仅主机模式 自定义模式 桥接模式 你的虚拟机是利用你的真实网卡，一块本地有线网卡(以太网)，一块是本地无线网卡（wlan）装完虚拟机后会出现两块虚拟网卡，一块vmnet1，一块vmnet8，如何选择桥接你的虚拟机将会利用你的网卡以太网或无线网卡和你的真实机进行通信 好处是配置简单，和你的真是机的ip地址设置同一网段，和真机进行通信，局域网的其他电脑也可以通信。 缺点占用同意网段的一个ip地址，在家里宿舍没有问题，在教室可能会和其他同学的ip地址冲突。 NAT模式你的虚拟机是通过VMnet8这块假的虚拟网卡和你的真实机连接 仅主机模式你的虚拟机是通过VMnet1这块假的虚拟网卡和你的真实机连接。 1234桥接是不仅和你的真机通信还可以与你的局域网的其他机器通信，和一台笔记本是一个意思。Nat hostonly 只能和你真机通信，不可以和局域网其他电脑通信，不用占用你的真实网段的一个ip地址。Hostonly 只能和你计算机主机通信。NAT和主机通信，如果你的主机可以联网，虚拟机也可以联网。 df盘符分区查询df （-h）文件系统 1k-块 已用 可用 已用% 挂载点 文件处理命令文件处理命令 ls 查看目录下的文件 -a 显示所有文件，包括隐藏文件 -l 详细信息显示 -d 查看目录属性 -i Inode索引节点 mkdir 创建目录 -p 创建递归 cd 切换目录 pwd 显示当前目录 rmdir 删除空目录 cp 复制文件或目录 -rp 【源文件或目录】 【目标目录】 -r 复制目录 -p 保留文件属性 clear 清屏操作 mv 【源文件或目录】【目标目录】 剪切文件、改名 rm 删除文件 rm -rf 【文件或目录】 -r 删除目录 -f 强制执行 touch 创建空文件 cat 显示文件内容 -n 显示行号 more 分页显示文件内容 （空格）或f 翻页 （Enter） 换行 q或Q 退出 less 分页显示文件内容 ln 生成链接文件 -s创建软连接 软链接特征：类似windows快捷方式 lrwxrwxrwxl软链接 文件大小——只是符号链接 /tmp/issue.soft -> /etc/issue 箭头指向源文件 硬链接特征： 1、拷贝cp -p + 同步更新 2、通过i节点识别 3、不能跨分区 4、不能针对目录使用 shutdown命令 -c 取消前一个关机命令 -h 关机 -r 重启 vi/vim 建立、编辑、显示文本文件 vim是一个功能强大的全屏幕文本编辑器，是Linux/UNIX上最常用的文本编辑器，它的作用是建立、编辑、显示文本文件。 vim没有菜单，只有命令。 vim工作模式 插入命令 命令 作用 a 在光标所在字符后插入 A 在光标所在行尾插入 i 在光标所在字符前插入 I 在光标所在行行首插入 o 在光标下插入新行 O 在光标上插入新行 定位命令 命令 作用 :set nu 设置行号 :set nonu 取消行号 gg 到第一行 G 到最后一行 nG 到第n行 :n 到第n行 $ 移至行尾 0 移至行首 删除命令 命令 作用 x 删除光标所在处字符 nx 删除光标所在处后n个字符 dd 删除光标所在行，ndd删除n行 dG 删除光标所在行到文件末尾内容 D 删除光标所在处到行尾内容 n1,n2d 删除指定范围的行 复制和剪切命令 命令 作用 yy 复制当前行 nyy 复制当前行以下n行 dd 剪切当前行 ndd 剪切当前行以下n行 p、P 粘贴在当前光标所在行下或行上 搜索和搜索替换命令 命令 作用 /string 搜索指定字符串 搜索时忽略大小写：set ic n 搜索指定字符串的下一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/g 在一定范围内替换指定字符串 保存和退出命令 命令 作用 :w 保存修改 :w new_filename 另存为指定文件 :wq 保存修改并退出 zz 快捷键，保存修改并退出 :q! 不保存修改退出 :wq! 保存修改并退出（文件所有者及root可使用） 权限管理命令 chmod 改变文件或目录（权限） -R 递归修改 权限的数字表示 r——4 w——2 x——1 文件目录权限总结 代表字符 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以在目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 chown 改变文件或目录的（所有者） chmod 【用户】【文件或目录】 chgrp 改变文件或目录的（所属组） chgrp 【用户组】【文件或目录】 Groupadd 所有组名称新的命令 umask 显示、设置文件的缺省权限 umask -S 以rwx形式显示新建文件缺省权限 在linux创建文件默认权限没有x权限，防止病毒可执行脚本 在linux创建目录默认权限有x 文件搜索命令 find 【搜索范围】【匹配条件】 尽量不要使用linux里的find命令浪费资源特别是使用高峰期的时候，目录和文件合理的命名存放位置最重要。 Find 路径 选项 关键字 完全搜索 Find / -name a 模糊搜索 Find / -name a 模糊搜索 Find / -name *a?? 模糊搜索 Find / -iname *a?? （iname不区分大小写） Find -size +n -n n 查询文件大小 (+大于 -小于 什么都不写 =) Find -user dj 根据所有者查找文件 find /etc -cmin -5 在/etc下查找5分钟内被修改过属性的文件和目录（+超过多长时间 -在多少分钟内） amin 访问时间 access cmin 文件属性 change mmin 文件内容 modify 帮助命令用户管理命令简介：所以越是对服务器安全性要求高的服务器， 越需要建立合理的用户权限登记制度和服务器操作规范。 在linux中主要是通过用户配置文件来查看好修改用户信息。 #1、添加更多的用户分配不同的权限。 #2、通过配置文件添加信息，使用命令重启可能就消失了。 1、用户配置文件——用户用户信息文件 /etc/passwd 第1字段：用户名称 第2字段：密码标志 第3阶段：UID（用户ID） 0： 超级用户 1-499： 系统用户（伪用户） 500-65535： 普通用户 第4字段：GID（用户初始组ID） 第5字段：用户说明 第6字段：家目录 普通用户：/home/用户名/ 超级用户：/root/ 第7字段：登录之后的Shell 初始组和附加组 初始组：就是指用户一登录就立刻拥有这个用户组的相关权限，每个用户的初始组只能有一个，一般就是和这个用户名相同的组名作为这个用户的初始组。 附加组：指用户可以加入多个其他的用户组，并拥有这些组的权限，附加组可以有多个。 shell是什么？ 1、shell就是Linux的命令解释器 2、在/etc/passwd当中，除了标准shell是/bin/bash之外，还可以写如/sbin/nologin。 2、用户配置文件——影子1、影子文件/etc/shadow 第1字段：用户名 第2字段：加密密码 加密算法升级为SHA512散列加密算法 如果密码位是“!!”或“*”代表没有密码，不能登录 第3字段：密码最后一次修改日期 使用1970年1月1日作为标准时间，每过一天时间戳加1 第4字段：两次密码的修改间隔时间（和第三字段相比） # 0代表随时可以修改密码，10代表10天后才能修改密码 第5字段：密码有效期（和第3字段相比） 第6字段：密码修改到期前的警告天数（和第5字段相比） 第7字段：密码过期后的宽限天数（和第5字段相比） 0：代表密码过期后立即失效 -1：则代表密码永远不会失效。 第8字段：账号失效时间 （到了不管过期 6 7） 要用时间戳表示 第9阶段：保留 2、时间戳换算 把时间戳换算为日期 >> date -d \"1970-01-01 16066 days\" 把日期换算为时间戳 >> echo $(($(date --date=\"2014/01/06\" +%s)/86400 +1)) 3、用户配置文件——组件1、组信息文件/etc/group 第1字段：组别 第2字段：组密码标志 第3阶段：GID 第4字段：组中附加用户 2、组密码文件/etc/gshadow 第1字段：组别 第2字段：组密码 第3阶段：组管理员用户名 第4字段：组中附加用户 4、用户管理相关文件用户的家目录 普通用户：/home/用户名/，所有者和所属组都是此用户，权限是700 超级用户：/root/，所有者和所属组都是root用户，权限是550 5、用户管理命令 useradd命令 useradd[选项] 用户名 -u UID：手工指定用户的UID号 -d 家目录 手工指定用户的家目录 -c 用户说明 手工指定用户的说明 -g 组名 手工指定用户的初始组 -G 组名 指定用户的附加组 -s shell 手工指定用户的登录shell。默认是/bin/bash 例 ：添加默认用户 >> useradd sc >> grep sc /etc/passwd >> grep sc /etc/shadow >> grep sc /etc/group >> grep sc /etc/gshadow passwd命令格式 passwd[选项] 用户名 -S 查询用户密码的密码状态。仅root用户可用。 -l 暂时锁定用户。仅root用户可用。 -u 解锁用户。仅root用户可用。 --stdin 可以通过管道符输出的数据作为用户的密码。 修改用户信息usermod usermod[选项] 用户名 -u UID 修改用户的uid号 -c 用户说明 修改用户的说明信息 -G 组名 修改用户附加组 -L 临时锁定用户(lock) -U 解锁用户锁定（Unlock） 例 ：修改用户信息 >> usermod -c \"test user\" xifu #修改用户的说明 >> usermod -G root xifu #把西芙用户加入到root组 >> usermod -L xifu #锁定用户 >> usermod -U xifu #解锁用户 修改用户密码状态chage chage[选项] 用户名 -l 列出用户的详细密码状态 -d 日期 修改密码最后一次更改时间（shadow3字段） -m 天数 两次密码修改间隔（4字段） -M 天数 密码有效期（5字段） -W 天数 密码过期前警告天数（6字段） -l 天数 密码过期后宽限天数（7字段） -E 日期 账号失效时间（8字段） 例：修改密码状态 >> chage -d 0 xifu #这个命令其实是把密码修改日期归0了（shadow第3字段） 这样用户一登录就要修改密码 删除用户userdel userdel[-r] 用户名 -r 删除用户的同时删除用户家目录 切换用户身份su su [选项] 用户名 - 选项只使用“-”代表连带用户的环境变量一起切换 -c命令 仅执行一次命令，而不切换用户身份 压缩解压命令网络命令 write 给用户发信息 网络 w命令查看用户在线情况 打开两个窗体（写错的时候 退格键 ctrl+backspace键） 写好了 ctrl+D保存结束 远程终端第一个0 终结符EOF wall 【message】 发广播信息 广播信息 在线用户 ping 测试网络连通性 ping 选项 IP地址 -C 指定发送次数 ifconfig 查看和设置网卡信息 eth0第一块网卡 Ethernet网络昵称以太网 网络类型目前我们接触的都是以太网 Hwaddr 网卡的物理地址 Inet addr 当前计算机地址 Bcast 发送广播的ip地址 Mask 子网掩码网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分 Rx接受数据包数量 byte 接收到的数据包的总大小 TX发送数据包数量 Interrupt 网卡在内存中的物理地址 Lo回环网卡每台机器都有用来做本机网络测试的 mail 查看发送电子邮件 不在线也能收到 收到直接数据mail 直接回车 n代表没有读的邮件 1代表一份邮件 输入1回车查看第一份信内容 last 列出目前与过去登入系统的用户信息 计算机所有用户登录系统信息 dj pts/1 192.168.40.1 Thu Mar 7 13:04 still logged in 第二个远程终端 登录的远程ip 一直在登录 netstat 【选项】 显示网络相关信息 选项 用途 -t TCP协议 -u UDP协议 -l 监听 -r 路由 -n 显示IP地址和端口号 范例 netstat -tlun 查看本机监听的端口 netstat -an 查看本机所有的网络连接 netstat -rn 查看本机路由器 netstat -ntlp Tcp http 用的协议 三次握手 安全可靠 传输 打电话 Udp 快 发短信 端口 ip地址为公司名字 找某人 就是端口 Destination Gateway Genmask Flags MSS Window irtt Iface setup 配置网络 dhcp 自动分配自动获取服务 * 默认相当于windows自动获取ip地址没有用，原因个人电脑和家里的环境不会有路由分配。 setup操作结束后 使用 service network restart命令 关机重启命令 shutdown命令 -c 取消前一个关机命令 -h 关机 -r 重启 linux shell基础shell概述 shell是什么 shell是一个命令行解释器。 它为用户提供了一个向linux内核发送请求以便运行程序的界面系统级程序。 用户可以用shell来启动、挂起、停止甚至是编写一些程序。 注释： 内核 机器语言01010 外层 pwd ls命令 shell就是黑色及交互命令窗体 shell还是一个功能相当强大的编程语言，易编写，易调试，灵活性较强。shell是解释执行的脚本语言，在shell中可以直接调用Linux系统命令。 shell的分类 1、Bourne Shell: 从1979起Unix就开始使用Bourne Shell，Bourne Shell的主文件名为sh。 2、 C Shell： C Shell主要在BSD版的Unix系统中使用，其语法和C语言相类似而得名。 shell的两种语法类型有Bourne和C，这两种语法彼此不兼容。Bourne家族主要包括sh、ksh、Bash、psh、zsh；C家族主要包括：csh、tcsh linux标准shell是伯恩 shell bash ei Bash:Bash与sh兼容，现在使用的linux就是使用bash作为用户的基本shell。 linux支持的shell /etc/shells shell脚本运行 echo输出命令 echo [选项] [输出内容] -e 支持反斜线控制的字符转换 转移符 echo -e ‘\\e[1,31m abcd \\e[0m’ 变色 第一个脚本 > vi hello.sh #linux不区分扩展名,告诉linux为base脚本vim有颜色 > > #!/bin/Bash # #为注释，第一行标志shell脚本 脚本执行 >> 1、赋予执行权限，直接运行 >> chmod 755 hello.sh >> ./hello.sh >> 2、或者通过Bash调用执行脚本 >> Bash hello.sh linux标准shellbase 历史命令 history [选项] [历史命令保存文件] -c 清空历史记录 -W 把缓存中的历史命令写入历史命令保存文件 ~/.bash_history * 历史命令默认会保存1000条，可以在环境变量配置文件/etc/profile中进行修改 正常退出后才写入到history文件里，黑客基本就会清空，太多了就会删掉第一条，正在保存在vim /root/.bash_history。 命令与文件补全 在bash中，命令与文件补全是非常方便与常用的功能，我们只要在输入命令或文件时，按“Tab”键就会自动进行补全。 bash常用快捷键 标准输入输出 设备 设备文件名 文件描述符 类型 键盘 /dev/stdin 0 标准输入 显示器 /dev/sdtout 1 标准输出 显示器 /dev/sdterr 2 标准错误输出 输出重定向 1、夜里12点执行一些命令 保存到文件里 输出重定向最终结果 把输出结果给管理员随时查看 必须保证命令有输出信息 2、1>/dev/null 2>&1 将错误绑定到标准输出上， 标准输出重定向到/dev/null中， 所有到这个下的命令不会被保存， 就像黑洞一样被吸引， 该条shell命令将不会输出任何信息到控制台， 也不会有任何信息输出到文件中。 通常用于运行shell命令 输入重定向 命令ll -a/etc | more >netstat -an | grep \"ESTABLISHED\" >more只能看文件内容 >命令1必须正确输出 多个管道符嵌套目的就是得到正确命令输出 gerp grep [选项] “搜索内容” -i 忽略大小写 -n 输出行号 -v 反向查找 --color=auto 搜索出的关键字用颜色显示 通配符 例：通配符 >cd /tmp >rm -rf * >touch abc >touch abcd >touch 012 >ls ?abc >ls [0-9]* >ls [^0-9]* bash中其他特殊符号 双引号与单引号 > name=sc > echo ‘$name’ > echo “$name” > echo ‘$(date)’ > echo “$(date)” xifu@xifu-virtual-machine:~$ echo \"$name\" xifu@xifu-virtual-machine:~$ echo '$(date)' $(date) xifu@xifu-virtual-machine:~$ echo \"$(date)\" 2019年 05月 15日 星期三 20:37:30 CST xifu@xifu-virtual-machine:~$ echo \"hahahha $(date)\" hahahha 2019年 05月 15日 星期三 20:37:35 CST 反引号与$() > echo ‘ls’ > echo $(date) xifu@xifu-virtual-machine:~$ echo $(date) 2019年 05月 15日 星期三 20:39:35 CST shell变量 变量的定义 变量是计算机内存的单元，其中存放的值可以改变。 如shell脚本需要保存一些信息时，如一个文件名或是一个数字，就把它存放在一个变量中。 每个变量有一个名字，所以很容易引用它。 使用变量可以保存有用信息，使系统获知用户相关设置，变量也可以用于保存暂时信息。 变量设置规则 变量分类 例：用户自定义变量 >name=\"xifu\" #变量定义 >#变量叠加 >aa=123 >aa=\"$aa\"456 >aa=${aa}789 >echo $name #变量调用 >set #变量查看 > unset name #变量删除 环境变量： 用户自定义变量只在当前的shell中生效， 而环境变量会在当前shell和这个shell的所有子shell中生效。 如果把环境变量写入相应的配置文件， name这个环境变量就会在所有的shell中生效。 环境变量语法 >export 变量名=变量值 #申明变量 >env #查询变量 >unset 变量名 #删除变量 系统常见环境变量 系统命令操作直接敲即可原因是写入到path里了，此操作为临时生效自己的脚本也可以这么做，写好脚本，设置权限，cp文件到bin 输入文件名操作系统的命令就是吧当前命令写到了path具体每个命令路径地址下的，所以会自动扫描找到每个目录下的就执行，没有找到就报错 位置参数变量 预定义变量 && || 是通过什么第二命令知道第一命令是否正常运行的呢 其实是通过$?的数字才判断的 程序员用户眼睛判断命令是否正确 计算机是通过$?判断命令是否正确 接收键盘输入 read[选项][变量名] -p “提示信息”：在等待read输入时，输出提示信息 -t 描述 read命令会一直等待用户输入，使用此选项可以指定等待时间 -n 字符数 read命令只接受指定的字符数，就会执行 -s 隐藏输入的数据，适用于机密信息的输入 declare声明变量类型 declare [+/] [选项] [变量名] - 给变量设定类型属性 + 取消变量的类型属性 -i 将变量声明为整数型（integer） -x 将变量声明为环境变量 -p 显示指定变量的被声明的类型 例：方法一，数值运算——方法1 >aa=11 >bb=22 >给变量aa和bb赋值 >declare -i cc=$aa+$bb 例：方法二，expr或let数值运算工具 >aa=11 >bb=22 >#给变量aa和bb赋值 >dd=$(expr $aa + $bb) >#dd的值是aa和bb的和。注意“+”号左右两侧必须有空格 例：方法三，”$((运算式))” 或”$[运算式]” >aa=11 >bb=22 >ff=$(( $aa+$$bb )) >gg=$[ $aa+$$bb ] #单个小括号是系统命令 #双个小括号代表数值运算符 修改环境变量从新登陆才能生效 用source直接生效 .有空格 Path 命令路径 Histsize 历史命令保存条数 Ps1 提示符 Hostname 主机名 [root@localhost ~]# echo $PATH /usr/lib/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root [root@localhost ~]# PATH=\"$PATH\":/root [root@localhost ~]# echo $PATH /usr/lib/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root:/root [root@localhost ~]# echo $HOSTNAME set 一组文件目录下所有文件etc目录下对所有用户生效,家目录当前目录生效/etc目录下所有登录用户都生效 ~代表家目录 1、家目录是用户的/home目录，其分为两种情况(1)、普通用户/home/linuxprobe (linuxprobe是设置的用户名，也可以自己设置) 1(2)、root用户你修改了哪个用户的家目录就对哪个用户有效 /etc/profile的作用 默认环境变量配置文件 正则表达式 条件判断 -if desc 'test2' [root@localhost tmp]# cat sys.sh #!/bin/bash rate=$(df -h | grep /dev/sda3 | awk '{print $5}'|cut -d \"%\" -f1) if [ $rate -ge 1 ] then echo \"/dev/sda3 is full\" fi #!/bin/bash rate=$(df -h | grep /dev/sda3 | awk '{print $5}'|cut -d \"%\" -f1) if [ $rate -ge 1 ] then echo \"$(date) /dev/sda3 is full\" >> /tmp/shellwaring else echo \"$(date) /dev/sda3 is not full\" >> /tmp/shellinfo fi 多分支case条件语句 条件判断-for 条件判断-while 条件判断-until Linux 服务管理1、服务管理分类 源码包可以看到源代码 可以自定义 Rpm没有源代码 自定义差 独立服务：服务直接在内存中 客户直接调用服务，服务直接相应用户，速度快，服务多了浪费内存资源 基于xinetd服务本身是独立的 本身没有功能，后面有一系列服务rsync 网络备份服务，通过xinted相应 rsync 相应最后客户端相应速度慢 本身不占用内存 0关机 1单用户 2不完全多用户 3字符界面 4未分配 5图形界面 6重启动 1查看所有启动服务的进程 ps aux 2、RPM服务管理 3、源码包服务管理 Linux系统备份与恢复1、备份和恢复概述 完全备份： 优点是数据恢复方便 缺点备份的数据量较大,备份时间较长,占用的空间较大 增量备份： 优点备份的数据较少，耗时较短，占用的空间较小； 缺点是数据恢复比较麻烦，先恢复完全备份的数据每次增量备份的数据，最终才能恢复所有的数据。 差异备份： 优点恢复数据简单方便快捷 缺点数据量庞大、备份速度缓慢、占用空间较大。 2、备份和恢复命令 Linux系统管理1、进程管理查看 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.5 0.0 2872 1416 ? Ss 15:36 0:01 /sbin/init root 2 0.0 0.0 0 0 ? S 15:36 0:00 [kthreadd] 用户 进程id 占用cpu 内存 内存daxiao 物理大 终端 状态 开始时间 占用cpu时间 名 2、进程管理终止 3、工作管理4、系统资源查看5、系统定时任务 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"python爬虫笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/65abd992/","text":"一、爬虫如何抓取网页数据网页三大特征：1、网页都有自己的唯一的URL（统一资源定位符）来进行定位。2、网页都使用HTML(超文本标记语言)来描述页面信息。3、网页都使用HTTP/HTTPS(超文本传输协议)协议来传输HTML数据。 爬虫的设计思路1、首先确定需要爬取的网页URL地址。2、通过HTTP/HTTP协议协议来获取对应的HTML页面。3、提取HTML页面里有用的数据： a.如果是需要的数据，就保存起来。 b.如果是页面里的其他URL，那就继续执行第二步。 #为什么选择Python做爬虫？PHP：是世界上最好的语言，但他天生不是干爬虫的，对多线程，异步支持不够好。爬虫是工具性程序，对速度和效率的要求比较高。Java爬虫生态圈很完善，是Python爬虫最大的对手。但是Java语言本身很笨重，代码量很大。爬虫经常需要修改部分采集代码，所以Java不合适。C/C++运行效率和性能几乎最强，但是学习成本很高。代码成型比较慢。能用C/C++做爬虫，只能说是能力的表现，但不是正确的选择。Python语法优美、代码简介、开发效率高、支持的模块多，相关HTTP请求模块还有强大的爬虫Scrapy，以及成熟高效的scrapy-redis分布式策略。而且，调用其他接口也非常方便（胶水语言） 二、如何抓取HTML页面：HTTP请求的处理，urllib、urllib2、requests 处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件 #解析服务器响应的内容 re、xpath、BeautifulSoup4（bs4）、jsonpath、pyquery等 使用某种描述性一样来给我们需要提取的数据定义一个匹配规则 符合这个规则的数据就会被匹配 #如何采集动态HTML、验证码的处理 通过动态页面采集，Selenium+PhantomJS(无界面)：模拟真实浏览器加载js、ajax等非静态的数据。 Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入/专门的打码平台。 sccrapy框架：（Scrapy，Pyspider） 搞定制性高性能（异步网络框架 twisterd），所以数据下载速度非常快，提供了数据存储、数据下载、提取规则等组件。 分布式策略：scrapy-redis，在Scrapy的基础上添加了一套以Redis数据库为核心的一套组件。让Scrapy框架支持分布式的功能。主要在Redis里做请求指纹去重、请求分配、数据临时存储。 三、爬虫–反爬虫–反反爬虫 之间的斗争：其实怕重做到最后，最头疼的不是复杂的页面，也是灰色的数据，而是网站另一边的反爬虫人员。 User-Agent、代理、验证码、动态数据加载、加密数据。 数据价值、是否值的去费劲做反爬虫。 1. 机器成本 + 人力成本 > 数据价值，就不反了，一般做到封IP就结束了。 2、面子的战争...... 爬虫和反爬虫之间的斗争，最后一定是爬虫获胜。 为什么？只要是真实用户可以浏览的网页数据，爬虫就一定能爬下来！ #根据使用场景 分为 ：通用爬虫 聚焦爬虫1通用爬虫：搜索引擎用的爬虫系统。1、目标：就是尽可能吧互联网上所有的网页下载下来，放到本地服务器里形成备份； 再对这些网页做相关处理（提取关键字、去掉广告），最后提供一个用户检索接口。2、抓取流程： a） 首选选取一部分已有的URL，把这些URL放到待爬取队列。 b） 从队列里取出这些URL，然后解析DNS得到主机IP，然后去这个IP对应的服务器里下载HTML页面，保存到搜索引擎的本地服务器。之后把这个爬过的URL放入已爬取队列。 c）分析这些网页内容，找出网页里其他的URL链接，继续执行第二步，直到爬取条件结束。3、搜索引擎如何获取一个新网站的URL: A).主动向搜索引擎提交网址, B).向其他网站里设置网站的外链。 C).搜索引擎会和DNS服务商进行合作，可以快速收录新的网站。 DNS：就是把域名解析成IP的一种技术。4、通用爬虫并不是万物皆可爬，它也需要遵守规则：Robots协议：协议会指明通用爬虫可以爬取网页的权限。Robots.txt 只是一个建议。并不是所有爬虫都遵守，一般只有大型的搜索引擎爬虫才会遵守。咱们个人写的爬虫，就不用管了。5、通用爬虫工作流程：爬取网页 - 存储数据 - 内容处理 - 提供检索/排名服务6、搜索引擎排名： ·PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站越值钱，排名越靠前。 ·竞价排名：谁给钱多，谁排名就高。7、通用爬虫的缺点： 1、只能提供和文本相关的内容（HTML、Word、PDF）等等，但是不能提供多媒体（音乐、图片、视频）和二进制文件（程序、脚本）等。 2、提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。 3、不能理解人类语义上的检索。 2聚焦爬虫：爬虫程序员写的针对某种内容爬虫。面向主题爬虫、面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求息息相关。 http的端口号：80；https的端口是：443； Python自带的模块：/usr/lib/python2.7/urllib2.py Python的第三方模块： /usr/local/lib/python2.7/site-packages urllib2 默认的 User-Agent：Python-urllib/2.7 User-Agent: 是爬虫和反爬虫斗争的第一步，养成好习惯，发送请求带User-Agent response 是服务器响应的类文件，除了支持文件操作的方法外，还支持以下常用的方法： 返回 HTTP的响应码，成功返回200，4服务器页面出错，5服务器问题 print response.getcode() 返回 返回实际数据的实际URL，防止重定向问题print response.geturl() 返回 服务器响应的HTTP报头print response.info() 四、User-Agent 历史： Mosaic 世界上第一个浏览器：美国国家计算机应用中心 Netscape 网景：Netscape（支持框架），慢慢开始流行….(第一款支持框架的浏览器) Microsoft 微软：Internet Explorer（也支持框架） 第一次浏览器大战：网景公司失败..消失 Mozilla 基金组织：Firefox 火狐 - （Gecko内核）(第一款浏览器内核) User-Agent 决定用户的浏览器，为了获取更好的HTML页面效果。 IE开了个好头，大家都开就给自己披着了个 Mozilla 的外皮 Microsoft公司：IE（Trident） Opera公司：Opera（Presto） Mozilla基金会：Firefox（Gecko） Linux组织：KHTML （like Gecko） Apple公司：Webkit（like KHTML） Google公司：Chrome（like webkit） 其他浏览器都是IE/Chrome内核 五、 Scrapy架构图（绿线是数据流向）：ScrapyEngine（引擎）：负责通讯，信号、数据传递 制作Scrapy爬虫 四步骤： 新建项目（scrapy startproject xxx）:新建一个新的爬虫项目 明确目标（编写items.py）：明确你想要爬取的目标 制作爬虫（spiders/xxspider.py）：制作爬虫开始爬取网页 存储内容（pipelines.py）：设计管道存储爬取内容 创建爬虫项目1234#创建普通项目scrapy startproject 项目名#创建模板scrapy startproject 项目名 网站名 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]}]