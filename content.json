[{"title":"jvm","date":"2020-01-12T16:00:00.000Z","path":"/posts/f0675471/","text":"JVMjava虚拟机（java virtual machine，JVM），一种能够运行java字节码的虚拟机。JVM不只是专用于java语言，只要生成的编译文件匹配JVM对加载编译文件格式要求，任何语言都可以由JVM编译运行。如kotlin、scala等。 jvm有很多，不只是Hotspot，还有JRockit、J9等。 一、JVM的基本结构JVM由三个主要的子系统构成 类加载子系统 运行时数据区（内存结构） 执行引擎 二、类加载子系统1、类的生命周期加载->连接（验证，准备，解析）->初始化->使用->卸载 1.加载将.class文件从磁盘读到内存 2.连接2.1验证验证字节码文件的准确性 2.2准备给类的静态变量分配内存，并赋予默认值 2.3解析类装载器装入类所引用的其他所有类 3.初始化为类的静态变量赋予正确的初始值，上述的准备阶段为静态变量赋予的是虚拟机默认的初始值，此处赋予的才是编程编写者为变量分配的真正的初始值，执行静态代码块。 4.使用5.卸载2、类的加载器种类启动类加载器（Bootstrap ClassLoader）负责加载JRE的核心类库，如JRE目标下的rt.jar，charsets.jar等 扩展类加载器（Extension ClassLoadEer）负责加载JRE扩展目录ext中jar类包 系统类加载器（Application ClassLoader）负责加载ClassPath路径下的类包 用户自定义加载器（User ClassLoader）负责加载用户自定义路径下的类包 2、类加载机制全盘负责委托机制当一个ClassLoader加载一个类的时候，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个ClassLoader载入 双亲委派机制指先委托父类加载器寻找目标类，在找不到的情况下载自己的路径中查找并载入目标类 双亲委派模式的优势 沙箱安全机制：比如自己写的String.class类不会被加载，这样可以防止核心库被随意篡改 避免类的重复加载：当父ClassLoader已经加载了该类的时候，就不需要子ClassLoader再加载一次 GC算法和收集器新生代：轻GC 老年代：full GC 如何判断对象可以被回收堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能被任何途径使用的对象） 引用计数法给对象添加一个引用计数器，每当有一个地方引用，计数器就加1。当引用失效，计数器就减1。任何时候计数器为0的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中没有选择这个算法来管理内存，最主要的原因是它很难解决对象之前相互循环引用的问题。 可达性分析算法！！这个算法的基本思想就是通过一系列的称为“GC Roots”的对象作为起点，这些节点开始向下搜索，节点走过的路径与当一个对象到GC Roots没有任何引用链相连的话，则证明此对象不可用的。 GC Roots根结点：类加载器、Thread、虚拟机栈的本地变量表、static成员、常量引用、本地方法栈的变量等等。 对象的引用：强引用 软引用 弱引用 虚引用 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。 如果在常量池中存在字符串“abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量“abc”就是废弃常量。 如何判断一个类是无用的类需要满足以下三个条件： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 垃圾回收算法标记-清除算法它是最基础的收集算法，这个算法分为两个阶段，“标记”和“清除”。首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它有两个不足的地方： 1.效率问题，标记和清除两个过程的效率都不高。 2.空间问题，标记清除后会产生大量不连续的碎片。 复制算法为了解决效率问题，复制算法出现了。它可以把内存分为大小相同的两块，每次只使用其中一块。当这一块的内存使用完后，就将还存活的对象复制到另一块区，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对 内存区间的一般进行回收。 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程和“标记-清除”算法一样，但是后续步骤不是直接对可回收对象进行回收，而是让所有存活的对象向一段移动然后直接清理掉边界以外的内存。 分代收集算法现在的商用虚拟机的垃圾收集器基本都采用“分代收集”算法，这种算法就是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法， 在新生代中，每次收集都有大量对象死去，所以可以选择复制算法，只要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，就必须选择“标记-清除”或者“标记-整理”算法进行垃圾收集。 三、垃圾收集器Java虚拟机规范对垃圾收集器应该如何实现没有任何规定，因为没有所谓最好的垃圾收集器出现，只能根据具体的应用场景选择合适的垃圾收集器。 Serial收集器Serial（串行）收集器是最基本、历史最悠久的垃圾收集器。是一个单线程收集器。它的“单线程”的意义不仅仅意味着它只会使用一条垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有线程（“Stop The World”），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程应用程序暂停——应用程序线程 Serial收集器相比其他垃圾收集器更加简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是不错的选择。 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等）和Serial收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程 多线程并发 应用程序暂停—— 应用程序线程 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器）配合工作。 Parallel Scavenge收集器Parallel Scavenge收集器类似于ParNew收集器。 Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成。 新生代采用复制算法，老年代采用标记-整理算法。 应用程序线程——GC线程 多线程并发 应用程序暂停—— 应用程序线程 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种是JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 Parallel Old收集器Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以用Scavenge收集器和Parallel Old收集器。 CMS收集器并行和并发概念补充： 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集器同线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种”标记-清除”算法实现的，它的运作过程分为四个步骤： 初始标记（CMS initial mark）：暂停所有的其他线程，并记录下直接与root相连的对象，速度很快。 并发标记（CMS concurrent mark）：同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记（CMS remark）：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清除（CMS concurrent sweep）：开启用户线程，同时GC线程开始对为标记的区域做清扫。 CMS主要优点：并发收集、低停顿。 缺点： 对CPU资源敏感； 无法处理浮动垃圾； 它使用的回收算法“标记-清除”算法导致收集结束时会有大量空间碎片产生。 G1收集器G1（Garbage-First）是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器。以极高概率满足GC停顿时间要求的同时，还具备高吞吐性能特征。（被视为JDK1.7中HotSpot虚拟机的一个重要进化特征） 并行和并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但还是保留了分代的概念。 空间整合：与CMS的“标记-清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器，从局部上来看是基于“” 可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。 G1收集器的运作步骤： 初始标记 并发标记 最终标记 筛选回收 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率。 怎么选择垃圾收集器？1、优先调整堆的大小让服务自己来选择 2、如果内存小于100m，使用串行收集器 3、如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 4、如果允许停顿时间超过1秒，选择并行或者JVM自己选 5、如果响应时间最重要，并且不能超过1秒，使用并发收集器 官方推荐G1，性能高 调优JVM调优主要是调整下面两个指标 停顿时间：垃圾收集器做垃圾回收中断应用执行时间。-XX:MaxGCPauseMillis 吞吐量：垃圾收集的时间和总时间的占比：1/（1+n），吞吐量为1-1/（1+n）。-XX:GCTimeRatio=99 GC调优步骤 ！！1、打印GC日志 123idea在vm options处加入-XX:+PrintGCDetails测试:代码调用system.gc后输出以下内容: 测试:代码调用system.gc后输出以下内容: Tomcat可以直接加载Java_OPTS变量里 2、分析日志得到关键性指标 3、分析GC原因，调优JVM参数 使用GCeasy分析 四、运行时数据区1、内存私有程序计数器 较小的内存空间，线程所执行的字节码的行号指示器，线程私有Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式实现的。在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。每个线程都有一个独立的程序计数器。1、如果执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址2、如果正在执行的是Native方法，这个计数器值则为空。 Java虚拟机栈 线程私有的，生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型。每个方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。通常我们讲的栈就是局部变量表部分。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char…..）、对象引用（reference类型）、returnArrdress类型（指向了一条字节码指令的地址） 本地方法栈 本地方法栈则为虚拟机使用到的Native方法服务。本地方法栈区域也会抛出StackOverflowError的OutOfMemoryError异常 Java堆 是JVM所管理的内存最大的一块。唯一的目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。java堆是垃圾收集器管理的主要区域，有时候也称为GC堆Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在堆中没有完成实例分配，并且堆也无法再扩展时。会抛出OutOfMemoryError异常 方法区 方法区是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。运行时常量池是方法区的一部分 执行引擎新生代 老年代Java堆是Java虚拟机管理的最大的一块内存空间，主要存放对象实例。 在Java中，堆被分为两块区域：新生代、老年代。 堆大小=新生代+老年代。（分别占堆空间为1/3、2/3） 新生代新生代又被分为Eden、from survivor、to survivor（8:1:1） 新生代这样划分是为了更好的管理堆内存中的对象，方便GC算法–>“复制算法”来进行垃圾回收。 JVM每次只会使用Eden和其中一块survivor来为对象服务，所以无论什么时候，都会有一块survivor空间，因此新生代实际可用空间为90%。 新生代GC（minor gc）：指发生在新生代的垃圾回收动作，因为Java对象大多数都是“朝生夕死”的特性，所以minor GC非常频繁，使用复制算法快速的回收。 新生代几乎是所有Java对象出生的地方，Java对象申请的内存和存放都是在这个地方。 当对象在Eden（包括一个survivor，假如是from），当此对象经过一次minor GC后仍然存活，并且能够被另一块survivor所容纳（这里的survivor则是to），则使用复制算法将这些仍然存活的对象复制到to survivor区域中，然后清理掉Eden和from survivor区域，并将这些存活的对象年龄+1，以后对象在survivor中每熬过一次则+1，当达到某个值（默认为15），这些对象会成为老年代！ 事情不是绝对，有些较大的对象（需要分配连续的内存空间），则直接进入老年代。 老年代老年代GC（major GC）：指发生在老年代的垃圾回收动作，所采用的的是“标记–整理”算法。 老年代几乎都是从survivor中熬过来的，不会轻易“死掉”，因此major GC不会像minor GC那样频繁 什么叫复制算法 两块survivor，每次使用其中的块。当这一块使用完了，就将还存储着的对象复制到另一块survivor上面，然后再把已经使用过的内存空间一次清理掉，下图为示意图。 优点：不用考虑内存碎片问题，实现简单，运行效率高。 缺点：当对象存活较高（PS：老年代）时，就要进行较多的复制操作，效率会很低。 复制算法示意图 什么叫标记–整理算法 与“标记-清理”算法相似，只是后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，下图为示意图。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"JAVAEE笔记04——微服务","date":"2020-01-06T16:00:00.000Z","path":"/posts/7a6c41a4/","text":"一、原始服务的调用通过网页的请求，这个网页请求到另一个网页，利用nginx做负载均衡。 消费者 12345678910@RestControllerpublic class IndexController { @Autowired RestTemplate restTemplate; @RequestMapping(\"/indexClient.do\") public String index() { ResponseEntity responseEntity = restTemplate.getForEntity(\"http://127.0.0.1/index.do\",String.class); return responseEntity.getBody(); }} RestTemplate： 是Spring用于同步client端的核心类，简化了与http服务的通信，并满足RestFul原则，程序代码可以给它提供URL，并提取结果。默认情况下，RestTemplate默认依赖jdk的HTTP连接工具。当然你也可以 通过setRequestFactory属性切换到不同的HTTP源，比如Apache HttpComponents、Netty和OkHttp。 生产者：1111 1234567891011@RestControllerpublic class IndexController { @RequestMapping(\"/index.do\") public List index() { List list= new ArrayList(); Map map = new HashMap(); map.put(\"key\",\"port:1111\"); list.add(map); return list; }} 生产者：1121 1234567891011@RestControllerpublic class IndexController { @RequestMapping(\"/index.do\") public List index() { List list= new ArrayList(); Map map = new HashMap(); map.put(\"key\",\"port:1121\"); list.add(map); return list; }} nginx负载均衡 12345678910111213141516171819upstream backser { server 127.0.0.1:1111; server 127.0.0.1:1121;} server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://backser; } } 二、 服务注册中心eureka Eureka 是 Netflix 开发的，一个基于 REST 服务的，服务注册与发现的组件 它主要包括两个组件：Eureka Server 和 Eureka Client Eureka Client：一个Java客户端，用于简化与 Eureka Server 的交互（通常就是微服务中的客户端和服务端） Eureka Server：提供服务注册和发现的能力（通常就是微服务中的注册中心） Eureka Server依赖包 1234 org.springframework.cloud spring-cloud-starter-netflix-eureka-server Eureka Client依赖包 1234 org.springframework.cloud spring-cloud-starter-netflix-eureka-client 依赖管理 1234567891011 org.springframework.cloud spring-cloud-dependencies ${spring-cloud.version} pom import Server application.yml 123456789101112server: port: 9000eureka: server: enable-self-preservation: false instance: hostname: localhost client: register-with-eureka: false fetch-registry: false service-url: default-zone: http://${eureka.instance.hostname}:${server.port}/eureka ServerApplication 1234567@SpringBootApplication@EnableEurekaServerpublic class EurakeApplication { public static void main(String[] args) { SpringApplication.run(EurakeApplication.class, args); }} client application.yml 123456789server: port: 1111eureka: client: service-url: defaultZone: http://127.0.0.1:9000/eurekaspring: application: name: shadow-order-service-1111 ClientApplication 1234567@SpringBootApplication@EnableDiscoveryClientpublic class OrderApplication1111 { public static void main(String[] args) { SpringApplication.run(OrderApplication1111.class, args); }} 三、dubbo zookeeperzookeeper服务器启动的逻辑：1、初始化配置 2、监听端口 3、初始化DataTree 4、领导者选举 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"shiro笔记","date":"2019-12-24T02:05:40.081Z","path":"/posts/7d9cd029/","text":"一、概念什么是shiroshiro是基于java的开源的安全管理框架。可以完成认证，授权，会话管理，加密，缓存等功能。 Primary ConcernsAuthentication：认证 验证用户是否合法。也就是登陆。 Authorization：授权 授予谁具有访问某些资源的权限。 Session Management：会话管理 用户登录后的用户信息通过Session Management来进行管理，不管是在上面应用中。 Cryptograpy：加密 提供了常见的一些加密算法，使得在应用中很方便的实现数据安全，并且使用很便捷。 Supporting FeaturesWeb support：web应用程序支持 ​ Shiro可以很方便的集成到web应用程序中。 Caching：缓存 ​ Shiro提供了对缓存的支持，多种缓存架构：如ehcache，还支持缓存数据库：如redis Concurrency：并发支持 ​ 支持多线程并发访问 Testing：测试 RunAs：一个允许用户假设为另一个用户身份（如果允许）的功能。 Remember Me：记住我 Shiro的架构Subject（org.apache.shiro.subject.Subject） ​ 当前与软件进行交互的实体（用户，第三方服务，cron job，等等）的安全特定“视图”。 SecurityManager(org.apache.shiro.mgt.SecurityManager)：安全管理器 ​ 如上所述，SecurityManager是Shiro架构的心脏。它基本上是一个“保护伞”对象，协调其管理的组件以确保它们能够一起顺利的工作。它还管理每个应用程序用户的shiro的视图，因此它知道如何执行每个用户的安全操作 Authenticator(org.apache.shiro.authc.Authenticator)：认证器 ​ 负责验证用户的身份 Authorizer：授权器 ​ 负责为合法的用户指定其权限。控制用户可以访问那些资源。 Realms：域 用户通过shiro来完成相关的安全工作，shiro是不会维护数据信息的。在shiro的工作过程中，数据的查询和获取工作是通过Realm从不同的数据源来获取的。Realm可以获取数据库信息，文本信息等。 二、用户认证Authentication：用户认证 验证用户是否合法，需要提交身份和凭证给shiro。 principals：用户的身份信息，是subject的标识属性。能够唯一标识Subject。如：电话号码，电子邮箱，身份证号码等。 Credentials 凭证：密码。是只被subject知道的秘密值。可以是密码，也可以是数字证书等。 Pricipals/Credentials 最常见的组合：用户名/密码。在shiro中通常使用UsernamePasswordToken来指定身份和凭证信息。 在shiro中用户的认证流程1、subject -> 2、sucurity manager ->3、Authenticator ->4、Authentication-> 5、JDBC Realm shiro.ini 123[users]zhangsan=1111lisi=1111 AuthenticationDemo 1234567891011121314151617181920212223242526public class AuthenticationDemo { public static void main(String[] args) { //1、创建SecurityManager工厂 读取相应的配置文件 Factory factory = new IniSecurityManagerFactory(\"classpath:shiro.ini\"); //2、通过SecurityManager工厂获取SecurityManager的实例 SecurityManager securityManager = factory.getInstance(); //3、将SecurityManager对象 设置到运行环境中 SecurityUtils.setSecurityManager(securityManager); //4、通过SecurityUtils获取主体Subject Subject subject = SecurityUtils.getSubject(); try { //5、加入登录的用户名zhangsan和1111， UsernamePasswordToken token = new UsernamePasswordToken(\"zhangsan\", \"1111\"); //进行用户身份验证 subject.login(token); //通过subject判断用户是否通过验证 if (subject.isAuthenticated()) { System.out.println(\"用户登录成功\"); } else { System.out.println(\"用户名或密码不正确\"); } }catch (AuthenticationException e) { e.printStackTrace(); } }} 常见的异常信息及处理在认证过程中有一个父异常为：AuthenticationException。 该异常有几个子类： DisabledAccountException 账户失效异常 ExcessiveAttemptsException 尝试次数过多 UnknowAccountException 用户不正确 ExpiredCredentialsException 凭证过期异常 IncorrrectCredentialsException 凭证不正确 执行流程 通过shiro相关api，创建securityManager及获取Subject实例 封装token信息 通过subject.login(token)进行用户认证 Subject接收token，通过其实现类DelegatingSubject将token委托给SecurityManager来完成认证。SecurityManager是接口通过DefaultSecurityManager来完成相关的功能。由DefaultSecurityManager中login来完成认证过程。在login调用了该类authenticate()来完成认证。该方法是由AuthenticatingSecurityManager来完成的。在该类的authenticate()中，通过调用authenticator(认证器)来完成认证工作。Authenticator是由其默认实现类ModularRealmAuthentucator来完成认证。通过ModularRealmAuthentucator中的doAuthenticate来获取Realms信息。如果是单realm直接将token和realm中的数据进行比较，判断是否认证成功。如果是多realm那么需要通过Authentication Strategy来判断对应的认证工作。 通过subject.isAuthenticate()来判断是否认证成功。 依赖包1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 org.apache.shiro shiro-aspectj 1.3.2 org.apache.shiro shiro-cas 1.3.2 org.apache.shiro shiro-core 1.3.2 org.apache.shiro shiro-ehcache 1.3.2 org.apache.shiro shiro-guice 1.3.2 org.apache.shiro shiro-quartz 1.3.2 org.apache.shiro shiro-spring 1.3.2 org.apache.shiro.tools shiro-tools-hasher 1.3.2 org.apache.shiro shiro-web 1.3.2 org.slf4j slf4j-log4j12 1.7.12 test org.slf4j slf4j-api 1.7.21 com.mchange c3p0 0.9.5.5 mysql mysql-connector-java 5.1.47 三、JDBCRealm及Authentication Strategy1、使用shiro框架来完成认证工作，默认情况下使用的是IniRealm。如果需要使用其他Realm，name需要进行相关的配置。 2、Ini配置文件： [main] section是配置应用程序SecurityManager实例及任何它的依赖组件（如Realms）的地方 1234[main]myRealm=cn.sxt.realm.MyRealm# 依赖注入securityManager.reakm=$myRealm [users]section允许定义一组静态的用户账户。在这大部分拥有少数用户账户或用户账户不需要在运行时被动地创建的环境下是很有用的。 123[users]Zhangsan=1111Lisi=2222,role1,role2 [roles]section允许把定义在[user]section中的角色与权限关联起来。另外，这在大部分拥有少数用户账户或账户或用户账户不需要在运行时被动态地创建的环境下是很有用的。 12345[users]Zhangsan=1111Lisi=2222,role1,role2[roles]role1=user:add,user:delete 3.使用JdbcRealm来完成身份认证 需要实现JdbcRealm 需要为JdbcRealm设置dataSource 在指定的dataSource所对应的数据库中应用表users，该表中有username，password，password_salt等字段 4.实现步骤： 新建数据库表 配置shiro.ini文件 编写认证 5.Authentication Strategy：认证策略，在shiro中有三种认证策略； AtLeastOneSuccessfulStrategy：如果一个（或更多Realm）验证成功，则整体尝试被认为是成功的。如果没有一个成功，则整体尝试失败。 第一个成功则被认为成功 所有成功被认为成功 四、MD5加密123456789101112131415public class Md5Demo { public static void main(String[] args) { //使用md5加密算法加密 Md5Hash md5 = new Md5Hash(\"1111\"); System.out.println(\"1111==\"+md5.toString()); //加盐 md5 = new Md5Hash(\"1111\", \"sxt\"); System.out.println(\"1111==\"+md5.toString()); //迭代次数 md5 = new Md5Hash(\"1111\", \"sxt\",2); System.out.println(\"1111==\"+md5.toString()); SimpleHash hash = new SimpleHash(\"md5\", \"1111\", \"sxt\", 2); System.out.println(hash.toString()); }} 12341111==b59c67bf196a4758191e42f76670ceba1111==96c0335dbdd59d920980f1c6a74ed1b01111==e41cd85110c7533e3f93b729b25235c3e41cd85110c7533e3f93b729b25235c3 六、授权1、授权：给身份认证通过的人，授予他可以访问某些资源的权限。 2、权限粒度：分为粗粒度和细粒度。 粗粒度：对user的crud操作。 细粒度：对记录的操作。如：值允许查询id为1的user的工资。 shiro一般管理的是粗粒度。如：菜单、按钮、url。一般细粒度的权限是通过业务来管理的。 3、角色：权限的集合。 4、权限表示规则：资源:操作:实例。可以用通配符表示： 如：user:add 表示对user有添加的权限。user:* 表示对user具有所有操作的权限 user:delete:100表示对user标识为100的记录有删除的权限。 5、shiro中的权限流程： image-20191230173141540 七、Shiro中的权限检查方式有3种 编程式 123if(subject.hasRole(\"管理员\")) {//操作某个资源} 注解式 1234@RequireRolepublic void list(){ //查询数据} 标签 123 更新 八、授权流程： 获取subject主体 判断主体是否通过认证 调用subject.isPermitted*/hasRole*来进行权限的判断 Subject是由其实现类DelegatingSubject来调用方法的，该类将处理交给了SecurityManager SecurityManager是由其实现类DefaultSecurityManager来进行处理，该类的isPermitted来处理，其本质父类AuthorizingSecurityManager来处理的。该类将 处理交给了authorizer（授权器） Authorizer由其实现类ModularRealmAuthorizer来处理该类可以调用对应的Realm来获取数据，在该类有PermissionResolver对权限字符串进行解析，在对应的Realm中也有对应的PermissionResolver交给WildcardPermissionResolver该类调用WildcardPermission来进行权限字符串的解析 返回处理结果 九、自定义Realm实现授权1、仅仅通过配置文件来指定权限不够灵活，并且不方便。在实际的应用中大多数情况下都是将用户信息，角色信息，权限信息保存到了数据库中。所以需要从数据库中去获取相关的数据信息。可以使用shiro提供的JdbcRealm来实现，也可以自定义realm来实现。使用jdbcRealm往往不够灵活， document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"git笔记","date":"2019-12-19T01:05:31.891Z","path":"/posts/d9283bc6/","text":"一、git init搭建仓库通过git init命令把这个目录变成Git可以管理的仓库 这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 二、git add添加所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等。 把一个文件放到Git仓库只需要两步。 第一步，用命令git add告诉Git，把文件添加到仓库： 1$ git add readme.txt 三、git commit提交第二步，用命令git commit告诉Git，把文件提交到仓库： 1$ git commit -m \"wrote a readme file\" git commit命令执行成功后会告诉你： 1 file changed：1个文件被改动（我们新添加的readme.txt文件）； 2 insertions：插入了两行内容（readme.txt有两行内容）。 添加文件到Git仓库，分两步： 使用命令git add ，注意，可反复多次使用，添加多个文件； 使用命令git commit -m ，完成。 四、git status查看状态123456789$ git statusOn branch masterChanges not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: readme.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") git status命令可以让我们时刻掌握仓库当前的状态，上面的命令输出告诉我们，readme.txt被修改过了，但还没有准备提交的修改。 五、git diff查看文件具体修改内容查看某文件修改的具体内容 123456789$ git diff readme.txt diff --git a/readme.txt b/readme.txtindex 46d49bf..9247db6 100644--- a/readme.txt+++ b/readme.txt@@ -1,2 +1,2 @@-Git is a version control system.+Git is a distributed version control system. Git is free software. 要随时掌握工作区的状态，使用git status命令。 如果git status告诉你有文件被修改过，用git diff可以查看修改内容。 六、git log查看日志，历史版本查看修改历史 123456789101112131415161718$ git logcommit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master)Author: Michael Liao Date: Fri May 18 21:06:15 2018 +0800 append GPL //修改名commit e475afc93c209a690c39c13a46716e8fa000c366Author: Michael Liao Date: Fri May 18 21:03:36 2018 +0800 add distributed //修改名commit eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0Author: Michael Liao Date: Fri May 18 20:59:18 2018 +0800 wrote a readme file //修改名 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数： 1234$ git log --pretty=oneline1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -> master) append GPLe475afc93c209a690c39c13a46716e8fa000c366 add distributedeaadf4e385e865d25c48e7ca9c8395c3f7dfaef0 wrote a readme file 1094adb7b9b3807259d8cb349e7df1d4d6477073是commit id()版本号，通过SHA1计算。 七、git reset 回退版本12$ git reset --hard HEAD^HEAD is now at e475afc add distributed git log 查看版本库的现在状态，发现最新版已经看不到了，已经回退 如果想回到原来的最新版，办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是1094adb...，于是就可以指定回到未来的某个版本： 12$ git reset --hard 1094aHEAD is now at 83b0afe append GPL Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL： 123456789┌────┐│HEAD│└────┘ │ └──> ○ append GPL │ ○ add distributed │ ○ wrote a readme file 改为指向add distributed： 123456789┌────┐│HEAD│└────┘ │ │ ○ append GPL │ │ └──> ○ add distributed │ ○ wrote a readme file 然后顺便把工作区的文件更新了。所以你让HEAD指向哪个版本号，你就把当前版本定位在哪。 八、git reflog记录每次命令可以通过这个命令查看历史操作，可以查看commit id，方便 12345$ git refloge475afc HEAD@{1}: reset: moving to HEAD^1094adb (HEAD -> master) HEAD@{2}: commit: append GPLe475afc HEAD@{3}: commit: add distributedeaadf4e HEAD@{4}: commit (initial): wrote a readme file 九、工作区与暂存区工作区（Working Directory）：当前电脑创建的git管理的仓库。 版本区（Repository）： Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 分支和HEAD的概念我们以后再讲。 前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的： 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。 简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。 修改文件，并add添加到暂存区后，状态变为 git-stage commit后，版本库变为下面图，暂存区没有任何内容。 git-stage-after-commit 十、git checkout丢弃工作区的修改1$ git checkout -- readme.txt 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 git checkout -- file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令 十一、git reset HEAD 撤销暂存区的内容，重新放回到工作区123$ git reset HEAD readme.txtUnstaged changes after reset:M readme.txt git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 再用git status查看一下，现在暂存区是干净的，工作区有修改： 12345$ git statusOn branch masterChanges not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) 小结场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD ，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，git reset xxxx，不过前提是没有推送到远程库。 十二、删除文件的解决办法。如果先添加一个文件，并提交，然后把该文件给删了。 123$ git add test.txt$ git commit -m \"add test.txt\"$ rm test.txt 这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了： 123456789$ git statusOn branch masterChanges not staged for commit: (use \"git add/rm ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) deleted: test.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") 现在你有两个选择，1、一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： 12$ git rm test.txt$ git commit -m \"remove test.txt\" 现在，文件就从版本库中被删除了。 2、另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1$ git checkout -- test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 小结命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 十三、设置SSH Key第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key： 1$ ssh-keygen -t rsa -C \"youremail@example.com\" 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面： 然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容： 点“Add Key”，你就应该看到已经添加的Key： 十四、添加远程库现在的情景是，你已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，真是一举多得。 首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库：在Repository name填入learngit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库： 目前，在GitHub上的这个learngit仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。 现在，我们根据GitHub的提示，在本地的learngit仓库下运行命令： 1$ git remote add origin git@github.com:xifujiang/learngit.git 请千万注意，把上面的michaelliao替换成你自己的GitHub账户名，否则，你在本地关联的就是我的远程库，关联没有问题，但是你以后推送是推不上去的，因为你的SSH Key公钥不在我的账户列表中。 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。 下一步，就可以把本地库的所有内容推送到远程库上： 1$ git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 从现在起，只要本地作了提交，就可以通过命令： 1$ git push origin master 把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！ 小结要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git； 关联后，使用命令git push -u origin master第一次推送master分支的所有内容； 此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改。 十五、克隆地址1git clone 地址 小结要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 分支管理Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch 切换分支：git checkout 或者git switch 创建+切换分支：git checkout -b 或者git switch -c 合并某分支到当前分支：git merge 删除分支：git branch -d 解决冲突当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 用git log --graph命令可以看到分支合并图。 推送分支推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： 1$ git push origin master 如果要推送其他分支，比如dev，就改成： 1$ git push origin dev 抓取分支多人协作的工作模式通常是这样： 首先，可以试图用git push origin 推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin 推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to origin/。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 小结 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"NoSQL02——MongoDB","date":"2019-12-17T16:00:00.000Z","path":"/posts/9d19f718/","text":"一、定义&概念1.1定义&概念mongoDB是文档型数据库，文档是一组键值（key-value）对（即BSON）。C++编写 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将_id字段设置为主键 保留的数据库名，可以直接访问这些有特殊作用的数据库。 admin： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 1.2命名规范需要注意的是： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 文档键命名规范： 键不能含有\\0 (空字符)。这个字符用来表示键的结尾。 .和$有特别的意义，只有在特定环境下才能使用。 以下划线”_”开头的键是保留的(不是严格要求的)。 合法的集合名 集合名不能是空字符串””。 集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。 集合名不能以”system.”开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Java笔记04——一些常用的函数","date":"2019-12-05T07:26:00.000Z","path":"/posts/f77bb893/","text":"StringUtils 函数 用途 isBlank(CharSequence cs) 字符串是否只有 “”,\\n\\n\\t,null isNotBlank(CharSequence cs) 字符串是否不只有 “”,\\n\\n\\t,null MultiValueMapMap之一个Key存多个Value 1234567891011121314MultiValueMap stringMultiValueMap = new LinkedMultiValueMap();stringMultiValueMap.add(\"早班 9:00-11:00\", \"周一\");stringMultiValueMap.add(\"早班 9:00-11:00\", \"周二\");stringMultiValueMap.add(\"中班 13:00-16:00\", \"周三\");stringMultiValueMap.add(\"早班 9:00-11:00\", \"周四\");stringMultiValueMap.add(\"测试1天2次 09:00 - 12:00\", \"周五\");stringMultiValueMap.add(\"测试1天2次 09:00 - 12:00\", \"周六\");stringMultiValueMap.add(\"中班 13:00-16:00\", \"周日\");//打印所有值Set keySet = stringMultiValueMap.keySet();for (String key : keySet) { List values = stringMultiValueMap.get(key); System.out.println(StringUtils.join(values.toArray(),\" \")+\":\"+key);} 123周一 周二 周四:早班 9:00-11:00周三 周日:中班 13:00-16:00周五 周六:测试1天2次 09:00 - 12:00 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Java包名","date":"2019-12-05T02:16:00.000Z","path":"/posts/c38d43d9/","text":"Java Bean、POJO、 Entity、 VO ， 其实都是java 对象，只不过用于不同场合罢了。 按照 Spring MVC 分层结构： JavaBean: 表示层 （Presentation Layer） Entity： 业务层 （Service layer） Dao： 数据访问层 （data access layer）。 Entity接近原始数据，Model接近业务对象～Entity：是专用于EF的对数据库表的操作，Model：是为页面提供数据和数据校验的，所以两者可以并存POJO：POJO是Plain OrdinaryJava Object的缩写不错，但是它通指没有使用Entity Beans的普通java对象，可以把POJO作为支持业务逻辑的协助类。domain：domain这个包国外很多项目经常用到，字面意思是域的意思。 POJO实质上可以理解为简单的实体类，顾名思义POJO类的作用是方便 程序员使用数据库中的数据表，对于广大的程序员，可以很方便的将POJO类当做对象来进行使用，当然也是可以方便的调用其get,set方法。 - JavaBean: 先说JavaBean，JavaBean更多的是一种规范，也即包含一组set和get方法的Java对象。 - POJO: 普通的Java对象，对于属性一般实现了JavaBean的标准，另外还可以包含一些简单的业务逻辑(方法)。 - PO: POJO在持久层的体现，对POJO持久化后就成了PO。PO更多的是跟数据库设计层面相关，一般PO与数据表对应，一个PO就是对应数据表的一条记录。 - DAO: PO持久化到数据库是要进行相关的数据库操作的(CRUQ)，这些对数据库操作的方法会统一放到一个Java对象中，这就是DAO。 - BO: POJO在业务层的体现，对于业务操作来说，更多的是从业务上来包装对象，如一个User的BO，可能包括name, age, sex, privilege, group等，这些属性在数据库中可能会在多张表中，因为每一张表对应一个PO，而我们的BO需要这些PO组合起来(或说重新拼装)才能成为业务上的一个完整对象。 - VO(Value Object/View Object): POJO在表现层的体现。 当我们处理完数据时，需要展现时，这时传递到表现层的POJO就成了VO。它就是为了展现数据时用的。 - DTO(Data Transfer Object): POJO在系统间传递时。当我们需要在两个系统间传递数据时，一种方式就是将POJO序列化后传递，这个传递状态的POJO就是DTO。 - EJB(Enterprise JavaBean): 我认为它是一组”功能”JavaBean的集合。上面说了JavaBean是实现了一种规范的Java对象。这里说EJB是一组JavaBean，的意思是这一组JavaBean组合起来实现了某个企业组的业务逻辑。这里的一组JavaBean不是乱组合的，它们要满足能实现某项业务功能的搭配。找个比方，对于一身穿着来说，包括一顶帽子，一件衣服，一条裤子，两只鞋,这穿着就是EJB. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:19:25.299Z","path":"/posts/0/","text":"1、线程1、线程的类别 Thread（类）、Runable、Callable、Future，后面三个都是接口。 Runable有个无返回值的run的抽象方法，使用时可通过实现类重写run方法，使用Thread封装实现类，然后运行。 Callable有一个有返回值V的call方法，并抛出Exception异常。使用时做Callable的实现类，使用FutureTask类做封装，再用Thread类对FutureTask做封装，运行 FutureTask继承RunableFuture，RunableFuture实现Runable和Callable Thread的构造方法有对Runable的参数，重载的方法还有（Runable r, String name） 2、线程池，创建方式 创建大小不固定的线程池：类实现Runable，重写run方法 1、创建大小不固定的线程池：主函数创建线程池的方式，具有缓冲功能的线程池，系统根据需要创建线程，线程会被缓冲到线程池中，如果线程池大小超过了处理任务所需要的线程，线程池就会回收空闲的线程池，当处理任务增加时，线程池可以增加线程来处理任务，线程池不会对线程的大小进行限制，线程池的大小依赖于操作系统 123456ExecutorService es=Executors.newCachedThreadPool();for(int i=0;i线程的一些方法4、同步锁可以锁什么 sync5、volatile关于jmm内存模型，可保证原子的操作吗4、Spring1、Spring的理解2、@Autwired 和 @Resource区别3、spring注入方式4、拦截器和过滤器的底层5、bean的作用域问题6、编译内核7、面向对象和面向切面8、mybatis的一对多，多对一，以及多对对的配置和使用5、数据库1、模糊查询2、索引有哪些8、前后端值传递分布式开发 注意并发计算机网络重定向状态码操作系统命令：网络、连接等命令TCP/IP客户端向服务器端发送SYN包；服务器端向客户端发送SYN+ACK；客户端回复ACK。 访问一个网站，浏览器工作的流程数据库连接过程1、加载驱动 2、建立连接 3、写sql语句 4、创建statement、preparedStatement 5、ResultSet接收 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:19:21.378Z","path":"/posts/0/","text":"1、自我介绍——一定要有亮点权限-》看需求文档-》了解文档-》分配文档-》了解模块-》编码开发-》记录每天工作量 面试官好，我是沈旖旎，来自吉首大学软件工程专业。 大学期间我对Java知识很感兴趣，Java基础知识较扎实。 现在想应聘的工作是Java开发工程师。 大学期间，学院有开设软件工程、计算机网络、设计模式等科目。我擅长数据开发和web开发，曾自学Java和大数据。 我在校期间做过几个项目，曾获得过最高分，其中基于Maven小说阅读网项目是我做的全栈项目，曾给全院同学做过演示，我对项目流程有一定的了解。 我在组织管理方面也有一定的能力，担任过实验室的管理员和社团社长。 我在校期间成绩优秀，有过ACM大赛的经历，多次获得奖学金，成绩排5%内。 我平常喜欢旅游、看动漫、逛一些技术网站等。 自我介绍大概就这样了。 我从大一的时候学习C语言，课设期间编写过一个双人泡泡龙对战游戏，获得最高分。大一的时候加入实验室，是实验室的管理员。大二的时候我自学Java，做过一个从需求分析、数据库开发、前端、后端独立开发的全栈web项目，小说文学网站，大三时期自学大数据，和实验室的学长们合作完成一个B2C的电商项目。我平常喜欢旅游、看动漫、逛一些技术网站等等。 自我介绍大概就这样了。 2、项目介绍简单介绍尚客零食采购电商平台这个项目是实验室老师接的外包项目，我负责的模块是数据分析和报表这块。我所拿到的初始数据比较杂乱无章，我先通过sqoop把数据导入到HDFS，通过 Map处理掉无用的脏数据，提取有效的列信息，按照tab切割，其中也利用了shuffle机制，再利用reduce到hdfs中，hdfs转存到hive，用hive做数据分析。 具体：例如针对10天内销量前10的城市排序，用已经处理的数据，写HQL语句， select cname, count(cid) as num from city group by cid order by num desc limit 1 10 用group by 对城市进行分组，取某个阶段的时间，再用order by降序，llimit 10； 查看每个城市每天完成订单数，取消订单数，下单订单数，下单用户数 select cname, sum(if(order_status=5, 1, 0)) as success, sum(if(order_status=3, 1, 0)) as cannel , sum(if(order_status = 1, 1, 0)) as ord,sum, count(distinct CUST_ID) as cnt_ord_user //用户ID去重 from order 小说阅读网目的：因为我个人喜欢阅读小说，我身边也有很多文学爱好者，我就开始筹备这个项目，当时我跟朋友讨论做自己喜欢的项目，我们讨论做这个阅读网站。 我们这个小说阅读网，有作者端和阅读的用户端，作者端的功能的话是上传作品，用户端主要负责阅读搜索、投币、评论等功能。 这个项目呢，是从需求分析、数据库设计、前端设计、后端设计完全是我独立开发的，算是一个全栈项目。 现在的话还挂在我的服务器上，之前经过一些途经的宣传收获一波关注量。现在我把它给关了，觉得系统还有很多需要完善的地方。 遇到的困难：我个人在ajax访问，和书的封面上传方面遇到了一些问题，书的图片上传方面的话，我通过一些字节流的处理解决了这个问题。 结果：我这个项目受到了一些好评，也给全校的学生展示过。 用的bootstrap搭建，使用jsp，ajax的技术，后台使用ssm技术，用了javamail、solr、友盟等一些插件，做了关于面向作者和阅读者的小说阅读网。该网站上传到我的云服务器，通过一些途径，像论坛、表白墙、群等的宣传，收获了不少注册量，平台的点击数也在友盟后台可以看到。该项目也是课设的代表项目，有幸在答辩期间给全院的同学作展示，当时获得展示机会的只有两个女生。 局域网聊天系统这是我一个基于Java SE的java项目，主要是利用socket套接字等技术。是用的C/S架构，模拟QQ做的局域网项目。 本地的进程间通信（IPC）有很多种方式，但可以总结为下面4类： 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC） 火拼泡泡龙游戏介绍这是款基于C语言的火拼泡泡龙游戏，游戏开始随机生成初始泡泡，玩家通过击落泡泡获取积分，打败对手，或者在对手打击你的时候消除自己的泡泡。该游戏利用了多线程，事件监听，递归等算法，使用链表、图的深搜等数据结构相关知识 3、大数据HDFS上传简言之：客户端上传文件，向NameNode发送请求，NameNode返回可执行，返回DN的位置后，客户端的OutputFormat再向DataNode建立连接和传输，传输完毕后客户端还要向NameNode汇报。 MapReduceMap端获取数据后对数据进行逻辑处理，两次排序，溢写到文件中，可以对数据分区操作。Shuffle端在Map、Reduce之间充当洗牌、发牌的角色，核心是分区、排序缓存，是对局部value排序，提高处理性能。reduce端接收数据，通过归并，把处理的数据写到目标文件夹下。 shuffle：对数据进行分区、排序、缓存 mapjoin： 1）把小表加载到缓存中 2） Yarn的job提交流程简言之：Job向RM请求Application，RM同意后创建任务到调度队列里，NodeManger领取到任务后创建容器，用来运行mrAppMaster，AppMaster再请求任务，用来运行MapTask任务和ReduceTask任务，运行mr。yarn就是封装了mapreduce的任务，使得hadoop解耦 。 Zookeeperzookeeper是一个分布式的应用程序协调服务，像hadoop HA、kafka、HBase都可以向zookeeper注册服务。 选举机制：半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以Zookeeper适合装在奇数台机器上。 Hive1、Hive 和数据库除了拥有类似的查询语言，再无类似之处。 2、Hive的元数据存储在数据库中，hive封装了执行器、sql解析器、编译器、优化器等，他的底层实现是mapreduce，所以处理起来延时较高，适合离线计算，hive数据存储在HDFS中，适合一次写入，多次读取。hive没有建立索引，是对全部数据暴力扫描，访问延迟较高。hive的可扩展性强，支持很大的数据规模。 3、内部表和外部表 默认创建的表都是管理表（内部表），hive会控制数据的生命周期，默认的存储位置为（/user/hive/warehouse）的目录下。删除一个管理表时，Hive也会删除这个表中的数据。所以管理表不适合和其他工具共享数据。 外部表，Hive并非人为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉 4、排序区别 a) Sort By：每个Reducer内部有序； b) Order By：全局排序，只有一个Reducer； c) Cluster By：当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。 d) Distrbute By：类似MR中partition，进行分区，结合sort by使用，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。 3、窗口函数 a) OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化 b) CURRENT ROW：当前行 c) n PRECEDING：往前n行数据 d) n FOLLOWING：往后n行数据 e) UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点 f) LAG(col,n)：往前第n行数据 g) LEAD(col,n)：往后第n行数据 h) NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。 4、UDF函数 FlumeAgent是JVM进程，以事件的形式将数据从源头传送到目的地 flume组成：source、channel、sink 传输单元：event、event有header+body，header为可选消息头 拦截器：自定义拦截器把日志分类，为每个Event加上header，header就是日志的类型，当日志传输到kafka时候，通过header便知道属于哪种类型的。我在使用的时候发现我的flume版本太低，我用的是0.9版本的，没有拦截器，后来重新装了个0.11版的，好像0.9.6版本是分水岭。 自定义拦截器的操作： 给项目pom添加flume依赖 寻找Flume拦截器类TimestampInterceptor（系统时间拦截器），把这个类的代码copy到自定义类，按要求修改，打包。 在flume的配置文件中指定拦截器的类型 监控器ganglia： 监控source写入channel的总数量与成功写入且提交的数量，看是否有丢失数据，sink尝试拉取的数量与成功读取的数量，还有channel启动时间，停止时间，等等。 双层flume 负载均衡、容灾 第一层用于数据采集 第二层用于数据聚合 kafkakafka数据丢失分为两个部分，分别是生产者和kafka应用交互时丢失，消费者丢失 1.生产者和kafka应用交互 kafka存储一个topic中的数据至kafka时，会存储至不同的partition，每个partition又会有多个副本，当某一个broker宕机时，kafka会去重新选举partition的leader，如果此时follower中的数据没有和leader中保持一致的话，就会导致数据丢失。 解决办法 kafka至少应该设置以下几个参数1.replication.factor:副本的数量，至少应该大于1，也就是说每个partition必须存储两个副本；2.min.insync.replications参数：这个值必须大于1，这个是要求每个leader至少感知到有一个follower还和自己保持同步；3.在producer中设置ack = all，确保每个数据必须是写入了所有副本之后，才算是写成功了；4.producer设置retries=MAX,失败之后，需要重新发送MAX次，这个值一般设置成一个很大的值，相当于无限重试。 2.消费者 消费者需要提交offset，在消费者消费到这个消息但是尚未处理的时候，自动去提交这个offset，但是此时程序发生异常，kafka不会再次发送数据，消费者又没有真正消费到这个数据。 解决办法 将自动提交offset关闭（auto.commit.enable=true 改为false） ISR(In-Sync Replicas)，副本同步队列，包括leader和follower，所有的副本统称为AR ISR是AR中的一个子集，leader维护ISR列表，follower从leader同步数据有一些延迟。 OSR（Outof-Sync Replicas）列表，新加入的follower会先存放在OSR中。 AR = ISR + OSR kafka分区分配策略 rebalance：某个消费者的所有权转移到其他消费者 需要满足下面的条件才会产生分区分配 同一个 Consumer Group 内新增消费者 消费者离开当前所属的Consumer Group，包括shuts down 或crashs 订阅的主题新增分区 range Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 但如果有多个话题，前面的消费者可能会比后面的多消耗几个分区，这就是Range strategy的一个很明显的弊端 roundRobin 使用RoundRobin策略有两个前提条件必须满足： 同一个Consumer Group里面的所有消费者的num.streams必须相等； 每个消费者订阅的主题必须相同。 RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，最后按照round-robin风格将分区分别分配给不同的消费者线程。 HBaseHBase是一个开源的、分布式的、版本化、列式存储的非关系型数据库。 优点： HDFS支持的海量存储，链家PC存储PB级数据仍能有百毫秒内的响应速度。（扩展性十分好） 极易扩展（可添加很多RS节点进行处理能力扩展，也可添加多个HDFS DataNode进行存储能力扩展），表自动分片，且支持自动Failover 高效地、强一致性地读写海量数据，CP MapReduce可读写HBase JavaAPI, Thrift / REST API, Shell 依赖Blockcache和布隆过滤器提供实时查询 服务端侧的过滤器实现谓词下推，加速查询 可通过JMX监控HBase各指标 面向列，列式存储，且列可以按需动态增加 稀疏。空Cell不占空间 数据多版本 数据类型单一，都是字符串，无类型（要用类型可用Phoenix实现） 作为曾经Hadoop项目的子项目，HBase还是与Hadoop生态关系密切。HBase底层存储用了HDFS，并可直接用MapReduce操作HBase 适用场景 持久化存储大量数据（TB、PB） 对扩展伸缩性有要求 需要良好的随机读写性能 简单的业务KV查询(不支持复杂的查询比如表关联等) 能够同时处理结构化和非结构化的数据 订单流水、交易记录、需要记录历史版本的数据等 不适用场景 数据量少、跨行事务（目前HBase只支持单行事务），SQL查询或复杂的业务结构，硬件太少（HBase依赖服务很多，如hdfs、zookeeper集群等）、需要表间Join 行列存储最大的不同就是表的组织方式不同，硬盘存储方式不同，可以看成一个是一行为单位的数组存储，一个是一列为单位的数组存储。 行列存储对比 行 列 优点 1.便于按行查询数据，OLTP往往是此场景 2.便于行级插入、删除、修改 3.易保证行级一致性 1.便于按列使用数据，如对列分组、排序、聚合等，OLAP很多是这样 2.列数据同类型，便于压缩 3.表设计灵活，易扩展列 缺点 1.当只需查询某几个列时，还是会读整行数据 2.扩展列代价往往较高 1.不便于按行使用数据 2.很难保证行级一致性 优化思想 读取过程尽量减少不需要的数据 提高读写效率 优化措施 1.设计表时尽量减少冗余列 2.内存中累积写入到阈值再批量写入 1.多线程方式并行读取不同列文件 2.行级一致性，可通过加入RDBMS中回滚机制、校验码等 3.内存中累积写入到阈值再批量写入 应用场景 OLTP（联机事务处理） OLAP（联机分析处理） Client Client有访问Hbase的接口，会去meta表查询目标region所在位置（此信息会放入缓存），并连接对应RegionServer进行数据读写。 当master rebalance region时，Client会重新进行查找。 Zookeeper HMaster和RegionServer都注册到ZK上，使HMaster可感知RegionServer上下线。 选举，保证HMaster HA。 保存.META.表所在RegionServer位置 HMaster 监控RegionServer状态，并为之分配Region，以维护整个集群的负载均衡 通过HMasterInterface接口维护集群的元数据信息，管理用户对table的增删改查操作 Region Failover：发现失效的Region，就到正常的RegionServer上恢复该Region RegionSever Failover：由HMaster对其上的region进行迁移 HRegionServer 处理用户读写请求，并和底层HDFS的交互。我们说RegionServer拥有某个region意思是这个region读写、flush之类的操作都是由当前regionserver管理的。如果该RegionServer本地没有HDFS DataNode 底层数据就要从其他DataNode节点远程读写。 负责Region变大以后的split 负责Storefile的合并工作 rowkey设计原则 1)长度原则： 最大值64kb，推荐长度为10-100byte 最好8的倍数，能端这段，rowkey太长会影响性能 2)唯一原则：rowkey应该具备唯一性 3)散列原则 在rowkey前增加随机数 3-2) 字符串反转：1312312334342， 1312312334345 电话号码： 133 + 0123 + 4567 3-3) 计算分区号：hashMap rowkey = regionNum + call1 + time + call2 + duration scala 正在学习中元组（元组元素的访问有下划线，并且访问下标从1开始，而不是0） 12345678方式1： for (elem println(i)) tuple1.produIterator.foreach(print(_)) 隐函数 隐式转换函数是以implicit关键字声明的带有单个参数的函数。这种函数将会自动应用，将值从一种类型转换为另一种类型。 1234implicit def a(d: Double) = d.toInt //不加上边这句你试试 val i1: Int = 3.5 println(i1) 柯里化 spark 正在学习中3、近期需要了解的1、dubbo2、红黑树3、HashMap我有简单了解HashMap，他是通过拉链法的hash，避免同义词冲突，允许存储空key和value。 4、大数据项目5、solrSolr是一个基于Luence的全文搜索的企业级应用服务器，搜索引擎的核心是索引库。我当时做这个项目的时候，导入了中文分词器，通过同步数据库的相关内容到solr索引库，用户通过搜索关键词检索书本或者作者，读取相关小说信息。 具体谈谈solr：他是对每个字进行检索，排序位置，对词建立语法数。实现快速检索 6、docker镜像7、redis1、主从复制2、哨兵（监控、提醒、故障迁移）3、集群（代理服务器 Twemproxy twitter），失败节点自动删除Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性：监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。特点：1、保证高可用。2、监控各个节点。3、自动故障迁移。缺点：主从模式，切换需要时间丢数据。没有解决 master 写的压力。 8、非关系型数据库CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 9、Linux分区linux分区boot 200mbtmp swap 内存2倍home 4096/ 其他 10、非关系型数据库CAP原则CAP定理指出，分布式系统可以保持以下三个特征中的两个： Consistency，一致性请求所有节点都看到相同的数据 Availability，可用性每个请求都能及时收到响应，无论成功还是失败。 Partition tolerance，分区容忍即使其他组件无法使用，系统也会继续运行。 11、OLTP 与 OLAP1、适用人员不同：OLTP主要供基层人员使用，进行一线业务操作。OLAP则是探索并挖掘数据价值，作为企业高层进行决策的参考。 2、面向内容不同：OLTP面向应用，OLAP面向主题； 4、数据特点不同：OLTP的数据特点是当前的、最新的、细节的, 二维的、分立的；而OLTP则是历史的, 聚集的, 多维的，集成的, 统一的； 5、存取能力不同：OLTP可以读/写数十条记录，而OLAP则可以读上百万条记录； 6、工作事件的复杂度不同：OLTP执行的是简单的事务，而OLAP执行的是复杂任务； 7、可承载用户数量不同：OLTP的可承载用户数量为上千个，而OLAP则是上百万个； 8、DB大小不同：OLTP的DB 大小为100GB，而OLAP则可以达到100TB； 9、执行时间要求不同：OLTP具有实时性，OLAP对时间的要求不严格。 12、布隆过滤器假设你现在要处理这样一个问题，你有一个网站并且拥有很多访客，每当有用户访问时，你想知道这个ip是不是第一次访问你的网站。这是一个很常见的场景，为了完成这个功能，你很容易就会想到下面这个解决方案： BitSet32位无符号int型能表示的最大值是4294967295，所有的ip都在这个范围内，我们可以用一个bit位来表示某个ip是否出现过，如果出现过，就把代表该ip的bit位置为1，那么我们最多需要429496729个bit就可以表示所有的ip了。举个例子比如10.0.0.1转换成int是167772161，那么把长度为4294967295的bit数组的第167772161个位置置为1即可，当有ip访问时，只需要检查该标志位是否为1就行了。4294967295bit = 536870912Byte = 512M。如果用hash表示所有4294967295范围内的数组的话，需要十几G的空间。 10、扯到其他的时候电脑 512的固态 分了40g给linux系统，用过deepin、elementaryOS、ubuntu 4、项目：通话记录数据离线分析项目 DATA尚硅谷大数据技术之电信客服综合案例2.资料流程图电信项目技术框架.png 1、生成模拟数据calllog 通过一张联系人表，模拟建立calllog，其中字段有 主叫用户、被叫用户、通话时间（yyyyMMddHHmmss）、通话时长(s)。模拟起始时间到终止时间。 2、flume数据采集 分析：flume source端使用exec接收数据，channel用memory channel，sink端使用kafka接收，充当kafka的生产者。 创建flume-ct-kafka.conf，填写配置文件。 启动命令：bin/flume-ng agent –conf conf/–name a1 –conf-file data/flume-2-kafka.conf 3、kafka消费 1、编写kafka消费的类，实现Consumer接口；2、创建kafkaConsumer对象，订阅主题；3、获取消费主题（0.1秒1次）4、控制台输出 主题为tc，消费组为xifu。 4、存入HBase中 kafka获取的内容存入到Hbase。 创建BaseDao抽象类，连接HBase 创建命名空间、表 rowkey设计（采用异或校验算法） 分区号_号码1_通话时间_号码2_时长_(0/1) 创建put对象，添加列，上传数据（列族，列族限定符，值） 使用协处理器 5、mapreduce计算，处理数据 scan 扫描列族，得到数据 map操作，按照key为(某个用户、年/月/日)，value为 通话时长 reduce操作，将(某个用户、年/月/日)的通话数累加，通话时长累加，key不变，valye为（通话数_通话时长） 6、保存到mysql中（redis缓冲） mapreduce输出到mysql中。 首先从mysql中提取两张表信息，分别是（时间表，有2018年【12+365+1 = 378】条数据。联系人表），存到redis中 根据mapreduce输出的结果，将（用户、日期、通话数、通话总时间）保存到mysql表中，记为ct_call 7、界面显示，使用echarts。 使用ssm框架，读取 mysql值 做前端查询，echarts分析展示结果 对公司的看法或建议1、我觉得从用户留存方面看的话，目前租租车这边像骑士卡的话现在好像还处于推广阶段，为了更好的实现用户留存，想借鉴像支付宝的蚂蚁森林，美团的果园等这些公益活动，让人们参与到公益事业，坚持每天给app打卡，通过打卡浇水啊这些形式，用户自己可以得到一些福利，通过浇水打卡，用户会更关注我们的app产品，每天使用我们app的时间增加，从而实现留存。 2、因为租租车是个租车平台，租车出行的话，停车也是必不可少的，我认为我们可以和一些停车厂合作，做一些有关停车导航的业务，我个人觉得可以。 我的想法还不太成熟，我想听一下你对我所说的有什么意见吗？ 请问还有什么想问我们公司的吗其实这是我第一个技术面试，我知道我的表现可能不太优秀，我想请问一下，我可以回去准备两个月，再投你们家公司吗？ 众所周知，大数据现在是一个很流行的行业。我的话其一是对数据比较敏感，其二，我认为用大数据开发与分析来解决一些业务问题，是一个从根本上解决问题的方法，我觉得非常有必要，非常的牛 4、JavaIOC的概念IoC(Inversion of Control)，意为控制反转，不是什么技术，而是一种设计思想。Ioc意味着将你设计好的对象交给容器控制，而不是传统的在对象内部直接控制。 传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对象的创建； IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等） 因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 线程Thread在多任务处理应用程序中起着至关重要的作用。之前所接触的应用程序使采用单线程处理模式。单线程在某些功能方面会受到限制，无法同时处理多个互不干扰的任务。 序列化 是把对象转换为一个可以存储和传输的东西，这样就可以被二进制流传输。 transient 数据库原子性 一致性 隔离性 持久性 ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证 一.HTTP通信机制是在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 1.域名解析 2.发起TCP的3次握手 3.Web浏览器向Web服务器发送http请求命令 一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令。例如：GET/sample/hello.jsp HTTP/1.1。 4.Web浏览器发送http请求头信息 浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 Web服务器应答 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK ，应答的第一部分是协议的版本号和应答状态码。 Web服务器发送应答头信息 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 Web服务器向浏览器发送数据 Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据。 Web服务器关闭TCP连接 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码： Connection:keep-alive TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 三.当我们在浏览器的地址栏输入 www.linux178.com ，然后回车，回车这一瞬间到看到页面到底发生了什么呢？ 以下过程仅是个人理解： 域名解析 –> 发起TCP的3次握手 –> 建立TCP连接后发起http请求 –> 服务器响应http请求，浏览器得到html代码 –> 浏览器解析html代码，并请求html代码中的资源（如js、css、图片等） –> 浏览器对页面进行渲染呈现给用户 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:19:18.066Z","path":"/posts/0/","text":"技术面试1.你知道现在Hadoop最新版本发行在几点几了，你现在用的是几点几的版本？1、hadoop现在发行到了3.几的版本，在2.0的基础上，hdfs增加了纠错码、对云计算平台的支持（虚拟化存储），增加了common机制（JDK1.8+、更强的兼容性；单个DataNode上，不同硬盘间的数据balancer；部分服务默认端口修改、重新实现的Shell、Classpath隔离）。2、hadoop二点版本以上在一点多版本上增加了yarn机制。3、我现在使用的是2.6.0的版本，还比较稳定。 2.那我就你笔试中的答题情况来问你一些问题，我们公司出的这些题你都会吗？【讨论题】我个人觉得你们公司出的题比较（容易/复杂）,不过我在来之前已经做好了准备，所以这套题对我来说难度不大。我对其中几道题印象比较深刻，像hql…语句xxxxx…. 3.NameNode节点启动的瓶颈是什么？内存。 在整个启动过程中，包括:内存中构建namespace、保存block->datanode list信息以及为保存副本的三元组结构，都会耗费相当多的内存。 fsimage阶段主要耗时的地方是在java原生接口的调用中，如字节流读取，类型转换等。 blockReport阶段耗时主要是跟当前的namenode设计以及内存结构有关。 原因： 1、先简述NameNode是如何启动的。NameNode的启动过程是先加载fsimage文件，读取头信息，存储多少个文件和目录，再加载fsimage里所存储的信息，像hdfs的文件和目录的元数据，如果最后读取的数量小于读取的数据，重新加载。（详细：NameNode依次读取fsimage的文件信息，如果是目录，则创建INodeDirectory，读取元数据并保存，如果是文件，则创建文件，并读取blocks信息，保存到INodeFile中，把所有blocks添加到BlocksMap中，添加好后，NameNode进程将进入rpc等待状态，等待所有的DataNode发送blockReports。当所有的DataNode汇报完block，NameNode针对每个DataNode的汇报进行过处理后，NameNode的启动过程到此结束。此时BlocksMap中block->DataNode的对应关系已经初始化完毕。如果此时已经达到安全模式的推出阈值，则hdfs主动退出安全模式，开始提供服务。） 1、第一点，如果NameNode第一次格式化后，先要保证hdfs的配置文件的准确性。 2、如果已经上传过多次文件，要保证磁盘的空间充足。 3、fsimage和editlog的文件完整性。因为fsimage存储的是hdfs文件和目录的元数据，如果损失对NameNode启动造成影响。 4、MapReduce和Hive数据倾斜5、MapJoinmapjoin的应用场景是大表join小表，将小表加载到内存中，可以省去reduce的时候shuffle大量数据传输。大表join小表也是会导致数据倾斜的，所以这边可以通过MapJoin进行解决。 6、Flume 的 Source，Sink，Channel 的作用？你们 Source 是什么类型？ 1、作用（1）Source 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括 avro(序列化格式)、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy（2）Channel 组件对采集到的数据进行缓存，可以存放在 Memory 或 File 中。（3）Sink 组件是用于把数据发送到目的地的组件，目的地包括 HDFS、Logger、avro、thrift、ipc、file、Hbase、solr、自定义 7、Flume采集数据会丢失吗？不会，Channel 存储可以存储在 File 中，数据传输自身有事务，像source到channel，channel到sink中都有事务性，遇到错误就回滚。同时的话，我们也可以采取用ganglia来监控flume。 8、Flume 的 Channel SelectorsChannel Selectors，可以让不同的项目日志通过不同的Channel到不同的Sink中去。 官方文档上Channel Selectors 有两种类型:Replicating Channel Selector (default)和 Multiplexing Channel Selector这两种Selector的区别是:Replicating 会将source过来的events发往所有channel,而 Multiplexing可以选择该发往哪些Channel。 HDFS外部表与内部表的区别对于一些原始日志文件，同时被多个部门同时操作的时候就需要使用外部表，如果不小心将meta data删除了，HDFS上 的data还在可以恢复，增加了数据的安全性。 HashMaphashMap 是一个散列表，他存储的内容是key-value键值对映射。 hashMap继承于abstractMap，实现了Map、cloneable、java.io.serializebale(启用其序列化功能的接口)。 hashMap的实现不是同步的，也就是意味着hashMap不是线程安全的。他的key-value都可以为null，此外他的映射不是有序的。 hashMap的实例有两个参数影响其性能：“初始容量”和”加载因子”。初始容量只是在哈希表在创建时桶的数量。加载因子是哈希表在其容量增加之前可以达到多满的一种尺度。当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行rehash操作（即重构内部数据结构），从而哈希表将具有两倍的桶数。 通常，默认加载因子是0.75。如果初始容量大于最大条目数除以加载因子，则不会发生rehash操作。 hashMap有四个构造方法。（默认构造方法、指定容量和加载因子的构造方法、指定容量的构造方法、包含子Map的构造方法） hashMap是通过拉链法实现的哈希表。成员变量有： table是一个Entry[]数组类型，而Entry实际是一个单向链表。哈希表的key-value都是储存在Entry数组中。 size 保存键值对的数量。 threshold 是hashMap的阈值，用于判断是否需要rehash，rehash操作使容量加倍。 loadFactor 加载因子。 modCount 用来实现fail-fast机制。 hash有两种法，一种是开放地址法，一种是拉链法。开放地址法，非同义词可能会引起冲突，而拉链法就是解决这种冲突，拉链法也适合增加和删除。 9、自定义UDF函数1．创建一个Maven工程Hive 2．导入依赖 1234567 org.apache.hive hive-exec 1.2.1 3．创建一个类 12345678910package com.atguigu.hive;import org.apache.hadoop.hive.ql.exec.UDF;public class Lower extends UDF { public String evaluate (final String s) { if (s == null) { return null; } return s.toLowerCase(); }} 4．打成jar包上传到服务器/opt/module/jars/udf.jar 5．将jar包添加到hive的classpath hive (default)> add jar /opt/module/datas/udf.jar; 6．创建临时函数与开发好的java class关联 hive (default)> create temporary function mylower as “com.atguigu.hive.Lower”; 7．即可在hql中使用自定义的函数strip hive (default)> select ename, mylower(ename) lowername from emp; 数据结构1、B树和B+树为什么会出现B树、B+树？ 二叉树速度很快，但他每层花在磁盘IO的时间太久了。而B树、B+树的话改变原来高瘦的树的情况，变得矮胖，牺牲比较次数来减少磁盘IO的时间。 B树 又称多路平衡查找树，B树中所有结点的孩子结点数的最大值称为B树的阶，通常用m表示。一课m阶B树或为空树，或为满足如下特性的m叉树： 1.根结点至少有两个子女。2.每个中间节点都包含k-1个元素和k个孩子，其中 ceil（m/2） ≤ k ≤ m3.每一个叶子节点都包含k-1个元素，其中 ceil（m/2） ≤ k ≤ m4.所有的叶子结点都位于同一层。5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域划分6.每个结点的结构为：（n，A0，K1，A1，K2，A2，… ，Kn，An） 其中，Ki(1≤i≤n)为关键字，且Ki { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:18:58.195Z","path":"/posts/0/","text":"技术面（1）java相关的课程的内容：JVM、IO流、线程、类加载机制、继承、多态 （2）网络编程 （3）线程 线程的创建方式、线程池的创建方式 （4）锁相关，synchronized怎么使用 （5）通过什么方式学习：看书、博客、视频。 （6）集合框架，arraylist和linkedlist、vector的区别，treeSet和hashSet的区别等等。 在这里插入图片描述 （7）实习项目。 （8）SQL注入、手写SQL语句 （9）数据库三大范式的概念 （10）数据库连接的方式，内连接和外连接的区别 （11）Netty （12）介绍一下spring中的AOP （13）mybatis和hibernate的区别与优劣 （14）redis有哪些数据类型 （15）手写一个单例模式 hr面（1）自我介绍 （2）说一下竞聘这个这位的优势 （3）对海康威视的了解 （4）有什么想问的 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-12-02T02:18:45.229Z","path":"/posts/0/","text":"# 东方通信面试 1、做过的项目，负责的模块，所用到的技术 小说阅读网，这是我做的一个全栈项目，与市面上的小说阅读网站类似，主要做同人小说网。 负责需求分析，数据库设计，前后台所有代码的编写，网站的发布等。 使用的是springmvc、spring、mybatis技术、solr搜索引擎、邮箱发送等技术。 电信项目，这是一个离线的大数据项目，主要根据用户的通话，做日话单、月话单、季度话单、年度话单、通话详情、通话记录、亲密度分析等等。使用的技术有数据生成、flume、kafka、hbase、mapReduce、redis、mysql、。 毕设项目，竞拍网站的设计与实现，使用的是vue+springboot前后端分离技术，后端使用了mybatis、jpa hibernate、SSL短信接口、redis等 2、对JVM的理解； 我个人对JVM的理解的话就是JVM类似于一个操作系统，跟大数据的yarn也很类似，像一个容器一样，封装、运行了java代码。他主要由三个子系统构成，类加载机制、运行时数据区、执行引擎。 类加载classLoader的生命周期为：加载（就是把.class文件从硬盘加载到内存中），连接（做一些验证、准备、解析工作）、初始化（为静态变量赋予初始值）、使用、卸载。运行时数据区（内存结构）有内存公有（堆区、方法区）、内存私有（本地方法栈、Java虚拟机栈、程序计数器），对于不同的数据存储不同的区域中。 还有的概念就是垃圾回收机制，GC算法和调优什么的，算法有标记-清除算法、复制算法。调优主要调停顿时间、吞吐等方面。 3、对垃圾回收机制的理解； 垃圾回收机制是程序在进行后，由于到了一定时间或者内存不足，系统自动调用的机制，也可以自己调用垃圾回收机制，一般是针对堆内存空间进行回收。 堆内存空间里有新生代（1/3）和老年代（2/3）。 新生代被分为Eden、from survivor、to survivor（8:1:1） 新生代这样划分是为了更好的管理堆内存中的对象，方便GC算法–>”复制算法“进行垃圾回收。 新生代GC（minor GC）：发生在新生代的垃圾回收动作，新生代GC非常频繁，使用复制算法快速回收。 老年代GC（major GC），采用的是”标记–整理“算法。老年代不会轻易的死掉。 复制算法：把survivor内存分为两块，对一块内存用完后，把还存在的内存移到另一块内存中。 标记–整理算法：与“标记-清理”算法相似，只是后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 GC调优，主要是针对停顿时间和吞吐量，可以用GCeasy可视化调优。 4、IO流用过哪些接口，介绍NIO IO流的三种分类方式 1、按方向划分：输入流和输出流 2、按数据的单位划分：字节流和字符流 3、按流的功能划分：字节流（从指定的地方读写的流）、过滤流（使用字节流输入或输出，过滤流是由已存在的输入流或者输出流连接创建的）。 IO和NIO的区别 原有的 IO 是面向流的、阻塞的，NIO 则是面向块的、非阻塞的。 5、谈谈异常 异常是Java程序中不可避免的一些错误。Java所有异常的根类是Throwable，throwable派生了两个子类：Error和Exception，Error是程序自身无法克服和恢复的问题，像比如说一些内存溢出，线程死锁等等。Exception主要是开发人员考虑不周的问题，软件可以在一定的程度上抛出异常或抓住异常，像比如说数组越界，空指针，类转换异常等等。 ​ 6、ArrayList和LinkedList的区别 ​ 7、HashMap的底层实现 ​ 8、怎么设置比较两个对象或者数值 ​ 9、排序怎么实现。 说说mybatis的优缺点 是否用过linux，是否知道linux的常用命令 如果sql查询速度过慢，应该怎样去优化？ 说说html常用标签。 说说js的常见用法。 java的反射机制 1.2 JPA与Hibernate 的区别 JPA和Hibernate之间的关系，可以简单的理解为JPA是标准接口,Hibernate是实现。 那么Hibernate是如何实现与JPA的这种关系的呢。Hibernate主要是通过三个组件来实现的，及hibernate-annotation、hibernate-entitymanager和hibernate-core。 hibernate-annotation：是Hibernate支持annotation方式配置的基础，它包括了标准的JPA annotation以及Hibernate自身特殊功能的annotation。 hibernate-core：是Hibernate的核心实现，提供了Hibernate所有的核心功能。 hibernate-entitymanager：实现了标准的JPA，可以把它看成hibernate-core和JPA之间的适配器，它并不直接提供ORM的功能，而是对hibernate-core进行封装，使得Hibernate符合JPA的规范。 数据库事务的四大级别、特点 原子性 一致性 隔离性 持久性 ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证 redis穿透问题 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"WebMagic","date":"2019-12-01T12:04:27.199Z","path":"/posts/a896d992/","text":"1、架构介绍1.1、四大组件WebMagic的结构分为Downloader、PageProcessor、Scheduler、Pipeline四大组件，并由Spider将彼此组织起来。这四大组件对应爬虫声明周期中的下载、处理、管理和持久化等功能。WebMagic的设计参考了Scapy，但是实现方式更Java化一些。 img 1.2、用于数据流转的对象1、Request request是对url地址的一层封装，一个Request对应一个url地址。 它是PageProcessor与Downloader交互的载体，也是PageProcessor控制Downloader唯一方式。 2、Page Page代表了从Downloader下载到的一个页面——可能是HTML，也可能是Json或者其他文本格式的内容。 Page是WebMagic抽取过程的核心对象，它提供一些方法可供抽取、结果保存等。 3、ResultItems ResultItems相当于一个Map，它保存PageProcessor处理的结果供Pipeline使用。它的API与Map很类似，值得注意的是它有一个字段skip，若设置为true，则不应被Pipeline处理。 2、实现PageProcessor2.1抽取元素SelectablewebMageic里使用的三种抽取技术：XPath、正则表达式和CSS选择器。对于 JSON格式的内容，可使用JsonPath进行解析。 2.2 抽取元素APISelectable相关的抽取元素链式API是WebMagic的一个核心 功能。使用Selectable接口，可以直接完成页面元素的链式抽取，也无需去关心抽取的细节。 从刚才的例子中可以看到，page.getHtml()返回的是一个Html对象，它实现了Selectable接口。这个接口包含的方法分为两类：抽取部分和获取结果部分。 方法 说明 xpath(String xpath) 使用XPath选择 html.xpath(“//div[@class=’title’]”) $(String selector) 使用Css选择器选择 html.$(“div.title”) $(String selector, String attr) 使用Css选择器选择 html.$(“div.title”,”text”) css(String selector) 功能同$(),使用Css选择器选择 html.css(“div.title”) links() 选择所有链接 html.links() regex(String regex) 使用正则表达式抽取 html.regex(“\\(.\\*?)\\“) 不同API获取一个或多个元素 说明 示例 get() 返回一条String类型的结果 String link = html.links().get() toString() 同get()，返回一条String类型的结果 String link = html.links().toString() all() 返回所有抽取结果 List links = html.links().all() 3、实现多页面爬虫流程开始-> 列表页面 -> 获取url（url去重） -> url加入任务 ->结束 3.1 Scheduler组件Scheduler是WebMagic中进行URL管理的组件。 作用： 对待抓取的URL队列进行管理。 对已抓取的URL进行去重。 WebMagic内置了几个常用的Scheduler。 类 说明 备注 DulicateRemovedScheduler 抽象基类，提供一些模板方法 继承它可以实现自己的功能 QueueScheduler 使用内存队列保存待抓取URL PriorityScheduler 使用带有优先级的内存队列保存待抓取URL 耗费内存较QueueScheduler更大，但是当设置request.priority之后，只能使用PriorityScheduler才可使优先级生效 FileCacheQueueScheduler 使用文件保存抓取URL，可以在关闭程序并下次启动时，从之前抓取到的URL继续抓取 需指定路径，会建立.urls.txt和.cursor.txt RedisScheduler 使用R额滴神保存抓取队列，可进行多台机器同时合作抓取 需要安装并启动redis 去重部分被单独抽象成一个接口：DuplicateRemover，从而可以为同一个Scheduler选择不同的去重方式，以适应不同的需要，目前提供两种去重方式。 类 说明 HashSetDuplicateRemover 使用HashSet来进行去重，占用内存较大 BloomFilterDuplicateRemover 使用BloomFilter来进行去重，占用内存较小，但是可能漏抓页面 3.2 三种去重方式 HashSet 使用Java中的HashSet不能重复的特点去重 优点：容易理解，使用方便。 缺点：占用内存大，性能较低。 Redis去重 使用Redis的set进行去重。 优点：速度快（Redis本身速度快），去重不会占有爬虫服务器的资源，可以处理更大数据量的数据爬取 缺点：需要准备Redis服务器，增加开发和使用成本。 布隆过滤器（BloomFilter） 优点：占用的内存要比使用HashSet要小的多，也适合大量数据的去重操作。 缺点：有误判的可能。没有重复可能会判断重复，但是重复的数据一定会判断重复。 介绍： 布隆过滤器是一种space efficient的概率型数据结构，用于判断一个元素是否在集合中。在垃圾邮件过滤黑白名单方法、爬虫（crawler）的网址判重模块中等等经常被用到。 哈希表也能用于判断元素是否在集合中，但布隆过滤器只需要哈希表的1/8或1.4的空间复杂度就能完成同样的问题。布隆过滤器可以插入元素，但不可以删除已有元素。其中的元素越多，误报率越大，但是漏报是不可能的。 原理： 布隆过滤器需要的是位数组（二进制数组），全部赋值为0，对于有n个元素的集合S={S1, S2…Sn}，通过k个映射函数{f1,f2,…..fk}，将集合S中的每个元素Sj（1 { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T11:42:01.012Z","path":"/posts/0/","text":"一、数据库1、left join 和 inner join的区别 1、举例a、b表： bId bNum 1 20 2 30 aId aNum 1 10 2 20 3 30 left join以左表为准 select * from a left join b on a.aId=b.bId; aId aNum bId bNum 1 10 1 20 2 20 2 30 3 30 NULL NULL inner join 选两张表都有的，一一相等的 aId aNum bId bNum 1 10 1 20 2 20 2 30 left join 会保留左边表的数据，而inner join是等值连接，必须满足条件才会有数据。 left join的速度比inner join快些。 2、drop、truncate、delete区别最基本： drop直接删掉表。 truncate删除表中数据，再插入时自增长id又从1开始。 delete删除表中数据，可以加where字句。 3.触发器的作用？ 触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。 4.什么是存储过程？用什么来调用？ 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 调用： 1）可以用一个命令对象来调用存储过程。 2）可以供外部程序调用，比如：java程序。 5.什么叫视图？游标是什么？ 视图： 是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。 游标： 是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。 6.非关系型数据库和关系型数据库区别，优势比较? 非关系型数据库的优势： 性能：NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性：同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 关系型数据库的优势： 复杂查询：可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持：使得对于安全性能很高的数据访问要求得以实现。 7、范式第一范式：（确保每列保持原子性），所有的字段值都是不可分解的原子值。 第二范式：（确保表中的每列都和主键相关）在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 第三范式：（确保每列都和主键列直接相关，而不是间接相关）数据表中的每一列数据都和主键直接相关，而不能间接相关。 8、索引1.什么是索引？ 何为索引： 数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2.索引的作用？它的优点缺点是什么？ 索引作用： 协助快速查询、更新数据库表中数据。 为表设置索引要付出代价的： 一是增加了数据库的存储空间 二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动) 3.索引的优缺点？ 创建索引可以大大提高系统的性能（优点）： 1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 增加索引也有许多不利的方面(缺点)： 1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4.哪些列适合建立索引、哪些不适合建索引？ 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。 一般来说，应该在这些列上创建索引： （1）在经常需要搜索的列上，可以加快搜索的速度； （2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； （3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； （4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； （5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； （6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 对于有些列不应该创建索引： （1）对于那些在查询中很少使用或者参考的列不应该创建索引。 这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 （2）对于那些只有很少数据值的列也不应该增加索引。 这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 （3）对于那些定义为text, image和bit数据类型的列不应该增加索引。 这是因为，这些列的数据量要么相当大，要么取值很少。 (4)当修改性能远远大于检索性能时，不应该创建索引。 这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 5.什么样的字段适合建索引 唯一、不为空、经常被查询的字段 6.MySQL B+Tree索引和Hash索引的区别?Hash索引和B+树索引的特点： Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位; B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问; 为什么不都用Hash索引而使用B+树索引？ Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样； Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样； Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用； Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据； Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。 9、事务1.什么是事务？ 事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。 2.事务四大特性（ACID）原子性、一致性、隔离性、持久性? 原子性（Atomicity）:原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency）:事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）:隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）:持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 !!!!! 事务隔离级别为：未提交读时，写数据只会锁住相应的行。 事务隔离级别为：可重复读时，写数据会锁住整张表。 事务隔离级别为：串行化时，读写数据都会锁住整张表。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大 10、JDBC连接步骤 1、加载JDBC驱动 2、建立连接 3、写SQL语句 4、得到preparedStatement 5、执行sql，得到结果集ResultSet 6、处理结果集 7、关闭资源 数据库优化1、存储引擎，包括myisam，innoDB，锁。 2、缓存 3、执行计划、SQL运行 4、分表 5、读写分离、主从复制 6、碎片整理 7、备份 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T11:40:28.576Z","path":"/posts/0/","text":"Redisredis支持的数据类型String，hash（哈希），List（列表），set（集合），zset（有序集合） Redis持久化持久化就是把内存的数据写到磁盘中去，防止服务器宕机了内存数据丢失。 持久化方式：RDB(默认) 和 AOF RDB：redis DataBase 功能核心函数rdbSave（生成RDB文件）和rdbLoad（从文件加载内存）两个函数 AOF：Append-only file 比较： 1、AOF文件比RDB更新频率高，优先使用AOF还原数据。 2、AOF比RDB更安全也更大 3、RDB性能比AOF好 4、如果两个都配了优先加载AOF 架构模式单机版： 特点：简单 问题：1、内存容量有限 2、处理能力有限 3、无法高可用。 主从复制： 有多个redis服务器，数据从主服务器（master）复制到从服务器（slave），数据写入主服务器，从服务器负责读取数据。 特点： 1、master/slave 角色 2、master/slave 数据相同 3、降低 master 读压力在转交从库 问题： 无法保证高可用 没有解决 master 写的压力 哨兵： Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。 特点： 1、保证高可用 2、监控各个节点 3、自动故障迁移 缺点：主从模式，切换需要时间丢数据 没有解决 master 写的压力 集群（proxy）： Twemproxy 是一个 Twitter 开源的一个 redis 和 memcache 快速/轻量级代理服务器； Twemproxy 是一个快速的单线程代理程序，支持 Memcached ASCII 协议和 redis 协议。 特点： 1、多种 hash 算法：MD5、CRC16、CRC32、CRC32a、hsieh、murmur、Jenkins 2、支持失败节点自动删除 3、后端 Sharding 分片逻辑对业务透明，业务方的读写方式和操作单个 Redis 一致 缺点：增加了新的 proxy，需要维护其高可用。 集群（直连型）： 从redis 3.0之后版本支持redis-cluster集群，Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。 特点： 1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。 2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。 3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。 4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本 5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master 的角色提升。 缺点： 1、资源隔离性较差，容易出现相互影响的情况。 2、数据通过异步复制,不保证数据的强一致性 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T10:47:57.762Z","path":"/posts/0/","text":"一、Spring1、Spring的理解 IOC容器，主要用来管理一个对象的，像以前的MVC经典三层，它们各层之间的对象耦合度特别的强，通过new来每一层，才能够调用每一层，而我使用IOC能够对这个MVC3层进行一个解耦，我具体的做法是在这个spring的一个配置文件中取一个bean标签使用一个叫做class的一个属性，然后将这个对象加入到这个IOC容器中，也要取一个ID属性，方便对这个对象的取用，将return回来的对象，然后通过@bean的一个注解，把它加入到一个spring的ioc容器中，它的方法名就是使用一个默认的ID，也可以使用@bean注解里面，起一个ID（属性），改一下名字，在起用这个IOC（容器中）对象的时候，可以使用两个注解，一个是@resource，可以按一个名字来取出这个对象，一个是@Autwired，是按照一个类型来取出这个对象的。这个IOC它使用的是一个map做这个IOC的容器。 AOP，讲到AOP先讲OOP的编程思想，OOP是一个自上而下的编程思想，AOP是横切式的编程思想，把代码横切，编入代码这种。AOP的编程思想方式主要有两种，一种是使用aspectj的一个方式，另一个使用springAOP的一个方式，springAOP是借助了aspectj的语法实现了AOP的编程思想，配置springAOP，就要在它的spring配置文件上面配置切面，需要增强的类型，相应的切入点。也可以使用注解的方式进行配置，起一个叫@aspect的一个切面类，在类里面起方法，这个方法里面的内容就是相应的要植入到目标类的一个逻辑代码，并加上一个增强注解，比如说@before，@after，然后用@pointcut来指定这个目标类上面的哪个方法执行，我使用比较多的aop场景，是在监控日志、事务控制、权限管理这块。 AOP的底层是怎么实现的：使用的是JDK的动态代理和cglib的动态代理。 2、@Autwired 和 @Resource区别3、spring注入方式4、拦截器和过滤器的底层5、bean的作用域问题6、编译内核7、面向对象和面向切面8、mybatis的一对多，多对一，以及多对对的配置和使用 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T10:32:11.282Z","path":"/posts/0/","text":"一、线程1、线程的类别 Thread（类）、Runable、Callable、Future，后面三个都是接口。 Runable有个无返回值的run的抽象方法，使用时可通过实现类重写run方法，使用Thread封装实现类，然后运行。 Callable有一个有返回值V的call方法，并抛出Exception异常。使用时做Callable的实现类，使用FutureTask类做封装，再用Thread类对FutureTask做封装，运行 FutureTask继承RunableFuture，RunableFuture实现Runable和Callable Thread的构造方法有对Runable的参数，重载的方法还有（Runable r, String name） 2、线程池，创建方式 创建大小不固定的线程池：类实现Runable，重写run方法 1、创建大小不固定的线程池：主函数创建线程池的方式，具有缓冲功能的线程池，系统根据需要创建线程，线程会被缓冲到线程池中，如果线程池大小超过了处理任务所需要的线程，线程池就会回收空闲的线程池，当处理任务增加时，线程池可以增加线程来处理任务，线程池不会对线程的大小进行限制，线程池的大小依赖于操作系统 123456ExecutorService es=Executors.newCachedThreadPool();for(int i=0;i { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-21T07:50:15.893Z","path":"/posts/0/","text":"一、计算机网络1、GET和POST区别GET GET 请求可被缓存 GET 请求保留在浏览器历史记录中 GET 请求可被收藏为书签 GET 请求不应在处理敏感数据时使用 GET 请求有长度限制 GET 请求只应当用于取回数据 POST POST 请求不会被缓存 POST 请求不会保留在浏览器历史记录中 POST 不能被收藏为书签 POST 请求对数据长度没有要求 2、TCP 与 UDP的区别1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接 2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付 Tcp通过校验和，重传控制，序号标识，滑动窗口、确认应答实现可靠传输。如丢包时的重发控制，还可以对次序乱掉的分包进行顺序控制。 3、UDP具有较好的实时性，工作效率比TCP高，适用于对高速传输和实时性有较高的通信或广播通信。 4.每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信 5、TCP对系统资源要求较多，UDP对系统资源要求较少。 3、一次完整的HTTP请求过程 1、域名解析 2、发起TCP的3次握手，建立TCP连接 3、发起http请求 4、服务器响应http请求，浏览器得到html代码 5、浏览器解析html代码，并请求html代码中的资源（如js、css、图片等） 6、浏览器对页面进行渲染呈现给用户 4、HTTPS和HTTP的区别 https协议需要到CA申请证书，一般免费证书很少，需要交费。http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协 议。http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。http默认使用80端口，https默认使用443端口 OSI分层 物理层、数据链路层、网络层、运输层、会话层、表示层、应用层 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-19T02:52:46.633Z","path":"/posts/0/","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"","date":"2019-11-18T12:41:17.234Z","path":"/posts/0/","text":"NIO原理解析BIO的缺点两个地方有阻塞——并发需要多线程支持——多线程会造成服务器资源浪费 高并发量引起的问题一个使用传统阻塞I/O的系统,如果还是使用传统的一个请求对应一个线程这种模式,一旦有高并发的大量请求,就会有如下问题： 1、线程不够用, 就算使用了线程池复用线程也无济于事; 2、阻塞I/O模式下,会有大量的线程被阻塞,一直在等待数据,这个时候的线程被挂起,只能干等,CPU利用率很低,换句话说,系统的吞吐量差; 3、如果网络I/O堵塞或者有网络抖动或者网络故障等,线程的阻塞时间可能很长。整个系统也变的不可靠; NIO的设计思路设计初衷：利用单线程处理并发 初始设计缺陷： for循环在Java应用程序中——–>OS———–> java—jni—自己写的一个c for循环太多，且可能大多无意义———->epoll redis——单线程——C——epoll epoll性能大于select IO和NIO的区别原有的 IO 是面向流的、阻塞的，NIO 则是面向块的、非阻塞的。 怎么理解IO是面向流的、阻塞的java1.4以前的io模型，一连接对一个线程。 原始的IO是面向流的，不存在缓存的概念。Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区 Java IO的各种流是阻塞的，这意味着当一个线程调用read或 write方法时，该线程被阻塞，直到有一些数据被读取，或数据完全写入，该线程在此期间不能再干任何事情了。 阻塞I/O模型 怎么理解NIO是面向块的、非阻塞的NIO是面向缓冲区的。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就增加了处理过程中的灵活性。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来,根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个。 NIO的核心实现在标准IO API中，你可以操作字节流和字符流，但在新IO中，你可以操作通道和缓冲，数据总是从通道被读取到缓冲中或者从缓冲写入到通道中。 NIO核心API Channel, Buffer, Selector 通道ChannelNIO的通道类似于流，但有些区别如下： \\1. 通道可以同时进行读写，而流只能读或者只能写 \\2. 通道可以实现异步读写数据 \\3. 通道可以从缓冲读数据，也可以写数据到缓冲: 可以从通道读取数据到缓冲区，也可以把缓冲区的数据写到通道中 缓存Buffer缓冲区本质上是一个可以写入数据的内存块，然后可以再次读取，该对象提供了一组方法，可以更轻松地使用内存块，使用缓冲区读取和写入数据通常遵循以下四个步骤： \\1. 写数据到缓冲区； \\2. 调用buffer.flip()方法； \\3. 从缓冲区中读取数据； \\4. 调用buffer.clear()或buffer.compat()方法； 当向buffer写入数据时，buffer会记录下写了多少数据，一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式，在读模式下可以读取之前写入到buffer的所有数据，一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。 Buffer在与Channel交互时，需要一些标志: buffer的大小/容量 - Capacity 作为一个内存块，Buffer有一个固定的大小值，用参数capacity表示。 当前读/写的位置 - Position 当写数据到缓冲时，position表示当前待写入的位置，position最大可为capacity – 1；当从缓冲读取数据时，position表示从当前位置读取。 信息末尾的位置 - limit 在写模式下，缓冲区的limit表示你最多能往Buffer里写多少数据； 写模式下，limit等于Buffer的capacity，意味着你还能从缓冲区获取多少数据。 下图展示了buffer中三个关键属性capacity，position以及limit在读写模式中的说明： buffer中三个关键属性capacity，position以及limit在读写模式中的说明 缓冲区常用的操作 向缓冲区写数据： ​ \\1. 从Channel写到Buffer； ​ \\2. 通过Buffer的put方法写到Buffer中； 从缓冲区读取数据： ​ \\1. 从Buffer中读取数据到Channel； ​ \\2. 通过Buffer的get方法从Buffer中读取数据； flip方法： ​ 将Buffer从写模式切换到读模式，将position值重置为0，limit的值设置为之前position的值； clear方法 vs compact方法： ​ clear方法清空缓冲区；compact方法只会清空已读取的数据，而还未读取的数据继续保存在Buffer中； Selector一个组件，可以检测多个NIO channel，看看读或者写事件是否就绪。 多个Channel以事件的方式可以注册到同一个Selector，从而达到用一个线程处理多个请求成为可能。 一个thread对应多个channel,一个channel处理一个请求。 当你调用Selector的select()或者 selectNow() 方法它只会返回有数据读取的SelectableChannel的实例. 模拟一个单线程的处理高并发的应用模拟JVM（C C++）实现socket对象的源码模拟一个C语言 JVM当中实现NIO document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"hadoop详细学习","date":"2019-09-13T09:50:11.087Z","path":"/posts/a5ead4c6/","text":"1、hadoop1.1 hadoop介绍1）hadoop是一个由Apache基金会所开发的分布式系统基础架构。 2）主要解决海量数据的存储和海量数据的分析计算问题。 3）Hadoop生态圈 1.2 Lucence框架1）Lucene框架使Doug Cutting开创的开源软件，用户Java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎。 2）2001年年底Lucene成为Apache基金会的一个子项目。 3）对于海量数据的场景，Lucence面对与Google同样的困难，存储数据困难，检索速度慢。 4）学习和模仿Google解决这些问题的办法：微型版Nutch。 5）Google是Haddop的思想之源。 6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 7）2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 8）2006年3月份，Map-Reduce和Nutch Distributed File System（NDFS）分别被纳入称为Hadoop的项目中。 9）名字来源于Doug Cutting儿子的玩具大象。 1.3 hadoop三大发行版本Apache、Cloudera、Hortonworks。 1.4 hadoop的优势（面试）1)高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。 2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。 3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 4）高容错性：能够自动将失败的任务重新分配。 1.5 hadoop1.x 和hadoop2.x区别hadoop1.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算+资源调度） CPU 8 内存：128MB 磁盘 8T hadoop2.x组成： Common（辅助工具）、HDFS（数据存储）、MapReduce（计算）、yarn（资源调度） CPU 8 内存：128MB 磁盘 8T 对比：hadoop1.x时代，MapReduce既处理运算，又处理资源调度，耦合性较大。hadoop1.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。 1.6 组件架构1、hdfs架构概述1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、福本数、文件权限），以及每个文件的块列表和块所在的DataNode等。 2）DataNode（dn）：在本地文件系统存储文件数据，以及数据的校验和。 3）Secondary NameNode（2nn）：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。加快启动速度。 2、Yarn架构1）ResourceManager （RM）主要作用如下 （1）处理客户端请求 （2）监控NodeManager （3）启动或监控ApplicationMaster（集群中运行的一个job） （4）资源的分配与调度 yarn架构 2）NodeManager（NM）主要作用如下 （1）管理单个节点上的资源 （2）处理来自ResourceManager的命令 （3）处理来自ApplicationMaster的命令 3）ApplicationMaster（AM）作用如下 （1）负责数据的切分 （2）为某个程序申请资源并分配给内部的任务 （3）任务的监控与容错 4）Container container是Yarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 3、MapReduce架构概述MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据。 2）Reduce阶段对Map结果进行汇总 1.7 大数据技术生态体系 大数据技术生态体系 图中涉及的技术名词解释如下： 1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行**数据的迁移，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： （1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 （2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 （3）支持通过Kafka服务器和消费机集群来分区消息。 （4）支持Hadoop并行数据加载。 4）Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。 6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。 7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 11）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。 12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 1.8 hadoop重要目录（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本 （2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 （3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） （4）sbin目录：存放启动或停止Hadoop相关服务的脚本 （5）share目录：存放Hadoop的依赖jar包、文档、和官方案例 1.9完全分布式运行模式1、准备三台客户机（关闭防火墙、静态ip、主机名称） 2、安装JDK 3、配置环境变量 4、安装hadoop 5、配置环境变量 6、配置集群 7、单点启动 8、配置ssh 9、群起并测试集群 2、HDFS2、1 介绍产生背景：数据量大，一个操作系统存不过来，需要一种系统管理多台机器上的文件 定义：HDFS(Hadoop Distributed File System)，分布式文件管理系统。用于存储文件，通过目录树来定位文件。 使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。 2.2.1 优点1、高容错性 （1）数据自动保存多个副本。通过增加副本的形式，提高容错性。 （2）某个副本丢失后，可以自动恢复 2、适合处理大数据 （1）数据规模：那个处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3、可构建在廉价机器上，通过多副本机制，提高可靠性。 2.2.2 缺点1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 2）无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 3）不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许 多个线程同时写。 （2）仅支持数据append（追加），不支持文件的随机修改。 2.3 组件功能1）NameNode（nn）：就是Master，它是一个管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据块（Block）映射信息； （4）处理客户端读写请求。 2）DataNode：就是slave。NameNode下达命令。DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 3）client：客户端 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如NameNode格式化； （5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作； 4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能替代NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 2.4 文件块大小HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版中是64M。 1、集群中的block，2、如果寻址时间约为10ms，即查到目标block的时间为10ms，3寻址时间为传输时间的1%时，则为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s。4 而目前磁盘的传输速率普遍为100MB/s。 思考：为什么块的大小不能设置太小，也不能设置太大？ 1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； 2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需要的时间。导致程序在处理这块数据时。会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 2.5 HDFS的数据流（面试重点）1、HDFS的写数据流程 1568448314627 然后重复3-7步骤 2、网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 1568448821111 3、副本节点选择 1568449200994 4、HDFS的读数据流程 1568449571543 2.6 NameNode和SecondaryNameNode（面试开发重点）1、NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。 1568450098120 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改。 \\2. 第二阶段：Secondary NameNode工作 ​ （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。 ​ （2）Secondary NameNode请求执行CheckPoint。 ​ （3）NameNode滚动正在写的Edits日志。 ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint。 ​ （7）拷贝fsimage.chkpoint到NameNode。 ​ （8）NameNode将fsimage.chkpoint重新命名成fsimage。 注： NN和2NN工作机制详解： Fsimage：NameNode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。 SecondaryNameNode首先会询问是否需要（触发需要满足两个条件中的任意一个，定时时间到和中数据写满了）。直接带回是否检查结果。执行操作，首先会让滚动并生成一个空的，滚动的目的是给打个标记，以后所有新的操作都写入，其他未合并的和会拷贝到的本地，然后将拷贝的和加载到内存中进行合并，生成，然后将拷贝给，重命名为后替换掉原来的。在启动时就只需要加载之前未合并的和即可，因为合并过的中的元数据信息已经被记录在中。 1568451755272 2.7 DataNode 工作机制 2.7.1DataNode掉线时限参数设置1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信 2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。 3、HDFS默认的超时时长为10分钟+30秒。 4=如果定义超时时间为TimeOut，则超时时长的计算公式为：timeout = 2XXX(分钟)+10\\YYY（秒）。 2.7.2 服役新数据节点、退役旧数据节点1、添加白名单 2、黑名单退役 3、不允许白名单和黑名单中同时出现同一个主机名称。 2.8 HDFS新特性2.8.1 集群间的数据拷贝1．scp实现两个远程主机之间的文件复制 12345scp -r hello.txt [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) // 推 pushscp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pullscp -r [root@hadoop103:/user/atguigu/hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt) root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2．采用distcp命令实现两个Hadoop集群之间的递归数据复制 12[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 2.8.2 小文件存档1、HDFS存储小文件的弊端 大量的小文件会消耗NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量与数据块的大小无关。 2、解决存储小文件的办法之一 HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。也就是说HDFS存档问你件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 3、实操 1）先启动hadoop集群 2）把某目录下的所有文件归档成xx.har的归档文件，并把归档后文件存储到xxx/output路径下。 1[root@master ~]# hadoop archive -archiveName input.har -p /data/input /data/out 3）查看归档 1234[root@master ~]# hadoop fs -ls -R har:///data/out/input.har-rw-r--r-- 3 root supergroup 38 2019-09-15 23:21 har:///data/out/input.har/1-rw-r--r-- 3 root supergroup 26 2019-09-15 23:21 har:///data/out/input.har/2-rw-r--r-- 3 root supergroup 97 2019-09-13 22:54 har:///data/out/input.har/wc.input 4）解归档文件 1hadoop fs -cp har:/// data/output/input.har/* /data 2.8.3 回收站功能参数1、默认值fs.trash.interval = 0, 0表示禁用回收站；其他值表示设置文件的存活时间。 2、默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。 3、要求：fs.trash.checkpoint.interval o.getSumFlow() ? -1 : 1; } 3.6.2 切片与MapTask并行度决定机制1．问题引出 MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。 思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 2．MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。 3.6.3 FileInputFormat切片源码解析：重点： 默认情况：切片大小= blocksize 每次切片时，要判断切完剩余部分是否大于块的1.1倍，不大于1.1倍就划分1块切片 img 切片时不考虑数据集整体，而是逐个对每一个文件单独切片。 3.6.4 FileInputFormat切片大小的参数配置 img 3.6.5 CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。 1、应用场景： CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。 2、虚拟存储切片最大值设置 CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 3、切片机制 生成切片过程包括：虚拟存储过程和切片过程二部分。！！！！ img 3.6 MapReduce详细工作流程 1568639170123 1568639177258 3.7 Shuffle机制3.7.1 Partition分区1、问题引出 要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 2、默认Partitioner分区 Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。如图4-14所示。 img 3.7.2 WritableComparable排序排序概述： ​ 排序是MapReduce框架中最重要的操作之一。 ​ MapTask和ReduceTask均会对数据按照Key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。 ​ 默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 ​ 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 ​ 1、部分排序 ​ MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。 ​ 2、全排序 ​ 最终输出结果只有一个文件，且文件内部有序。实现方法是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。 ​ 3、辅助排序（GroupingComparator分组） ​ 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 ​ 4、二次排序 ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 1、实现全排序 2、实现区内排序 3.7.3 Combiner合并（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。 （2）Combiner组件的父类就是Reducer （3）Combiner和Reducer的区别在于运行的位置 Combiner是在每一个MapTask所在的节点运行 Reducer是接收全局所有Mapper的输出结果 （4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。 （5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且Combiner的输出kv应该跟Reducer的输入kv类型对应起来。 order举例 3.8 MapReduce +shuffle 工作机制（面试） img （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 img （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.9 Join多种应用3.9.1 Reduce Join1、Reduce Join工作原理： ​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 ​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就OK了。 2、缺点及解决方案 ​ 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。 ​ 解决方案：Map端实现数据合并 3.9.2 Map Join 1．使用场景 Map Join适用于一张表十分小、一张表很大的场景。 2．优点 思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？ 在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。 3．具体办法：采用DistributedCache ​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 // 缓存普通文件到Task运行节点。 job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); 3.10 计数器应用hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 1、计数器API （1）采用枚举的方式统计计数 （2）采用计数器组、计数器名称的方式统计 （3）计数结果在程序运行后的控制台上查看。 3.11 数据清洗（ETL）3.12 Hadoop数据压缩3.12.1 压缩概念压缩概述： 压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。 数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启动压缩。 压缩策略和原则 优化策略 通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。 注意：采用压缩技术减少了磁盘IO，但同时也增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。 压缩原则： 1、运算密集型的job，少用压缩 2、IO密集型的job，多用压缩 3.12.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 3.12.3 压缩方式选择Gzip压缩： 优点：压缩率比较高，压缩/解压速度快；hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后130M以内的（1个块大小内），都可以考虑Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。 Bzip2压缩： 优点：支持split，具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便。 缺点：压缩/解压速度慢。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序的情况。 Lzo 优点：压缩/解压速度也比较快，合理的压缩率，支持split，是Hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定InputFormat为Lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越明显。 Snappy 优点：高速压缩速度和合理的压缩率。 缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装。 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另一个MapReduce作业的输入。 3.12.4 压缩位置的选择 img 5、Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 img 工作机制详解 ​ （1）MR程序提交到客户端所在的节点。 ​ （2）YarnRunner向ResourceManager申请一个Application。 ​ （3）RM将该应用程序的资源路径返回给YarnRunner。 ​ （4）该程序将运行所需资源提交到HDFS上。 ​ （5）程序资源提交完毕后，申请运行mrAppMaster。 ​ （6）RM将用户的请求初始化成一个Task。 ​ （7）其中一个NodeManager领取到Task任务。 ​ （8）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （9）Container从HDFS上拷贝资源到本地。 ​ （10）MRAppmaster向RM 申请运行MapTask资源。 ​ （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。 ​ （13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。 ​ （14）ReduceTask向MapTask获取相应分区的数据。 ​ （15）程序运行完毕后，MR会向RM申请注销自己。 img 6、优化6.1 MapReduce优化MapReduce程序效率的瓶颈在于两点： 1、计算机性能 ​ CPU、内存、磁盘健康、网络 2、I/O操作优化 （1）数据倾斜 （2）Map和Reduce数设置不合理 （3）Map运行时间太长，导致Reduce等待过久 （4）小文件过多 （5）大量的不可分块超大文件 （6）Spill次数过多 （7）Merge次数过多等。 优化方法： 1、数据输入 （1）合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 2、Map阶段 （1）减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO. （2）减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 （3）在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O。 3、Reduce阶段 （1）合理设置Map和Reduce数。 （2）设置Map、Reduce共存。调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。 （3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 （4）合理设置Reduce端的Buffer。 4、I/O传输 （1）采用数压缩的方式 （2）使用SequenceFile二进制文件 5、数据倾斜问题 1）数据倾斜现象 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 2）减少数据倾斜的方法 1）抽样和范围分区 2）自定义分区 3）Combine 4）采用Map Join， 尽量避免Reduce Join。 6.2HDFS小文件优化6.2.1 HDFS小文件弊端HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。 6.2.2 HDFS小文件解决方案小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。 （2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。 （3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。 1、MAVEN内存溢出!（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,nativeN -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 DataNode和NameNode进程同时只能工作一个。 clusterId不统一 10）执行命令不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 11）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 12）jps不生效。 原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。 13）8088端口连接不上 [atguigu@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102 ​ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"机器学习算法","date":"2019-09-09T13:04:00.000Z","path":"/posts/0/","text":"EM算法最大期望算法（Expectation-Maximization algorithm, EM），或Dempster-Laird-Rubin算法 [1] ，是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法 [2] ，通常作为牛顿迭代法（Newton-Raphson method）的替代用于对包含隐变量（latent variable）或缺失数据（incomplete-data）的概率模型进行参数估计 [2-3] 。 EM算法的标准计算框架由E步（Expectation-step）和M步（Maximization step）交替组成，算法的收敛性可以确保迭代至少逼近局部极大值 [4] 。EM算法是MM算法（Minorize-Maximization algorithm）的特例之一，有多个改进版本，包括使用了贝叶斯推断的EM算法、EM梯度算法、广义EM算法等 [2] 。 https://www.cnblogs.com/jiangxinyang/p/9278608.html document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"JAVA笔试经典题型及原因","date":"2019-09-09T13:04:00.000Z","path":"/posts/0/","text":"JAVA笔试经典题型及原因1、JVM​ JVM是Java Virtual Machine（Java虚拟机）的缩写，JVM是一种用于计算设备的规范，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 ​ 引入Java语言虚拟机后，Java语言在不同平台上运行时不需要重新编译。Java语言使用Java虚拟机屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。 1、内存管理​ （1）Java运行时涉及到的存储区域主要包括程序计数器、Java虚拟机栈、本地方法栈、java堆、方法区以及直接内存等等。 img 大多数JVM将内存区域划分为 Method Area(Non-Heap)(方法区),Heap(堆)Program Counter Register(程序计数器), VM Stack(虚拟机栈,也有翻译成JAVA方法栈的),Native Method Stack本地方法栈) 。 其中Method Area(方法区) 和Heap（堆） 是线程共享的， VM Stack，Native Method Stack 和Program Counter Register 是非线程共享的。 ​ 首先我们熟悉一下一个一般性的 Java 程序的工作过程。一个 Java 源程序文件，会被编译为字节码文件（以 class 为扩展名），每个java程序都需要运行在自己的JVM上，然后告知 JVM 程序的运行入口，再被 JVM 通过字节码解释器加载运行。 ​ 概括地说来，JVM初始运行的时候都会分配好 Method Area（方法区） 和Heap（堆） ，而JVM每遇到一个线程，就为其分配一个 Program Counter Register（程序计数器） , VM Stack（虚拟机栈）和Native Method Stack （本地方法栈）， 当线程终止时，三者（虚拟机栈，本地方法栈和程序计数器）所占用的内存空间也会被释放掉。这也是为什么我把内存区域分为线程共享和非线程共享的原因，非线程共享的那三个区域的生命周期与所属线程相同，而线程共享的区域与JAVA程序运行的生命周期相同，所以这也是系统垃圾回收的场所只发生在线程共享的区域（实际上对大部分虚拟机来说知发生在Heap上）的原因。 2、题目 1、下面有关JVM内存，说法错误的是？ A.程序计数器是一个比较小的内存区域，用于指示当前线程所执行的字节码执行到了第几行，是线程隔离的 B.虚拟机栈描述的是Java方法执行的内存模型，用于存储局部变量，操作数栈，动态链接，方法出口等信息，是线程隔离的 C.方法区用于存储JVM加载的类信息、常量、静态变量、以及编译器编译后的代码等数据，是线程隔离的 D.原则上讲，所有的对象都在堆区上分配内存，是线程之间共享的 解析：堆区、方法区是线程共享的，程序计数区、虚拟机栈区是线程隔离的。本题选C，出错的原因就在此。 2、SQL1、Statement、PreparedStatement、CallableStatement1.它们都是接口(interface)。 2.Statement继承自Wrapper、PreparedStatement继承自Statement、CallableStatement继承自PreparedStatement。 3.Statement接口提供了执行语句和获取结果的基本方法； PreparedStatement接口添加了处理 IN 参数的方法； CallableStatement接口添加了处理 OUT 参数的方法。 4. a.Statement: 普通的不带参的查询SQL；支持批量更新,批量删除; b.PreparedStatement: 可变参数的SQL,编译一次,执行多次,效率高; 安全性好，有效防止Sql注入等问题; 支持批量更新,批量删除; c.CallableStatement: 继承自PreparedStatement,支持带参数的SQL操作; 支持调用存储过程,提供了对输出和输入/输出参数(INOUT)的支持; Statement每次执行sql语句，数据库都要执行sql语句的编译,最好用于仅执行一次查询并返回结果的情形，效率高于PreparedStatement。 PreparedStatement是预编译的，使用PreparedStatement有几个好处 在执行可变参数的一条SQL时，PreparedStatement比Statement的效率高，因为DBMS预编译一条SQL当然会比多次编译一条SQL的效率要高。 安全性好，有效防止Sql注入等问题。 对于多次重复执行的语句，使用PreparedStament效率会更高一点，并且在这种情况下也比较适合使用batch； 代码的可读性和可维护性。 2、题目1、下面有关jdbc statement的说法错误的是？ 12> A.JDBC提供了Statement、PreparedStatement 和 CallableStatement三种方式来执行查询语句，其中 Statement 用于通用查询， PreparedStatement 用于执行参数化查询，而 CallableStatement则是用于存储过程> 12> B.对于PreparedStatement来说，数据库可以使用已经编译过及定义好的执行计划，由于 PreparedStatement 对象已预编译过，所以其执行速度要快于 Statement 对象”> 12> C.PreparedStatement中，“?” 叫做占位符，一个占位符可以有一个或者多个值> 12> D.PreparedStatement可以阻止常见的SQL注入式攻击> 解析：选C，占位符只能有1个值。 3、Spring事务事务属性的种类： 传播行为、隔离级别、只读和事务超时 a) 传播行为定义了被调用方法的事务边界。 传播行为 意义 PROPERGATION_MANDATORY 表示方法必须运行在一个事务中，如果当前事务不存在，就抛出异常 PROPAGATION_NESTED 表示如果当前事务存在，则方法应该运行在一个嵌套事务中。否则，它看起来和PROPAGATION_REQUIRED 看起来没什么俩样 PROPAGATION_NEVER 表示方法不能运行在一个事务中，否则抛出异常 PROPAGATION_NOT_SUPPORTED 表示方法不能运行在一个事务中，如果当前存在一个事务，则该方法将被挂起 PROPAGATION_REQUIRED 表示当前方法必须运行在一个事务中，如果当前存在一个事务，那么该方法运行在这个事务中，否则，将创建一个新的事务 PROPAGATION_REQUIRES_NEW 表示当前方法必须运行在自己的事务中，如果当前存在一个事务，那么这个事务将在该方法运行期间被挂起 PROPAGATION_SUPPORTS 表示当前方法不需要运行在一个是事务中，但如果有一个事务已经存在，该方法也可以运行在这个事务中 b) 隔离级别 在操作数据时可能带来 3个副作用，分别是脏读、不可重复读、幻读。为了避免这 3 中副作用的发生，在标准的 SQL 语句中定义了 4种隔离级别，分别是未提交读、已提交读、可重复读、可序列化。而在 spring 事务中提供了 5 种隔离级别来对应在 SQL 中定义的 4种隔离级别，如下： 隔离级别 意义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别 ISOLATION_READ_UNCOMMITTED 允许读取未提交的数据（对应未提交读），可能导致脏读、不可重复读、幻读 ISOLATION_READ_COMMITTED 允许在一个事务中读取另一个已经提交的事务中的数据（对应已提交读）。可以避免脏读，但是无法避免不可重复读和幻读 ISOLATION_REPEATABLE_READ 一个事务不可能更新由另一个事务修改但尚未提交（回滚）的数据（对应可重复读）。可以避免脏读和不可重复读，但无法避免幻读 ISOLATION_SERIALIZABLE 这种隔离级别是所有的事务都在一个执行队列中，依次顺序执行，而不是并行（对应可序列化）。可以避免脏读、不可重复读、幻读。但是这种隔离级别效率很低，因此，除非必须，否则不建议使用。 c) 只读 如果在一个事务中所有关于数据库的操作都是只读的，也就是说，这些操作只读取数据库中的数据，而并不更新数据，那么应将事务设为只读模式（ READ_ONLY_MARKER ） , 这样更有利于数据库进行优化 。 因为只读的优化措施是事务启动后由数据库实施的，因此，只有将那些具有可能启动新事务的传播行为(PROPAGATION_NESTED 、 PROPAGATION_REQUIRED 、 PROPAGATION_REQUIRED_NEW) 的方法的事务标记成只读才有意义。 如果使用 Hibernate 作为持久化机制，那么将事务标记为只读后，会将 Hibernate 的 flush 模式设置为 FULSH_NEVER, 以告诉 Hibernate 避免和数据库之间进行不必要的同步，并将所有更新延迟到事务结束。 d) 事务超时 如果一个事务长时间运行，这时为了尽量避免浪费系统资源，应为这个事务设置一个有效时间，使其等待数秒后自动回滚。与设 置“只读”属性一样，事务有效属性也需要给那些具有可能启动新事物的传播行为的方法的事务标记成只读才有意义。 2、题目下面有关SPRING的事务传播特性，说法错误的是？ 12345> A.PROPAGATION_SUPPORTS：支持当前事务，如果当前没有事务，就以非事务方式执行> B.PROPAGATION_REQUIRED：支持当前事务，如果当前没有事务，就抛出异常> C.PROPAGATION_REQUIRES_NEW：新建事务，如果当前存在事务，把当前事务挂起> D.PROPAGATION_NESTED：支持当前事务，新增Savepoint点，与当前事务同步提交或回滚> 解析： PROPAGATION_REQUIRED–支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS–支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY–支持当前事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW–新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED–以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。PROPAGATION_NEVER–以非事务方式执行，如果当前存在事务，则抛出异常。 所以答案选择B。 4、Servlet1、CGI简介CGI（Common Gateway Interface）公共网关接口，是外部扩展应用程序与 Web 服务器交互的一个标准接口。服务器端与客户端进行交互的常见方式多，CGI 技术就是其中之一。根据CGI标准，编写外部扩展应用程序，可以对客户端浏览器输入的数据进行处理，完成客户端与服务器的交互操作。CGI规范定义了Web服务器如何向扩展应用程序发送消息，在收到扩展应用程序的信息后又如何进行处理等内容。对于许多静态的HTML网页无法实现的功能，通过 CGI可以实现，比如表单的处理、对数据库的访问、搜索引擎、基于Web的数据库访问等等。使用CGI实现客户端与服务器的交互有以下几个标准步骤，具体步骤如下： （1）Web 客户端的浏览器将URL的第一部分解码与Web服务器相连。 （2）Web 浏览器将URL的其余部分提供给服务器。 （3）Web 服务器将URL转换成路径和文件名。 （4）Web 服务器发送 HTML 和别的组成请求页面的文件给客户。一旦页面内容传送完， 这个连接自动断开。 （5）在客户端，HTML脚本提示用户做动作或输入。当用户响应后，客户请求Web服务器建立一个新的连接。 （6）Web 服务器把这些信息和别的进程变量传送给由HTML以URL的形式指定CGI程序。 （7）CGI 根据输入作出响应，把响应结果传送给 Web 服务器。 （8）Web 服务器把响应的数据传给客户，完成后关闭连接。 2、Servlet简介Servlet（Server Applet）是Java Servlet的简称，称为小服务程序或服务连接器，用Java编写的服务器端程序，具有独立于平台和协议的特性，主要功能在于交互式地浏览和生成数据，生成动态Web内容。 3、Servlet生命周期 客户端请求该 Servlet； 加载 Servlet 类到内存； 实例化并调用init()方法初始化该 Servlet； service()（根据请求方法不同调用doGet() 或者 doPost()，此外还有doHead()、doPut()、doTrace()、doDelete()、doOptions()、destroy()。 加载和实例化 Servlet。这项操作一般是动态执行的。然而，Server 通常会提供一个管理的选项，用于在 Server 启动时强制装载和初始化特定的 Servlet。 4、Servlet与CGI对比 与传统的 CGI 和许多其他类似 CGI 的技术相比，Java Servlet 具有更高的效率，更容易使用，功能更强大，具有更好的可移植性，更节省投资。在未来的技术发展过程中，Servlet 有可能彻底取代 CGI。 在传统的 CGI中，每个请求都要启动一个新的进程，如果 CGI 程序本身的执行时间较短，启动进程所需要的开销很可能反而超过实际执行时间。而在 Servlet 中，每个请求由一个轻量级的 Java 线程处理（而不是重量级的操作系统进程）。 在传统 CGI 中，如果有 N 个并发的对同一 CGI程序的请求，则该CGI程序的代码在内存中重复装载了 N 次；而对于 Servlet，处理请求的是 N 个线程，只需要一份 Servlet 类代码。在性能优化方面，Servlet 也比 CGI 有着更多的选择。 5、题目1、下面有关servlet和cgi的描述，说法错误的是？ 12345> A.servlet处于服务器进程中，它通过多线程方式运行其service方法> B.CGI对每个请求都产生新的进程，服务完成后就销毁> C.servlet在易用性上强于cgi，它提供了大量的实用工具例程，例如自动地解析和解码HTML表单数据、读取和设置HTTP头、处理Cookie、跟踪会话状态等> D.cgi在移植性上高于servlet，几乎所有的主流服务器都直接或通过插件支持cgi> 解析：cgi的移植性很差 2、下面有关servlet service描述错误的是？ 12345> A.不管是post还是get方法提交过来的连接，都会在service中处理> B.doGet/doPost 则是在 javax.servlet.GenericServlet 中实现的> C.service()是在javax.servlet.Servlet接口中定义的> D.service判断请求类型，决定是调用doGet还是doPost方法> 解析：doget/dopost与Http协议有关，是在 javax.servlet.http.HttpServlet 中实现的 img 3、下列有关Servlet的生命周期，说法不正确的是？ 12345> A.在创建自己的Servlet时候，应该在初始化方法init()方法中创建Servlet实例> B.在Servlet生命周期的服务阶段，执行service()方法，根据用户请求的方法，执行相应的doGet()或是doPost()方法> C.在销毁阶段，执行destroy()方法后会释放Servlet 占用的资源> D.destroy()方法仅执行一次，即在服务器停止且卸载Servlet时执行该方法> 解析：init()不是来创造servlet实例，而是用于初始化。详细见上面的生命周期。选A。 4、下面有关servlet中init,service,destroy方法描述错误的是？ 12345> A.init()方法是servlet生命的起点。一旦加载了某个servlet，服务器将立即调用它的init()方法> B.service()方法处理客户机发出的所有请求> C.destroy()方法标志servlet生命周期的结束> D.servlet在多线程下使用了同步机制，因此，在并发编程下servlet是线程安全的> 解析：选D servlet在多线程下其本身并不是线程安全的。 如果在类中定义成员变量，而在service中根据不同的线程对该成员变量进行更改，那么在并发的时候就会引起错误。最好是在方法中，定义局部变量，而不是类变量或者对象的成员变量。由于方法中的局部变量是在栈中，彼此各自都拥有独立的运行空间*而不会互相干扰，因此才做到线程安全。 5、Struts21、下面有关struts1和struts2的区别，描述错误的是？ 12345> A.Struts1要求Action类继承一个抽象基类。Struts 2 Action类可以实现一个Action接口> B.Struts1 Action对象为每一个请求产生一个实例。Struts2 Action是单例模式并且必须是线程安全的> C.Struts1 Action 依赖于Servlet API，Struts 2 Action不依赖于容器，允许Action脱离容器单独被测试> D.Struts1 整合了JSTL，Struts2可以使用JSTL，但是也支持OGNL> 解析：选B， struts2是多例模式 从action类上分析: 1.Struts1要求Action类继承一个抽象基类。Struts1的一个普遍问题是使用抽象类编程而不是接口。 Struts 2 Action类可以实现一个Action接口，也可实现其他接口，使可选和定制的服务成为可能。Struts2提供一个ActionSupport基类去实现常用的接口。Action接口不是必须的，任何有execute标识的POJO对象都可以用作Struts2的Action对象。从Servlet 依赖分析: Struts1 Action 依赖于Servlet API ,因为当一个Action被调用时HttpServletRequest 和 HttpServletResponse 被传递给execute方法。 Struts 2 Action不依赖于容器，允许Action脱离容器单独被测试。如果需要，Struts2 Action仍然可以访问初始的request和response。但是，其他的元素减少或者消除了直接访问HttpServetRequest 和 HttpServletResponse的必要性。从action线程模式分析: Struts1 Action是单例模式并且必须是线程安全的，因为仅有Action的一个实例来处理所有的请求。单例策略限制了Struts1 Action能作的事，并且要在开发时特别小心。Action资源必须是线程安全的或同步的。 Struts2 Action对象为每一个请求产生一个实例，线程不安全。（实际上，servlet容器给每个请求产生许多可丢弃的对象，并且不会导致性能和垃圾回收问题） document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"vue项目创建过程","date":"2019-08-08T03:08:59.729Z","path":"/posts/0/","text":"1、Vue 项目创建过程1.1项目搭建过程 创建项目 1# vue init webpack projectname 运行项目 1# npm run dev 1.2 遇到的问题解决方法新手创建时死机是经常出现的事，解决办法如下： 1231、清除缓存 npm clean --force （可能是这个，不太记得了）2、更新 npm install3、chromedriver无法下载，应该是谷歌退出中国网站不让访问，只能下载本地的网址。 1.3 项目前期准备举个栗子： 1.3.1 新建前端界面前端页面开发页面组件存放于src\\components文件夹中，新建一个Login组件。 1234567891011121314151617181920212223242526272829303132333435363738394041 用户名: 密码： 登录 export default { name: 'Login', data () { return { loginForm: { username: '', password: '' }, responseResult: [] } }, methods: { login () { this.$axios .post('/login', { username: this.loginForm.username, password: this.loginForm.password }) .then(successResponse => { if (successResponse.data.code === 200) { this.$router.replace({path: '/index'}) } }) .catch(failResponse => { }) } } } 1.3.2 前端相关配置——设置反向代理修改 src\\main.js 代码如下： 1234567891011121314151617import Vue from 'vue'import App from './App'import router from './router'// 设置反向代理，前端请求默认发送到 http://localhost:8443/apivar axios = require('axios')axios.defaults.baseURL = 'http://localhost:8443/api'// 全局注册，之后可在其他组件中通过 this.$axios 发送数据Vue.prototype.$axios = axiosVue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: ''}) 1.3.3 前端相关配置——设置页面路由修改 src\\router\\index.js 代码如下： 1234567891011121314151617import Vue from 'vue'import Router from 'vue-router'// 导入刚才编写的组件import Login from '@/components/Login'Vue.use(Router)export default new Router({ routes: [ // 下面都是固定的写法 { path: '/login', name: 'Login', component: Login } ]}) 1.3.4 前端相关配置——跨域支持为了让后端能够访问到前端的资源，需要配置跨域支持。 在 config\\index.js 中，找到 proxyTable 位置，修改为以下内容 123456789proxyTable: { '/api': { target: 'http://localhost:8443', changeOrigin: true, pathRewrite: { '^/api': '' } }} 1.4 各种 npm 安装1.4.1 element安装与配置1、安装命令 1npm i element-ui -S 2、main.js 中引入和使用ElementUI 1234import ElementUI from 'element-ui'import 'element-ui/lib/theme-chalk/index.css'Vue.use(ElementUI) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"git上传仓库步骤","date":"2019-07-15T09:33:15.538Z","path":"/posts/0/","text":"git上传仓库步骤有关git上传仓库步骤，请享用。 1、新建仓库 2、git config –global user.name “xxx” 3、git config –global user.email “xxx” 进入文件夹后初始化 git init git add . git pull –rebase 地址 //该步骤是复制之前的master，使得版本代码相同!! git commit -m “xxx1.version” git push 地址 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"hbase笔记","date":"2019-05-09T07:36:00.000Z","path":"/posts/346bfea7/","text":"通用命令 status: 提供HBase的状态，例如，服务器的数量。 version: 提供正在使用HBase版本。 table_help: 表引用命令提供帮助。 whoami: 提供有关用户的信息。 数据定义语言（表操作命令）。 create: 创建一个表。 list: 列出HBase的所有表。 disable: 禁用表。 is_disabled: 验证表是否被禁用。 enable: 启用一个表。 is_enabled: 验证表是否已启用。 describe: 提供了一个表的描述。 alter: 改变一个表。 exists: 验证表是否存在。 drop: 从HBase中删除表。 drop_all: 丢弃在命令中给出匹配“regex”的表。 Java Admin API: 在此之前所有的上述命令，Java提供了一个通过API编程来管理实现DDL功能。在这个org.apache.hadoop.hbase.client包中有HBaseAdmin和HTableDescriptor 这两个重要的类提供DDL功能。 数据操纵语言 put: 把指定列在指定的行中单元格的值在一个特定的表。 get: 取行或单元格的内容。 delete: 删除表中的单元格值。 deleteall: 删除给定行的所有单元格。 scan: 扫描并返回表数据。 count: 计数并返回表中的行的数目。 truncate: 禁用，删除和重新创建一个指定的表。 Java client API: 在此之前所有上述命令，Java提供了一个客户端API来实现DML功能，CRUD（创建检索更新删除）操作更多的是通过编程，在org.apache.hadoop.hbase.client包下。 在此包HTable 的 Put和Get是重要的类。 创建样本模式 列族 创建表create 'emp', 'personal data', 'perfessional data' 更改列族单元格的最大数目hbase> alter 't1', NAME => 'f1', VERSIONS => 5 表范围运算符 使用alter，可以设置和删除表范围，运算符，如MAX_FILESIZE，READONLY，MEMSTORE_FLUSHSIZE，DEFERRED_LOG_FLUSH等。 #设置只读 hbase>alter 't1', READONLY(option) alter ‘emp’, READONLYalter ‘t1’, METHOD => ‘table_att_unset’, NAME => ‘MAX_FILESIZE’ drop 在删除一个表之前必须先将其禁用。hbase(main):018:0> disable 'emp' hbase(main):019:0> drop 'emp' HBase创建数据put 命令, add() - Put类的方法 put() - HTable 类的方法 使用put命令，可以插入行到一个表。它的语法如下： put '', 'row1', '','' 插入第一行 put 'emp', '1','personal data:name','xifu' put 'emp', '1', 'personal data:city','taizhou' put 'emp', '1', 'perfessional data:designation','manager' put 'emp', '1', 'perfessional data:salary','50000' document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"HDFS上传和读取流程","date":"2019-03-28T06:34:00.000Z","path":"/posts/e4062a35/","text":"![Git][git] 上传流程 1.根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。 2.namenode返回是否可以上传 3.client请求第一个 block该传输到哪些datanode服务器上 4.namenode返回3个datanode服务器ABC 5.client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将这个pipeline建立完成，逐级返回客户端 6.client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 7.当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。读取流程客户端读取HDFS数据相比写入数据要简单一些，以下是读取数据步骤： 1.client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 2.就近挑选一台datanode服务器，请求建立输入流。 3.开始读取这个数据的第一个block块，读取完全之后，开始接着读取这个文件的第二个block，直至把这个数据所有的block都读完了则文件读取完全了。 4.数据读完之后关闭流连接。 5.如果读取过程当中读取失败，将会依次读取该数据块的下一个副本，失败的节点将会被记录，不再连接。 ￼# SecondaryNameNodeSecondaryNameNode 是 HDFS 架构中的一个组成部分，它用来保存名称节点中对HDFS元数据信息的备份，减小Editlog文件大小，从而缩短名称节点重启的时间。 它一般是单独运行在一台机器上。 SecondaryNameNode让EditLog变小的工作流程如下: (1)SecondaryNameNode 会定期和 NameNode 通信，请求其停止使用 EditLog 文件，暂时将 新的写操作写到一个新的文件 edit.new 中，这个操作是瞬间完成的，上层写日志的函数完全感觉不到差别。 (2) SecondaryNameNode 通过 HTTP GET 方式从 NameNode 上获取到 Fslmage 和 EditLog 文 件，井下载到本地的相应目录下。 (3) SecondaryNameNode 将下载下来的 Fslmage 载入到内存，然后一条一条地执行 EditLog 文件中的各项更新操作，使内存中的 Fslmage 保持最新。 这个过程就是 EditLog 和 Fslmage 文件合 井。 (4) SecondaryNameNode 执行完（3）操作之后，会通过 post 方式将新的 Fslmage 文件发送 到 NameNode 节点上 (5) NameNode 将从 SecondaryNameNode 接收到的新的 Fslmage 替换旧的 Fslmage 文件，同 时将 Edit.new 替换 EditLog 文件，从而减小 EditLog 文件大小。￼从上面的过程可以看出，第二名称节点相当于为名称节点设置一个“检查点” ，周期性备份 名称节点中的元数据信息，但第二名称节点在 HDFS 设计中只是一个冷备份，并不能起到“热备 份”的作用。 HDFS 设计并不支持当名称节点故障时直接切换到第二名称节点。 HDFS FederationHDFSl.O 的单 NameNode 设计不仅存在单点故障问题，还存在可扩展性和性能问题。只有一 个 NameNode， 不利于水平扩展。 HDFS Federation (HDFS 联邦）特性允许一个 HDFS 集群中存在 多个 NameNode 同时对外提供服务，这些 NameNode 分管一部分目录（水平切分），彼此之间相 互隔离，但共享底层的 DataNode 存储资源。每个 NameNode 是独立的，不需要和其他 Nam巳Node 协调合作。 如图 4-5 所示 ， Federation 使用了 多 个独立的 NameNode/NameSpace 命名空间。这些 NameNode 之间是联合的，也就是说， 它们之间相互独立且不需要互相协调，各自分工管理自己的 区域。分布式的 DataNode 被用作通用的数据块存储设备。每个 DataNode 要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告， 并执行来自所有 NameNode 的命令。每一个 DataNode 作为统一的块存储设备被所有 NameNode 节点使用。 每一个 DataNode 节点都在所有的 NameNode 进行注册。 DataNode 发送心跳信息、块报告到 所有 NameNode，同时执行所有 NameNode 发来的命令。￼ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"python爬虫笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/65abd992/","text":"一、爬虫如何抓取网页数据网页三大特征：1、网页都有自己的唯一的URL（统一资源定位符）来进行定位。2、网页都使用HTML(超文本标记语言)来描述页面信息。3、网页都使用HTTP/HTTPS(超文本传输协议)协议来传输HTML数据。 爬虫的设计思路1、首先确定需要爬取的网页URL地址。2、通过HTTP/HTTP协议协议来获取对应的HTML页面。3、提取HTML页面里有用的数据： a.如果是需要的数据，就保存起来。 b.如果是页面里的其他URL，那就继续执行第二步。 #为什么选择Python做爬虫？PHP：是世界上最好的语言，但他天生不是干爬虫的，对多线程，异步支持不够好。爬虫是工具性程序，对速度和效率的要求比较高。Java爬虫生态圈很完善，是Python爬虫最大的对手。但是Java语言本身很笨重，代码量很大。爬虫经常需要修改部分采集代码，所以Java不合适。C/C++运行效率和性能几乎最强，但是学习成本很高。代码成型比较慢。能用C/C++做爬虫，只能说是能力的表现，但不是正确的选择。Python语法优美、代码简介、开发效率高、支持的模块多，相关HTTP请求模块还有强大的爬虫Scrapy，以及成熟高效的scrapy-redis分布式策略。而且，调用其他接口也非常方便（胶水语言） 二、如何抓取HTML页面：HTTP请求的处理，urllib、urllib2、requests 处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件 #解析服务器响应的内容 re、xpath、BeautifulSoup4（bs4）、jsonpath、pyquery等 使用某种描述性一样来给我们需要提取的数据定义一个匹配规则 符合这个规则的数据就会被匹配 #如何采集动态HTML、验证码的处理 通过动态页面采集，Selenium+PhantomJS(无界面)：模拟真实浏览器加载js、ajax等非静态的数据。 Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入/专门的打码平台。 sccrapy框架：（Scrapy，Pyspider） 搞定制性高性能（异步网络框架 twisterd），所以数据下载速度非常快，提供了数据存储、数据下载、提取规则等组件。 分布式策略：scrapy-redis，在Scrapy的基础上添加了一套以Redis数据库为核心的一套组件。让Scrapy框架支持分布式的功能。主要在Redis里做请求指纹去重、请求分配、数据临时存储。 三、爬虫–反爬虫–反反爬虫 之间的斗争：其实怕重做到最后，最头疼的不是复杂的页面，也是灰色的数据，而是网站另一边的反爬虫人员。 User-Agent、代理、验证码、动态数据加载、加密数据。 数据价值、是否值的去费劲做反爬虫。 1. 机器成本 + 人力成本 > 数据价值，就不反了，一般做到封IP就结束了。 2、面子的战争...... 爬虫和反爬虫之间的斗争，最后一定是爬虫获胜。 为什么？只要是真实用户可以浏览的网页数据，爬虫就一定能爬下来！ #根据使用场景 分为 ：通用爬虫 聚焦爬虫1通用爬虫：搜索引擎用的爬虫系统。1、目标：就是尽可能吧互联网上所有的网页下载下来，放到本地服务器里形成备份； 再对这些网页做相关处理（提取关键字、去掉广告），最后提供一个用户检索接口。2、抓取流程： a） 首选选取一部分已有的URL，把这些URL放到待爬取队列。 b） 从队列里取出这些URL，然后解析DNS得到主机IP，然后去这个IP对应的服务器里下载HTML页面，保存到搜索引擎的本地服务器。之后把这个爬过的URL放入已爬取队列。 c）分析这些网页内容，找出网页里其他的URL链接，继续执行第二步，直到爬取条件结束。3、搜索引擎如何获取一个新网站的URL: A).主动向搜索引擎提交网址, B).向其他网站里设置网站的外链。 C).搜索引擎会和DNS服务商进行合作，可以快速收录新的网站。 DNS：就是把域名解析成IP的一种技术。4、通用爬虫并不是万物皆可爬，它也需要遵守规则：Robots协议：协议会指明通用爬虫可以爬取网页的权限。Robots.txt 只是一个建议。并不是所有爬虫都遵守，一般只有大型的搜索引擎爬虫才会遵守。咱们个人写的爬虫，就不用管了。5、通用爬虫工作流程：爬取网页 - 存储数据 - 内容处理 - 提供检索/排名服务6、搜索引擎排名： ·PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站越值钱，排名越靠前。 ·竞价排名：谁给钱多，谁排名就高。7、通用爬虫的缺点： 1、只能提供和文本相关的内容（HTML、Word、PDF）等等，但是不能提供多媒体（音乐、图片、视频）和二进制文件（程序、脚本）等。 2、提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。 3、不能理解人类语义上的检索。 2聚焦爬虫：爬虫程序员写的针对某种内容爬虫。面向主题爬虫、面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求息息相关。 http的端口号：80；https的端口是：443； Python自带的模块：/usr/lib/python2.7/urllib2.py Python的第三方模块： /usr/local/lib/python2.7/site-packages urllib2 默认的 User-Agent：Python-urllib/2.7 User-Agent: 是爬虫和反爬虫斗争的第一步，养成好习惯，发送请求带User-Agent response 是服务器响应的类文件，除了支持文件操作的方法外，还支持以下常用的方法： 返回 HTTP的响应码，成功返回200，4服务器页面出错，5服务器问题 print response.getcode() 返回 返回实际数据的实际URL，防止重定向问题print response.geturl() 返回 服务器响应的HTTP报头print response.info() 四、User-Agent 历史： Mosaic 世界上第一个浏览器：美国国家计算机应用中心 Netscape 网景：Netscape（支持框架），慢慢开始流行….(第一款支持框架的浏览器) Microsoft 微软：Internet Explorer（也支持框架） 第一次浏览器大战：网景公司失败..消失 Mozilla 基金组织：Firefox 火狐 - （Gecko内核）(第一款浏览器内核) User-Agent 决定用户的浏览器，为了获取更好的HTML页面效果。 IE开了个好头，大家都开就给自己披着了个 Mozilla 的外皮 Microsoft公司：IE（Trident） Opera公司：Opera（Presto） Mozilla基金会：Firefox（Gecko） Linux组织：KHTML （like Gecko） Apple公司：Webkit（like KHTML） Google公司：Chrome（like webkit） 其他浏览器都是IE/Chrome内核 五、 Scrapy架构图（绿线是数据流向）：ScrapyEngine（引擎）：负责通讯，信号、数据传递 制作Scrapy爬虫 四步骤： 新建项目（scrapy startproject xxx）:新建一个新的爬虫项目 明确目标（编写items.py）：明确你想要爬取的目标 制作爬虫（spiders/xxspider.py）：制作爬虫开始爬取网页 存储内容（pipelines.py）：设计管道存储爬取内容 创建爬虫项目1234#创建普通项目scrapy startproject 项目名#创建模板scrapy startproject 项目名 网站名 六、正则表达式1、规范 表达式 描述 . 除了\\n和\\r的所有字符 \\d 数字 \\D 非数字 \\w 字母和下划线 \\W 非字母和下划线 \\s 空格（包括制表符、换页符等） [a-z] 小写英文字母 [a-zA-Z0-9] 大小写英文字母与数字 [123] 数字123 [^123] 不是数字123 * 出现次数>=0 + 出现次数>=1 {n} 出现次数=n {n,m} m>=出现次数>=n ^ 以开头 $ 以结尾 ? 关闭贪婪模式 () 用于获取括号内匹配成功的字符串 2、匹配div标签要取class=“class1”中的文本内容 1要匹配的内容 正则表达 div_pattern1=’ { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"python笔记","date":"2019-03-28T06:34:00.000Z","path":"/posts/c00b24b1/","text":"一、基本语法1、ipython命令技能实现python命令，也能实现部分linux命令。2、python2不能识别文件里的中文，如果硬要识别，在头文件中加上 -- coding:utf-8 -- （python官方推荐这种方式）3、输入一个数 1high = input(\"请输入一个数\") 4、输出一个数 12345age = 18print(\"age的值为%d\"%age)name = \"西芙\"print(\"name的值为%s\"%name);print(\"name的值是%s,年龄的值是%d\"%(name,age)) 4、if else语句12345678910age = 19if age>18: print(\"已经成年\")else print(\"未成年\")#input获取的所有数据都会当成字符串类型#python 规定 str（）>int()#所以如果 age = input(\"一个值\")#要与int比较，需要把age换成int类型，即#age_num = int(age) 5、python语言是弱类型的编程语言，及赋值的时候已经知道是什么类型6、查看变量类型 1type(a) 7、python的变量类型123456·Numbers(数字)： int(有符号整型)，long（长整型[也可代表八进制和十六进制]），float（浮点型），complex（复数）·布尔类型： True，False·String（字符串）·List（列表）·Tuple（元组）·Dictionary（字典） 8、关键字查看关键字命令 12import keyword #导入keyword包keyword.kwlist 9、运算符 12345\"s\"*10#输出'ssssssssss'#幂#** 10、is是地址相同的意思，在数值-5到256间，a与b地址相同，其余不同￼11、深拷贝与浅拷贝123456789101112131415161718192021222324252627282930313233#例1、浅拷贝a = [11, 22, 33]b = a #浅拷贝，地址拷贝，地址相同#例2、深拷贝import copyc = copy.deepcopy(a) #开辟一个内存空间，拷贝内容#例3、在数组中，深拷贝数组后，a = [11,22,33]b = [44,55,66]c = [a, b]d = copy.deepcopy[c]#查看c，d的id，并不相同，深拷贝#在a数组中添加[44],查看c[0],d[0]a.append(44)c[0][11,22,33,44]d[0][11,22,33]#说明深拷贝后，d另外拷贝了有关a、b的拷贝#例4、有关指向a，b的拷贝e = copy.copy(c)a.append(55)c[0]e[0]#所看到的结果都是[11,22,33,44,55]id(c)id(e)#所看到的地址不同，所以他们是指向不同的地址但所指向数组的内容地址是相同的#但如果是c = (a,b)，是元组的话，由于元组是不可变类型，e = copy.copy(c),他们所指向的地址是一样的#所以使用copy模块的copy功能时候，它会根据当前拷贝的数据类型是可变类型还是不可变类型有不同的处理方式 12、私有化12345678910class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__numt = Test()t.__num = 200 #可以使用print(t.__num) 升级版私有化 property使用123456789class Test(object): def __init__(self): self.__num = 100 def setNum(self, newNum): self.__num = newNum def getNum(self): return self.__num num = property(getNum, setNum)#!!!!#此时，可以用t.num = ?赋值 或者 t.num 取值，不用调用函数 装饰器使用12345678910class Test(object): def __init__(self): self.__num = 100 @property def num(self): return self.__num @num.setter def num(self, newNum): self.__num = newNum#与上面的代码意思相同。理解一下吧~ 1、xx:公有变量2、x:单前置下划线，私有化属性或方法，from somemodule import * 禁止导入，类对象和子类可以访问3、__x:双前置下划线，避免与子类中的属性命名冲突，无法在外部直接访问（名字重整所以访问不到）4、x：双前后下划线，用户名字空间的魔法对象或属性。例如init,__不要自己发明这样的名字5、xx_:单后置下划线，用于避免与python关键词的冲突 ###但是私有属性实际上是可以访问，python把私有属性的名字由原来的_私有属性名 改成 _类名私有属性名，所以可以通过对象._类名__私有属性名可以获取值！！！ 13、迭代器迭代器 迭代器是访问集合元素的一种方式，迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，知道所有的元素被访问完结束。迭代器只能往前不会后退。可迭代对象： 可直接作用于for循环的数据类型有以下几种： 一类是集合数据类型：如list、tuple、dict、set、str等 一类是generator，包括生成器和带yield的generator function。 可以直接作用于for循环的对象统称为可迭代对象：Iterable。 14、闭包：定义：函数里面有另外的函数，并且里面的函数用到了外面的函数的变量闭包的应用：1234567891011121314def test(number): print(\"--1--\") def test_in(number2): print(\"--2--\") print(number+number2) print(\"--3--\") return test_in ret = test(100)print(\"-\"*30)ret(1)ret(100)ret(200)#优点 简化了步骤 linux快捷键： %s/^/#/g 所有行前面加# 1,14/#//g 去掉1-14行的#15、装饰器12345678910111213141516171819def w1(func): def inner(): print(\"----正在验证权限---\") func() return innerdef f1(): print(\"---f1---\")def f2(): print(\"---f2---\")#innerFunc = w1(f1)#innerFunc()f1 = w1(f1) #把w1（f1）赋值给f1，也就是说f1 = w1.inner，f1指向的就是inner这个函数f1() #调用后，执行的是inner，但由于之前传入f1的参数，所以输出结果为inner的输出结果+f1函数的输出结果@w1 #与上面f1=w1(f1)意义相同，但只要python执行器执行到了这个代码，name就会自动的进行装饰，而不是等到调用的时候才装饰的 语法糖 @w1def f1(): print(\"---f1---\") 12345678910111213141516171819#若有参数def func(functionName): print(\"---func---1---\") def func_in(*args, **kwargs):#如果没有定义参数，会导致调用的时候出现问题 print(\"---func_in---1---\") functionName(*args, **kwargs)#传入同样的参数 print(\"---func_in---2---\") print(\"---func---2---\") return func_in@funcdef test(a, b, c): print(\"---test-a=%d,b=%d,c=%d---\"%(a,b,c)) @funcdef test2(a, b, c, d): print(\"---test-a=%d,b=%d,c=%d,d=%d---\"%(a,b,c,d))test(11,22,33)test2(44,55,66,77) 12345678910111213141516171819202122232425262728#通用装饰器def func(functionName): def func_in(*args, **kwargs): print(\"---记录日志---\") ret = functionName(*args, **kwargs) return ret return func_in@funcdef test(): print(\"---test---\") return \"haha\"@funcdef test2(): print(\"---test2---\")@funcdef test3(a): print(\"---test3---a=%d--\"%a) ret = test()print(\"test return value is %s\"%ret)a = test2()print(\"test2 return value is %s\"%a)test3(11) 16、作用域 #什么是命名空间在某个范围内所能用到的作用域 17、类方法与静态方法1234567891011121314151617#给类添加方法import types #导入types包class Person(object): def __init__(self, newName, newAge): self.name = newName self.age = newAge def eat(self): print(\"-----%s正在吃----\"%self.name)def run(self): print(\"-----%s正在跑----\"%self.name) p1 = Person(\"p1\", 10)p1.eat()p1.run = types.MethodType(run, p1)p1.run() 18、限制class实例添加的属性123456class Person(object): __slots__ = (\"name\", \"age\")p = Person()p.name = \"老王\"p.age = 20p.score = 100 #会报错！！！ 二、生成器、迭代器、装饰器、闭包19、生成器 12345678910In [1]: a = [x*2 for x in range(10)]In [2]: aOut[2]: [0, 2, 4, 5, 6, 10, 12, 14, 16, 18]#如果用[]，会生成数组，同时加载到内存中#如果用括号（），输出的是一个地址，需要时才提取出In [3]: b = (x*2 for x in range(10))In [4]: bOut[4]: In [5]:next(b)Out[5]:0 20、a，b交换123456789101112131415def creatNum(): print(\"----start----\") a, b = 0, 1 for i in range(5): print(\"----1----\") yield b #python中有一个非常有用的语法叫做生成器，所利用到的关键字就是yield。有效利用生成器这个工具可以有效地节约系统资源，避免不必要的内存占用。当执行到这的时候，会做停顿。 print(\"----2----\") a, b = b, a+b print(\"----3----\") print(\"----stop----\")#创建了一个生成器对象a = creatNum()#实现交换a, b = 0, 1a, b = b, a 21、斐波拉契数列12a,b = 0, 1a,b = b, a+b #重复此步骤 22、类当做装饰器12345678910111213141516171819202122232425262728#1class Test(object): def __call__(self): print(\"---test---\") t = Test()t()#out: ---test---#2class Test(object): def __init__(self, func): print('---初始化---') print('func name is %s'func.__name__) self.__func = func def __call__(self): print('---装饰器中的功能---') self.__func()@Test #相当于 t = Test(test) def test(): print('---test---')#out: ---初始化---#out:func name is 'test'test()#out: ---装饰器中的功能#out:---test--- 23、元类1234567891011Test = type(\"Test\", (), {})#相当于创建一个类Test#out:__main.Test#创建一个拥有属性num=0的Person类Person = type(\"Person\",(),{\"num\":0})#创建一个拥有方法printNum()的Test3类def printNum(self): print(\"--num-%d--\"%self.num)Test3 = type(\"Test3\",(),{\"printNum\":printNum}) 123456789101112131415161718192021222324#设置Foo类的属性变大写！！，通过metaclass设置def upper_attr(future_class_name, future_class_parents, future_class_attr): #Foo object {bar:\"bip\"} #遍历属性字典，把不是__开头的属性名字变成大写 newAttr = {} for name, value in future_class_attr.items(): if not name.startswith(\"__\"): newAttr[name.upper()] = value #调用type创建一个类 return type(future_class_name,future_class_parents,newAttr)class Foo(object,metaclass=upper_attr): # __metaclass__ = upper_attr #设置Foo类的元类为upper_attr python2 bar = \"bip\"print(hasattr(Foo, \"bar\"))print(hasattr(Foo, \"BAR\"))f = Foo()print(f.BAR)#out:False#out:True#out:bip 24、内建属性123456789101112131415161718class Itcast(object): def __init__(self,subject1): self.subject1 = subject1 self.subject2 = 'cpp' #属性访问时拦截器，打log def __getattribute__(self,obj): #obj-->\"subject1\" if obj == 'subject1': print('log subject1') return 'redirect python' else: return object__getattribute__(self,obj) def show(self): print('this is Itcast') s = Itcast(\"python\")print(s.subject1)print(s.subject2) 三、内建方法25、lambda之map123456789101112131415#函数需要一个参数map(lambda x: x*x, [1, 2,3])#结果为：[1, 4, 9]#函数需要两个参数map(lambda x, y: x+y, [1, 2, 3], [4, 5, 6])#结果为:[5,7,9]def f1(x, y): return (x, y)l1 = [0,1,2,3,4,5,6]l2 = ['Sun', 'M', 'T', 'W', 'T', 'F', 'S']l3 = map(f1, l1, l2)print(list(l3))结果为：[(0,'Sun'), (1,'M').....] 26、lambda之filter12345fliter(lambda x : x%2, [1, 2 ,3, 4]) #如果为1，则输出[1, 3]filter(None, \"she\")'she' 27、lambda之reduce12345678reduce(lambda x, y: x+y, [1,2,3,4])10 #先把1赋值给x，2赋值给2，x+y后=3，把3赋值给x，数组中的3赋值给y，再累加，以此类推reduce(lambda x, y:x+y, [1,2,3,4], 5)15 #如果前面是数组，先把5赋值给x，再累加[1,,2,3,4]reduce(lambda x, y: x+y, ['aa', 'bb', 'cc'], 'dd')ddaabbcc 28、sort123456789a = [9,8,7,6,5,4,3,2,1]a.sort()a#[1,2,3,4,5,6,7,8,9]b = ['dd','cc','bb','aa']b.sort()b#['aa','bb','cc','dd'] 28、python的functools包中提供了一个叫wraps的装饰器来消除这样的副作用1234567891011121314151617import functoolsdef note(func): \"note function\" @functoolswraps(func) def wrapper(): \"wrapper function\" print(\"note something\") return func() return wrapper@notedef test(): \"test function\" print('I am test')test()print(test.__doc__) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"Linux复习","date":"2019-03-28T06:34:00.000Z","path":"/posts/0/","text":"Linux一、Linux起源Linux系统诞生于1991年，由芬兰大学生李纳斯（Linux Torvalds）。 二、命令2.1文件处理命令 ls （-lh...） 查看当前目录 -rw-r--r-- 1 root root 8690 3月 1 13:23 install.log.syslog 1是引用计数（代表此文件被调用几次 引用几次） 文件所有者 文件所属组 字节文件大小 最后修改时间 文件名称 -rw-r--r-- - 文件类型（- 二进制文件 d目录 l软连接文件） rw- r-- r-- u g o u所有者 g所属组 o其他人 r读 w写 x执行 cd 进入目录 pwd 显示当前绝对路径 mkdir 2.2网络通信命令 ping 测试网络连接 ifconfig 显示或配置网络设备（网络接口卡）的命令 2.3系统命令 shutdown 关机 reboot 重启系统 2.3文本编辑器 vim/vi 三、虚拟机安装 CPU：建议主频为1GHz以上 内存：建议1GB以上 硬盘：建议分区空闲空间8GB以上 3.1Linux分区 boot：400MB home 分区： 4G swap交换分区：2G 没有挂载点，就是没有盘符，swap不是给用户用的，是给操作系统或内科自己调用的 设置为2G 超过两个G就没有意义了 逻辑分区 / 剩余所有空间：第四个分区，linux不知道要分几个区，所有先创建一个sda5（第一个逻辑分区），一块硬盘只能有4个分区是第四个默认为逻辑分区，三个主分区，一个逻辑分区 3.2系统磁盘分区 基本分区（primary partion） 扩充分区（extension partion） 基本分区和扩充分区的数目之和不能大于四个。且基本分区可以马上被使用单不能再分区。扩充分区必须再进行分区后才能使用，也就是说它必须还要进行二次分区。扩充分区下面是逻辑分区（logical partion），逻辑分区没有数量上的限制。 主分区：最多只能有4个。 扩展分区： 最多只能有1个。 主分区加扩展分区最多有4个。 不能写入数据，只能包含逻辑分区 3.3分区格式化目的：为了给这个分区写入文件系统，也就是告诉我们的操作系统，如何去管理这个分区的数据。 分区：把大硬盘分为小的逻辑分区 格式化：写入文件系统 分区设备文件名：给每个分区定义设备文件名 挂载：给每个分区分配挂载点 四、虚拟机网络连接方式 桥接模式 NAT模式 仅主机模式 自定义模式 4.1桥接模式 你的虚拟机是利用你的真实网卡，一块本地有线网卡(以太网)，一块是本地无线网卡（wlan）装完虚拟机后会出现两块虚拟网卡，一块vmnet1，一块vmnet8，如何选择桥接你的虚拟机将会利用你的网卡以太网或无线网卡和你的真实机进行通信 好处是配置简单，和你的真是机的ip地址设置同一网段，和真机进行通信，局域网的其他电脑也可以通信。 缺点占用同意网段的一个ip地址，在家里宿舍没有问题，在教室可能会和其他同学的ip地址冲突。 4.2NAT模式 你的虚拟机是通过VMnet8这块假的虚拟网卡和你的真实机连接 4.3仅主机模式 你的虚拟机是通过VMnet1这块假的虚拟网卡和你的真实机连接。 桥接是不仅和你的真机通信还可以与你的局域网的其他机器通信，和一台笔记本是一个意思。Nat hostonly 只能和你真机通信，不可以和局域网其他电脑通信，不用占用你的真实网段的一个ip地址。Hostonly 只能和你计算机主机通信。NAT和主机通信，如果你的主机可以联网，虚拟机也可以联网 4.4df盘符分区命令df （-h）文件系统 1k-块 已用 可用 已用% 挂载点 五、文件处理命令5.1ls 查看目录下的文件 -a 显示所有文件，包括隐藏文件 -l 详细信息显示 -d 查看目录属性 -i Inode索引节点 5.2mkdir 创建目录 -p 创建递归 5.3cd 切换目录5.4pwd 显示当前目录5.5rmdir 删除空目录5.6cp 复制文件或目录 -rp 【源文件或目录】 【目标目录】 -r 复制目录 -p 保留文件属性 5.7clear 清屏操作5.8mv 剪切文件、改名5.9rm 删除文件-r 删除目录 -f 强制执行 5.10touch 创建空文件5.11cat 显示文件内容-n 显示行号 5.12more 分页显示文件内容(空格)或f 翻页 (Enter) 换行 q或Q 退出 5.13less 分页显示文件内容5.14ln 生成链接文件 -s创建软连接 软链接特征：类似windows快捷方式 lrwxrwxrwxl软链接 文件大小——只是符号链接 /tmp/issue.soft -> /etc/issue 箭头指向源文件 硬链接特征： 1、拷贝cp -p + 同步更新 2、通过i节点识别 3、不能跨分区 4、不能针对目录使用 5.15shutdown命令 -c 取消前一个关机命令 -h 关机 -r 重启 5.16vi/vim 建立、编辑、显示文本文件 vim是一个功能强大的全屏幕文本编辑器，是Linux/UNIX上最常用的文本编辑器，它的作用是建立、编辑、显示文本文件。 vim没有菜单，只有命令。 插入命令 命令 作用 a 在光标所在字符后插入 A 在光标所在行尾插入 i 在光标所在字符前插入 I 在光标所在行行首插入 o 在光标下插入新行 O 在光标上插入新行 定位命令 命令 作用 :set nu 设置行号 :set nonu 取消行号 gg 到第一行 G 到最后一行 nG 到第n行 :n 到第n行 $ 移至行尾 0 移至行首 删除命令 命令 作用 x 删除光标所在处字符 nx 删除光标所在处后n个字符 dd 删除光标所在行，ndd删除n行 dG 删除光标所在行到文件末尾内容 D 删除光标所在处到行尾内容 n1,n2d 删除指定范围的行 复制和剪切命令 命令 作用 yy 复制当前行 nyy 复制当前行以下n行 dd 剪切当前行 ndd 剪切当前行以下n行 p、P 粘贴在当前光标所在行下或行上 搜索和搜索替换命令 命令 作用 /string 搜索指定字符串 搜索时忽略大小写：set ic n 搜索指定字符串的下一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/g 在一定范围内替换指定字符串 保存和退出命令 命令 作用 :w 保存修改 :w new_filename 另存为指定文件 :wq 保存修改并退出 zz 快捷键，保存修改并退出 :q! 不保存修改退出 :wq! 保存修改并退出（文件所有者及root可使用） 5.17 sort排序命令 sort [选项] 文件名 -f 忽略大小写 -n 以数值型进行排序 -r 反向排序 -t 指定分隔符，默认是分割符是制表符 -k n[,m] 按照指定的字段范围排序，从第n个开始，m字段结束（默认到行尾） 5.18 统计命令 wc wc [选项] 文件名 -l 只统计行数 -w 只统计单词数 -m 只统计字符数 5.19 awk grep sed grep 更适合单纯的查找或匹配文本 sed 更适合编辑匹配到的文本 awk 更适合格式化文本，对文本进行较复杂的格式处理（算一门编程语言） 六、权限管理命令6.1chmod 改变文件或目录（权限） -R 递归修改 权限的数字表示 r——4 w——2 x——1 文件目录权限总结 代表字符 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以在目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 6.2chown 改变文件或目录的（所有者）6.3chgrp 改变文件或目录的（所属组）chgrp 【用户组】【文件或目录】 6.4Groupadd 所有组名称新的命令6.5umask 显示、设置文件的缺省权限 umask -S 以rwx形式显示新建文件缺省权限 在linux创建文件默认权限没有x权限，防止病毒可执行脚本 在linux创建目录默认权限有x 七、文件搜索命令7.1find 【搜索范围】【匹配条件】尽量不要使用linux里的find命令浪费资源特别是使用高峰期的时候，目录和文件合理的命名存放位置最重要。 Find 路径 选项 关键字 完全搜索 Find / -name a 模糊搜索 Find / -name a 模糊搜索 Find / -name *a?? 模糊搜索 Find / -iname *a?? （iname不区分大小写） Find -size +n -n n 查询文件大小 (+大于 -小于 什么都不写 =) Find -user dj 根据所有者查找文件 find /etc -cmin -5 在/etc下查找5分钟内被修改过属性的文件和目录（+超过多长时间 -在多少分钟内） amin 访问时间 access cmin 文件属性 change mmin 文件内容 modify 八、用户管理命令 简介：所以越是对服务器安全性要求高的服务器，越需要建立合理的用户权限登记制度和服务器操作规范。在linux中主要是通过用户配置文件来查看好修改用户信息。1、添加更多的用户分配不同的权限。2、通过配置文件添加信息，使用命令重启可能就消失了。 8.1用户配置文件——用户用户信息文件 /etc/passwd 第1字段：用户名称第2字段：密码标志第3阶段：UID（用户ID） 0： 超级用户 1-499： 系统用户（伪用户） 500-65535： 普通用户第4字段：GID（用户初始组ID）第5字段：用户说明第6字段：家目录 普通用户：/home/用户名/ 超级用户：/root/第7字段：登录之后的Shell 初始组和附加组 初始组：就是指用户一登录就立刻拥有这个用户组的相关权限，每个用户的初始组只能有一个，一般就是和这个用户名相同的组名作为这个用户的初始组。 附加组：指用户可以加入多个其他的用户组，并拥有这些组的权限，附加组可以有多个。 shell是什么？ 1、shell就是Linux的命令解释器2、在/etc/passwd当中，除了标准shell是/bin/bash之外，还可以写如/sbin/nologin。 8.2用户配置文件——影子1、影子文件/etc/shadow 第1字段：用户名 第2字段：加密密码 加密算法升级为SHA512散列加密算法 如果密码位是“!!”或“*”代表没有密码，不能登录 第3字段：密码最后一次修改日期 使用1970年1月1日作为标准时间，每过一天时间戳加1 第4字段：两次密码的修改间隔时间（和第三字段相比） # 0代表随时可以修改密码，10代表10天后才能修改密码 第5字段：密码有效期（和第3字段相比） 第6字段：密码修改到期前的警告天数（和第5字段相比） 第7字段：密码过期后的宽限天数（和第5字段相比） 0：代表密码过期后立即失效 -1：则代表密码永远不会失效。 第8字段：账号失效时间 （到了不管过期 6 7） 要用时间戳表示 第9阶段：保留 2、时间戳换算 把时间戳换算为日期 >> date -d \"1970-01-01 16066 days\" 把日期换算为时间戳 >> echo $(($(date --date=\"2014/01/06\" +%s)/86400 +1)) 8.3用户配置文件——组件1、组信息文件/etc/group 第1字段：组别 第2字段：组密码标志 第3阶段：GID 第4字段：组中附加用户 2、组密码文件/etc/gshadow 第1字段：组别 第2字段：组密码 第3阶段：组管理员用户名 第4字段：组中附加用户 8.4用户管理相关文件用户的家目录 普通用户：/home/用户名/，所有者和所属组都是此用户，权限是700超级用户：/root/，所有者和所属组都是root用户，权限是550 8.5用户管理命令 useradd命令 useradd[选项] 用户名 -u UID：手工指定用户的UID号 -d 家目录 手工指定用户的家目录 -c 用户说明 手工指定用户的说明 -g 组名 手工指定用户的初始组 -G 组名 指定用户的附加组 -s shell 手工指定用户的登录shell。默认是/bin/bash passwd命令格式 passwd[选项] 用户名 -S 查询用户密码的密码状态。仅root用户可用。 -l 暂时锁定用户。仅root用户可用。 -u 解锁用户。仅root用户可用。 --stdin 可以通过管道符输出的数据作为用户的密码。 修改用户信息usermod usermod[选项] 用户名 -u UID 修改用户的uid号 -c 用户说明 修改用户的说明信息 -G 组名 修改用户附加组 -L 临时锁定用户(lock) -U 解锁用户锁定（Unlock） 修改用户密码状态chage chage[选项] 用户名 -l 列出用户的详细密码状态 -d 日期 修改密码最后一次更改时间（shadow3字段） -m 天数 两次密码修改间隔（4字段） -M 天数 密码有效期（5字段） -W 天数 密码过期前警告天数（6字段） -l 天数 密码过期后宽限天数（7字段） -E 日期 账号失效时间（8字段） 删除用户userdel userdel[-r] 用户名 -r 删除用户的同时删除用户家目录 切换用户身份su su [选项] 用户名 - 选项只使用“-”代表连带用户的环境变量一起切换 -c命令 仅执行一次命令，而不切换用户身份 九、压缩解压命令tar命令 解包：tar zxvf FileName.tar 打包：tar czvf FileName.tar DirName 十、网络命令10.1write 给用户发信息 网络 w命令查看用户在线情况 打开两个窗体（写错的时候 退格键 ctrl+backspace键） 写好了 ctrl+D保存结束 远程终端第一个0 终结符EOF 10.2wall 【message】 发广播信息广播信息 在线用户 10.5ping 测试网络连通性ping 选项 IP地址 -C 指定发送次数 10.4ifconfig 查看和设置网卡信息eth0第一块网卡 Ethernet网络昵称以太网 网络类型目前我们接触的都是以太网 Hwaddr 网卡的物理地址 Inet addr 当前计算机地址 Bcast 发送广播的ip地址 Mask 子网掩码网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分 Rx接受数据包数量 byte 接收到的数据包的总大小 TX发送数据包数量 Interrupt 网卡在内存中的物理地址 Lo回环网卡每台机器都有用来做本机网络测试的 10.5mail 查看发送电子邮件不在线也能收到 收到直接数据mail 直接回车 n代表没有读的邮件 1代表一份邮件 输入1回车查看第一份信内容 10.6last 列出目前与过去登入系统的用户信息 计算机所有用户登录系统信息 dj pts/1 192.168.40.1 Thu Mar 7 13:04 still logged in 第二个远程终端 登录的远程ip 一直在登录 10.7netstat 【选项】 显示网络相关信息 选项 用途 -t TCP协议 -u UDP协议 -l 监听 -r 路由 -n 显示IP地址和端口号 范例 netstat -tlun 查看本机监听的端口 netstat -an 查看本机所有的网络连接 netstat -rn 查看本机路由器 netstat -ntlp Tcp http 用的协议 三次握手 安全可靠 传输 打电话 Udp 快 发短信 端口 ip地址为公司名字 找某人 就是端口 Destination Gateway Genmask Flags MSS Window irtt Iface setup 配置网络 dhcp 自动分配自动获取服务 * 默认相当于windows自动获取ip地址没有用，原因个人电脑和家里的环境不会有路由分配。 setup操作结束后 使用 service network restart命令 十一、shell概述 hell是一个命令行解释器。它为用户提供了一个向linux内核发送请求以便运行程序的界面系统级程序。用户可以用shell来启动、挂起、停止甚至是编写一些程序。 注释： 内核 机器语言01010外层 pwd ls命令shell就是黑色及交互命令窗体 shell还是一个功能相当强大的编程语言，易编写，易调试，灵活性较强。shell是解释执行的脚本语言，在shell中可以直接调用Linux系统命令。 shell的分类 1、Bourne Shell: 从1979起Unix就开始使用Bourne Shell，Bourne Shell的主文件名为sh。 2、 C Shell： C Shell主要在BSD版的Unix系统中使用，其语法和C语言相类似而得名。 shell的两种语法类型有Bourne和C，这两种语法彼此不兼容。Bourne家族主要包括sh、ksh、Bash、psh、zsh；C家族主要包括：csh、tcsh linux标准shell是伯恩 shell bash ei Bash:Bash与sh兼容，现在使用的linux就是使用bash作为用户的基本shell。 linux支持的shell /etc/shells 11.1shell脚本运行echo输出命令 echo [选项] [输出内容] -e 支持反斜线控制的字符转换转移符 echo -e ‘\\e[1,31m abcd \\e[0m’ 变色 11.2linux标准shellbase历史命令 history [选项] [历史命令保存文件] -c 清空历史记录 -W 把缓存中的历史命令写入历史命令保存文件 ~/.bash_history 历史命令默认会保存1000条，可以在环境变量配置文件/etc/profile中进行修改 11.2标准输入输出 设备 设备文件名 文件描述符 类型 键盘 /dev/stdin 0 标准输入 显示器 /dev/sdtout 1 标准输出 显示器 /dev/sdterr 2 标准错误输出 11.3输出重定向 类型 符号 作用 标准输出重定向 命令 > 文件 以覆盖的方式，把命令中的正确输出输出到指定的文件或设备当中 标准输出重定向 命令 >> 文件 以追加的形式，把命令中的正确输出输出到指定的文件或设备当中 标准错误输出重定向 错误命令 2>文件 以覆盖的方式，把命令的错误输出输出到指定的文件或设备当中 标准错误输出重定向 错误命令 2>>文件 以追加的形式，把命令的错误输出输出到指定的文件或设备当中 13.4输入重定向 命令name=\"xifu\" #变量定义 >#变量叠加 >aa=123 >aa=\"$aa\"456 >aa=${aa}789 >echo $name #变量调用 >set #变量查看 > unset name #变量删除 环境变量 用户自定义变量只在当前的shell中生效，而环境变量会在当前shell和这个shell的所有子shell中生效。如果把环境变量写入相应的配置文件，name这个环境变量就会在所有的shell中生效。 环境变量语法 export 变量名=变量值 #申明变量env #查询变量unset 变量名 #删除变量 位置参数变量 位置参数变量 作用 $n n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数需要用大括号包含，如${10} $* 这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体 $@ 这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待 $# 这个变量代表命令行中所有参数的个数 例1： 12345#!/bash/bashnum1=$1num2=$2sum=$(($num1+$num2))echo $sum #打印变量sum的值 例2： 1234567#!/bash/bash#使用$#代表所有参数的个数echo 'A total of $# parameters'#使用$*代表所有参数echo 'The parameters is: $*'#使用$@代表所有参数echo 'The parameters is: $@' 预定义变量 预定义变量 作用 $? 最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。 $$ 当前进程的进程号（PID） $! 后台运行的最后一个进程的进程号（PID） && || 是通过什么第二命令知道第一命令是否正常运行的呢 其实是通过$?的数字才判断的 程序员用户眼睛判断命令是否正确 计算机是通过$?判断命令是否正确 接收键盘输入 read -p “提示信息”：在等待read输入时，输出提示信息 -t 描述 read命令会一直等待用户输入，使用此选项可以指定等待时间 -n 字符数 read命令只接受指定的字符数，就会执行 -s 隐藏输入的数据，适用于机密信息的输入 12345678910111213#提示“请输入姓名”并等待30秒，把用户的输入保存入变量name中read -t 30 -p \"Please input your name:\" nameecho \"Name is $name\"#年龄是隐私，用“-s”选项隐藏输入read -s -t 30 -p \"Please enter your age:\" ageecho -e \"\\n\"echo \"Age is $age\"#使用“-n 1”选项只接收一个输入字符就会执行（都不用输入回车）read -n 1 -t 30 -p \"please select your gender[M/F]:\" genderecho -e \"\\n\"echo \"sex is $gender\" declare声明变量类型 declare [+/] [选项] [变量名] 给变量设定类型属性 取消变量的类型属性 -i 将变量声明为整数型（integer） -x 将变量声明为环境变量 -p 显示指定变量的被声明的类型 例：方法一，数值运算——方法1 1234aa=11bb=22给变量aa和bb赋值declare -i cc=$aa+$bb 例：方法二，expr或let数值运算工具 12345aa=11bb=22#给变量aa和bb赋值dd=$(expr $aa + $bb)#dd的值是aa和bb的和。注意“+”号左右两侧必须有空格 例：方法三，”$((运算式))” 或”$[运算式]” 123456aa=11bb=22ff=$(( $aa+$$bb ))gg=$[ $aa+$$bb ]#单个小括号是系统命令#双个小括号代表数值运算符 变量置换方式 变量y没有设置 变量y为空值 变量y设置值 x=${y-新值} x=新值 x为空值 x=$y x=${y:-新值} x=新值 x为新值 x=$y x=${y+新值} x为空 x为新值 x=新值 x=${y:+新值} x为空 x为空值 x=新值 x=${y=新值} x=新值，y=新值 x为空值，y值不变 x=$y，y值不变 x=${y:=新值} x=新值，y=新值 x为新值，y为新值 x=$y，y值不变 x=${y?新值} 新值输出到标准错误输出 x为空值 x=$y x=${y:?新值} 新值输出到标准错误输出 新值输出到标准错误输出 x=$y 十二、正则表达式 正则表达式用来在文件中匹配符合条件的字符串，正则是包含匹配。grep、awk、sed等命令可以支持正则表达式。 通配符用来匹配符合条件的文件名，通配符是完全必配。ls、find、cp这些命令不支持正则表达式，所以只能使用shell自己的通配符来进行匹配。 元字符 作用 * 前一个字符匹配0次或任意多次 . 匹配除了换行符外任意一个字符 ^ 匹配行首。 $ 匹配行尾。 [] 匹配中括号中的任意一个字符，例如[a-z0-9] [^] 匹配除中括号的字符以外的任意一个字符。[^a-z] \\ 转义符 {n} 表示其前面的字符恰好出现几次 {n,} 表示其前面的字符出现不小于n次 {n,m} 表示前面的字符至少出现n次，最多出现m次。 十三、Linux 服务管理13.1服务管理分类 源码包可以看到源代码 可以自定义Rpm没有源代码 自定义差 独立服务：服务直接在内存中 客户直接调用服务，服务直接相应用户，速度快，服务多了浪费内存资源基于xinetd服务本身是独立的 本身没有功能，后面有一系列服务rsync 网络备份服务，通过xinted相应 rsync 相应最后客户端相应速度慢 本身不占用内存 13.2启动与自启动服务启动：就是当前系统让服务运行，并提供功能 服务自启动：是指让服务在系统开机或重启之后，随着系统的启动而自启动服务。 13.3查询已安装的服务RPM包安装的服务 chkconfig --list 123456aegis 0:off 1:off 2:on 3:on 4:on 5:on 6:offbt 0:off 1:off 2:on 3:on 4:on 5:on 6:offnetconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offnginx 0:off 1:off 2:off 3:off 4:off 5:off 6:off 0关机 1单用户 2不完全多用户 3字符界面 4未分配 5图形界面 6重启动 源码包安装的服务 查看服务安装位置 卸载 rpe -e --nodeps（不检查依赖性） 包名 十四、Linux系统备份与恢复14.1备份和恢复概述 Linux系统中需要备份的数据 /root/目录 /home/目录 /var/spool/mail/目录 /etc/目录 其他目录 14.2 备份策略 完全备份： 优点是数据恢复方便 缺点备份的数据量较大,备份时间较长,占用的空间较大增量备份： 优点备份的数据较少，耗时较短，占用的空间较小； 缺点是数据恢复比较麻烦，先恢复完全备份的数据每次增量备份的数据，最终才能恢复所有的数据。差异备份： 优点恢复数据简单方便快捷 缺点数据量庞大、备份速度缓慢、占用空间较大。 14.3、备份和恢复命令 备份命令 dump [选项]备份之后的文件名 原文件或目录 恢复命令 restore [模式选项][选项] 十五、Linux系统管理1、进程管理查看 ps aux用户 进程id 占用cpu 内存 内存daxiao 物理大 终端 状态 开始时间 占用cpu时间 命令 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.5 0.0 2872 1416 ? Ss 15:36 0:01 /sbin/init root 2 0.0 0.0 0 0 ? S 15:36 0:00 [kthreadd] 2、进程管理终止 kill命令3、工作管理4、系统资源查看5、系统定时任务 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]},{"title":"bat批量存放信息到excel","date":"2019-03-28T06:34:00.000Z","path":"/posts/5fe19fd1/","text":"情景：最近喵仙子有个需求，需要抽取日志文件中的有效信息到excel，刚开始我用MapReduce给他做了一份，不过他有点蠢配不了环境，于是乎我研究了下bat，终于给他整了个好方法，请看下文。 需求需要从log中抽取两列数据到excel中。 本西芙没了解过bat语法，所以查了一下，通过结合这两篇解答完成bat命令。1、实现了按照空格批处理分割txt数据，存储excelhttps://zhidao.baidu.com/question/1770548208106936980.html123456789101112@echo offrem 保存为bat文件跟txt文件放一起运行md \"结果\\\" 2>nulfor /f \"delims=\" %%a in ('dir /a-d/b *.txt') do ( setlocal EnableDelayedExpansion (for /f \"delims=\" %%b in ('type \"%%a\"') do ( set \"str=%%b\" echo;!str: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 2、批处理分割字符串https://blog.csdn.net/zhju85126com/article/details/46649961123456789101112@echo offset str=100x200x300y400y500x600y700for /f \"tokens=1,3-5,* delims=x|y\" %%a in (\"%str%\") do ( set c1=%%a set c3=%%b set c4=%%c set c5=%%d set c6=%%e)echo %c1%, %c3%, %c4%, %c5%, %c6%pauserem 输出结果为输出结果为：100, 300, 400, 500, 600y700。其中tokens=1,3-5,*表示提取第1、3至5列，同时把第5列后所有剩余字符串作为第6列，一个输出了5个变量，也可以写作tokens=1,3,4,5,*。 最终我所加工的代码，其中代码中tokens=1,2是按照_或空格分割的位置，相当于数组的某一元素、把它赋值给str112345678910111213@echo offmd \"结果\\\" 2>nulrem 循环数据for /f \"delims=\" %%a in ('dir /a-d/b *.log') do ( setlocal EnableDelayedExpansion rem 切割方式 (for /f \"tokens=1,2 delims=_| \" %%b in ('type \"%%a\"') do ( set \"str1=%%b %%c\" echo;!str1: =,! ))>\"结果\\%%~na.csv\" Endlocal)pause 首先根据第一篇解答套入转excel模板，其次根据第二篇解答对bat for循环中分割和选取的描述，修改第五行代码。可以实现。 演示 文件内容 文件内容 目录下的文件 目录下的文件 2、点击aa.bat,生成csv文件 生成csv文件 3、任意打开一个csv文件 csv文件内容 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[]}]